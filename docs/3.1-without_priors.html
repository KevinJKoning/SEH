<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Without Priors – Statistics for Engineers in a Hurry (an Illustrated Primer)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e8e9342e845377dfc7513ec3bb59cd67.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content=".">
<script src="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/run-python-blocks.js" type="module"></script>
<link href="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/shinylive.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Statistics for Engineers in a Hurry (an Illustrated Primer)</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-parameter_estimation.html">Likelihood of Model</a></li><li class="breadcrumb-item"><a href="./3.1-without_priors.html">Without Priors</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./2-data_generation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.1-discrete_probability_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discrete Probability Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.2-continuous-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Continuous Probability Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.3-statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.4-machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./3-parameter_estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Likelihood of Model</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-without_priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Without Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-with_priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">With Priors</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preview" id="toc-preview" class="nav-link active" data-scroll-target="#preview">Preview</a></li>
  <li><a href="#guessing-the-best-model-parameter" id="toc-guessing-the-best-model-parameter" class="nav-link" data-scroll-target="#guessing-the-best-model-parameter">Guessing the Best Model Parameter</a></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood">Likelihood</a>
  <ul class="collapse">
  <li><a href="#calculating-py_i-mid-theta" id="toc-calculating-py_i-mid-theta" class="nav-link" data-scroll-target="#calculating-py_i-mid-theta">Calculating <span class="math inline">\(P(y_i \mid \theta)\)</span></a></li>
  </ul></li>
  <li><a href="#estimating-maximum-likelihood-with-discrete-data" id="toc-estimating-maximum-likelihood-with-discrete-data" class="nav-link" data-scroll-target="#estimating-maximum-likelihood-with-discrete-data">Estimating Maximum Likelihood with Discrete Data</a>
  <ul class="collapse">
  <li><a href="#reinforcement-for-calculating-py_i-mid-theta" id="toc-reinforcement-for-calculating-py_i-mid-theta" class="nav-link" data-scroll-target="#reinforcement-for-calculating-py_i-mid-theta">Reinforcement for Calculating <span class="math inline">\(P(y_i \mid \theta)\)</span></a></li>
  <li><a href="#other-things-to-note" id="toc-other-things-to-note" class="nav-link" data-scroll-target="#other-things-to-note">Other Things to Note</a></li>
  </ul></li>
  <li><a href="#computational-strategies-for-finding-the-maximum-likelihood-estimate" id="toc-computational-strategies-for-finding-the-maximum-likelihood-estimate" class="nav-link" data-scroll-target="#computational-strategies-for-finding-the-maximum-likelihood-estimate">Computational Strategies for finding the Maximum Likelihood Estimate</a>
  <ul class="collapse">
  <li><a href="#grid-search" id="toc-grid-search" class="nav-link" data-scroll-target="#grid-search">Grid Search</a></li>
  <li><a href="#gradient-ascent" id="toc-gradient-ascent" class="nav-link" data-scroll-target="#gradient-ascent">Gradient Ascent</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimate-with-continuous-data" id="toc-maximum-likelihood-estimate-with-continuous-data" class="nav-link" data-scroll-target="#maximum-likelihood-estimate-with-continuous-data">Maximum Likelihood Estimate with Continuous Data</a>
  <ul class="collapse">
  <li><a href="#normal-distribution-with-mle-estimate" id="toc-normal-distribution-with-mle-estimate" class="nav-link" data-scroll-target="#normal-distribution-with-mle-estimate">Normal Distribution with MLE Estimate</a></li>
  </ul></li>
  <li><a href="#statistical-modelslinear-regression" id="toc-statistical-modelslinear-regression" class="nav-link" data-scroll-target="#statistical-modelslinear-regression">Statistical Models/Linear Regression</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-with-constant-normal-variance" id="toc-maximum-likelihood-with-constant-normal-variance" class="nav-link" data-scroll-target="#maximum-likelihood-with-constant-normal-variance">Maximum Likelihood with Constant Normal Variance</a></li>
  </ul></li>
  <li><a href="#motivation-for-bayesian-with-prior-methods" id="toc-motivation-for-bayesian-with-prior-methods" class="nav-link" data-scroll-target="#motivation-for-bayesian-with-prior-methods">Motivation for Bayesian (With Prior) Methods</a></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning">Machine Learning</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#comparison-to-likelihood-function" id="toc-comparison-to-likelihood-function" class="nav-link" data-scroll-target="#comparison-to-likelihood-function">Comparison to Likelihood Function</a></li>
  <li><a href="#training-a-simple-neural-network" id="toc-training-a-simple-neural-network" class="nav-link" data-scroll-target="#training-a-simple-neural-network">Training a Simple Neural Network</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-parameter_estimation.html">Likelihood of Model</a></li><li class="breadcrumb-item"><a href="./3.1-without_priors.html">Without Priors</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Without Priors</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="preview" class="level2">
<h2 class="anchored" data-anchor-id="preview">Preview</h2>
<p>We finally flip to <span class="math inline">\(\mathcal{L}(M|D)\)</span> the likelihood of a <span style="text-decoration: underline;">model</span> given the observed <span style="text-decoration: underline;">data</span>. In practice, we typically find <span class="math inline">\(\mathcal{L}(M|D)\)</span> so that we can find the <em>best</em> parameter values for our chosen model type, although it could also be used to select between model types. We will describe what <em>best</em> is soon.</p>
<p>Like before, we begin with a simple dice model as we consider it to be an intuitive subject. We then work forward in generally the same order as the first half of the primer. We work with probability distributions and how to find the most likely parameters given the observed data. We pause to examine how likely we are to calculate one set of parameters when the true process has another set of parameters. As usual we use computation to avoid misunderstood analytical solutions.</p>
<p>Throughout the chapter we assume no prior knowledge of what the best model parameters may be - although in an app where you can guess and check you are probably not guessing randomly, which is already a hint at what a prior would be. In the next chapter we’ll formalize how to find optimal models when we have prior information about the parameter values. In more statistical terms, this chapter utilizes a <strong>Frequentist</strong> perspective and the next a Bayesian perspective.</p>
</section>
<section id="guessing-the-best-model-parameter" class="level2">
<h2 class="anchored" data-anchor-id="guessing-the-best-model-parameter">Guessing the Best Model Parameter</h2>
<p>In part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we use observed data to find the most likely model of the data generating process. Intuitively, this is just a model that generates or predicts data that is similar to our observed data. Subsequently we start with an intuitive excercise before diving further into the technical details.</p>
<p>The simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

# --- Precompute the "permanent" histogram for 7 dice, 10,000 rolls ---
FIXED_NUM_DICE = 7
FIXED_NUM_ROLLS = 10000

fixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]
fixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)

app_ui = ui.page_fluid(
    ui.h2("Dice Rolling Demo"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "num_dice",
                "Number of Dice (1–10)",
                min=1,
                max=10,
                value=2,
                step=1
            ),
        ),
        ui.output_plot("dicePlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def user_sums():
        # Always roll the user-selected dice 10,000 times
        N_ROLLS = 10000
        n_dice = input.num_dice()
        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]
        return rolls

    @output
    @render.plot
    def dicePlot():
        # Get the user’s histogram
        sums = user_sums()
        user_unique_vals, user_counts = np.unique(sums, return_counts=True)
        
        # Determine the union of x-values (totals) so both histograms can share the same axis
        all_x = np.arange(
            min(fixed_unique_vals[0], user_unique_vals[0]),
            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1
        )
        
        # Convert the unique/value arrays to dictionaries for easy indexing
        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))
        user_map = dict(zip(user_unique_vals, user_counts))
        
        # Pull out frequency or 0 if total not present in the distribution
        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]
        user_freqs  = [user_map.get(x, 0) for x in all_x]
        
        # Plot
        fig, ax = plt.subplots()
        
        # Bar chart for the fixed 7-dice histogram
        ax.bar(all_x, fixed_freqs, color="lightblue", alpha=0.6, label="Fixed Dice")
        
        # Overlay user histogram as points
        ax.scatter(all_x, user_freqs, color="red", marker="o", label="User Selected Dice")
        
        ax.set_title("Update the Input Parameter to Match Observations")
        ax.set_xlabel("Dice Total")
        ax.set_ylabel("Frequency")
        ax.legend()
        
        # Make x-axis tick at every possible total
        plt.xticks(all_x, rotation=90)
        
        return fig

app = App(app_ui, server)</code></pre>
<p>I’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.</p>
</section>
<section id="likelihood" class="level2">
<h2 class="anchored" data-anchor-id="likelihood">Likelihood</h2>
<p>In our earlier dice example, we were just eyeballing the right model parameters to maximize the probability of observing our data under the model. We’d like a more consistent and mathematical way to achieve the same intent, which is maximizing a <strong>likelihood</strong> function. Its general form is:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid \mathbf{y}) = \prod_{i=1}^{N} P(y_i \mid \theta)
\]</span></p>
<ol type="1">
<li><p><span class="math inline">\(\mathcal{L}(\theta \mid \mathbf{y})\)</span> is likelihood of the model parameter[s] <span class="math inline">\(\theta\)</span> given the observed data <span class="math inline">\(\mathbf{y}\)</span>.</p></li>
<li><p><span class="math inline">\(P(y_i \mid \theta)\)</span> is the probability (or probability density) of the <span class="math inline">\(i\)</span>th data point given the parameters.</p></li>
<li><p>The product <span class="math inline">\(\prod_{i=1}^{N}\)</span> assumes that the data points are independent. In practice, we often work with the log-likelihood (i.e., sum of log probabilities) for numerical stability.</p></li>
</ol>
<section id="calculating-py_i-mid-theta" class="level3">
<h3 class="anchored" data-anchor-id="calculating-py_i-mid-theta">Calculating <span class="math inline">\(P(y_i \mid \theta)\)</span></h3>
<p>Performing the likelihood calculation described above hinges on being able to calculate <span class="math inline">\(P(y_i \mid \theta)\)</span>. To start, we assume we are using a model of a data generating process where the model is just a named parametric probability distribution. To have an example, let’s assume we are measuring the reliability of similar manufacturing lines across several different plants. Over the course of a year, there are a wide variety of unplanned shutdowns, with Line A having 7, Line B having 0, Line C having 2, etc. Across all the plants we want to know the most likely value for the rate of unplanned shutdowns, assuming our data follows a Poisson distribution.</p>
<p>So we can proceed with an example calculation, we simply try reasonable candidate model parameters. Based on a brief look at the data we think 5 is a reasonable rate per year, so <span class="math inline">\(\lambda = 5\)</span> for our candidate model. To find <span class="math inline">\(P(y_i \mid \lambda = 5)\)</span> for a single data point, we can use the app below which is similar to apps we used in the first half of the primer. We keep <span class="math inline">\(\lambda = 5\)</span> in the app and try to find <span class="math inline">\(P(y_i=7 \mid \lambda = 5)\)</span>.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 550

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Poisson Distribution Probability Calculator"),
    
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "lambdaInput", "Rate (λ):",
                min=1, max=10, value=5, step=1
            ),
            ui.input_slider(
                "xInput", "Value (y):",
                min=0, max=15, value=5, step=1
            ),
            ui.br(),
            ui.h4("Probability:"),
            ui.output_text("probOutput"),
            width=300
        ),
        
        # Main panel with plot
        ui.output_plot("poissonPlot", height="400px"),
    )
)

def server(input, output, session):
    # Calculate probability P(X = x)
    @reactive.Calc
    def calculate_probability():
        lam = input.lambdaInput()
        x = input.xInput()
        # Using PMF for P(X = x)
        return stats.poisson.pmf(x, lam)

    # Show the probability
    @output
    @render.text
    def probOutput():
        prob = calculate_probability()
        return f"{prob:.4f}"

    # Plot the Poisson PMF with highlighted bar
    @output
    @render.plot
    def poissonPlot():
        lam = input.lambdaInput()
        x_val = input.xInput()
        
        # Generate range of x values for plotting
        x_max = max(int(lam*3), x_val + 10)
        x_vals = np.arange(0, x_max + 1)
        
        # Calculate PMF values
        pmf_vals = stats.poisson.pmf(x_vals, lam)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot all bars
        bars = ax.bar(x_vals, pmf_vals, width=0.8, alpha=0.7, color='lightblue', 
                     edgecolor='blue')
        
        # Highlight the specific bar for P(X = x)
        for i in range(len(x_vals)):
            if x_vals[i] == x_val:
                bars[i].set_color('red')
                bars[i].set_alpha(0.7)
        
        # Add text with probability
        # prob = calculate_probability()
        # ax.text(0.95, 0.95, f'P(X = {x_val}) = {prob:.4f}', 
        #         transform=ax.transAxes, ha='right', va='top',
        #         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # Set integer ticks on x-axis
        if len(x_vals) &gt; 15:
            # Display approximately 10-15 ticks at regular intervals
            step = max(1, len(x_vals) // 10)
            ax.set_xticks(x_vals[::step])
        else:
            ax.set_xticks(x_vals)
            
        # Ensure x-axis limits provide proper spacing
        ax.set_xlim(-0.6, x_max + 0.6)
        
        ax.set_title(f"Poisson Distribution (λ={lam})")
        ax.set_xlabel("y (number of events)")
        ax.set_ylabel("Probability")
        ax.grid(True, alpha=0.3)
        
        # Single legend with clear explanation
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='red', edgecolor='blue', alpha=0.7, label=f'Selected value (y = {x_val})')
        ]
        ax.legend(handles=legend_elements, loc='upper right')
        
        return fig

app = App(app_ui, server)</code></pre>
<p>Using the app <span class="math inline">\(P\)</span> = 0.1044. Once we can find <span class="math inline">\(P(y_i \mid \theta)\)</span> for one data point, we can do it for the rest and calculate <span class="math inline">\(\mathcal{L}(\theta \mid \mathbf{y})\)</span>. Of course, we have not yet <em>maximized</em> that likelihood function.</p>
</section>
</section>
<section id="estimating-maximum-likelihood-with-discrete-data" class="level2">
<h2 class="anchored" data-anchor-id="estimating-maximum-likelihood-with-discrete-data">Estimating Maximum Likelihood with Discrete Data</h2>
<p>If our goal is to get the Maximum Likelihood Estimate of a model’s parameters, the last example left a couple of things out:</p>
<ol type="1">
<li>It did not calculate the likelihood for multiple data points.</li>
<li>It did nothing to help us estimate the parameters that gave the <em>maximum</em> likelihood.</li>
</ol>
<p>We will walk through another example that fills in these blanks. We’ll shift examples slightly, but we’ll still be modeling a data generating process for which simply using a named parametric distribution is a good fit. In this example we have a manufacturing process with a final quality control step. A batch of 1,000 products are manufactured and then tested. The table below shows the number that pass in each batch. We want to find the most likely value for the pass rate - i.e.&nbsp;we want to find the value of the probability (<span class="math inline">\(p\)</span>) parameter in the binomial distribution that gives the maximum likelihood for the model.</p>
<p>Although the app below does not automatically give you the Maximum Likelihood Estimate, it will hopefully make it easy for you to approximate the value. Pick several <span class="math inline">\(p\)</span> values you think are reasonable and for each select ‘Add to plot’.</p>
<div class="content-width">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Batch Number</th>
<th>Passing</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>992</td>
</tr>
<tr class="even">
<td>2</td>
<td>995</td>
</tr>
<tr class="odd">
<td>3</td>
<td>989</td>
</tr>
<tr class="even">
<td>4</td>
<td>998</td>
</tr>
</tbody>
</table>
</div>
<style>
.content-width {
    width: 50%;  /* Adjust as needed */
    margin: auto;
}
</style>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 600

import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render
from scipy.stats import binom

app_ui = ui.page_fluid(
    ui.h3("For data above, what is the most likely pass probability?"),
    
    # Input row
    ui.row(
        ui.column(
            4,
            ui.input_numeric(
                "pInput", "Enter probability (p):",
                value=0.993, min=0, max=1, step=0.001
            ),
        ),
        ui.column(
            4,
            ui.br(),
            ui.br(),
            ui.input_action_button("addBtn", "Add to Plot"),
            ui.input_action_button("resetBtn", "Reset", class_="btn-danger ms-2"),
        ),
        ui.column(
            4,
            ui.input_action_button("showMleBtn", "Show MLE", class_="btn-primary"),
            ui.output_text("mleOutput"),
        ),
    ),
    
    ui.br(),
    
    # Plots
    ui.row(
        ui.column(
            6,
            ui.output_plot("batchPlot"),
        ),
        ui.column(
            6,
            ui.output_plot("probPlot"),
        ),
    ),
)

def server(input, output, session):
    # Data for the four batches
    batch_data = [(992, 1000), (995, 1000), (989, 1000), (998, 1000)]  # (successes, trials)
    
    # Store the user-selected probabilities and their log-likelihoods
    stored_probs = reactive.Value([])
    stored_lls = reactive.Value([])
    
    # Flag to control MLE visibility
    show_mle = reactive.Value(False)
    
    # Calculate MLE estimate
    total_success = sum(x[0] for x in batch_data)
    total_trials = sum(x[1] for x in batch_data)
    mle_p = total_success / total_trials
    
    @output
    @render.text
    def mleOutput():
        if show_mle.get():
            return f"p = {mle_p:.6f}"
        return ""
    
    # Reset function
    @reactive.Effect
    @reactive.event(input.resetBtn)
    def _():
        stored_probs.set([])
        stored_lls.set([])
        show_mle.set(False)
    
    # Toggle MLE display when button is clicked
    @reactive.Effect
    @reactive.event(input.showMleBtn)
    def _():
        show_mle.set(True)
    
    # Add new probability when button is clicked
    @reactive.Effect
    @reactive.event(input.addBtn)
    def _():
        p = input.pInput()
        if p not in stored_probs.get():
            probs = list(stored_probs.get())
            probs.append(p)
            stored_probs.set(probs)
            
            # Calculate cumulative log likelihood for this p
            total_ll = 0
            batch_lls = []
            for success, trials in batch_data:
                ll = binom.logpmf(success, trials, p)
                total_ll += ll
                batch_lls.append(total_ll)
            
            lls = list(stored_lls.get())
            lls.append((p, batch_lls))
            stored_lls.set(lls)
    
    @output
    @render.plot
    def batchPlot():
        fig, ax = plt.subplots(figsize=(8, 6))
        
        for p, batch_lls in stored_lls.get():
            ax.plot(range(1, 5), batch_lls, 'o-', label=f'p={p:.3f}')
        
        ax.set_xlabel('Batch Number')
        ax.set_ylabel('Cumulative Log Probability')
        ax.set_title('Cumulative Log Probability by Batch')
        ax.grid(True)
        
        # Set x-axis ticks to integers
        ax.set_xticks(range(1, 5))
        
        if stored_lls.get():
            ax.legend()
        
        return fig
   
    @output
    @render.plot
    def probPlot():
        fig, ax = plt.subplots(figsize=(8, 6))
        
        if stored_lls.get():
            # Sort the data by probability before plotting
            sorted_data = sorted(stored_lls.get(), key=lambda x: x[0])
            probs = [x[0] for x in sorted_data]
            final_lls = [x[1][-1] for x in sorted_data]
            
            ax.plot(probs, final_lls, 'o-')
            
            # Only show MLE line if button has been clicked
            if show_mle.get():
                ax.axvline(mle_p, color='red', linestyle='--', 
                          label=f'MLE (p={mle_p:.3f})')
        
        ax.set_xlabel('Probability (p)')
        ax.set_ylabel('Total Log Probability')
        ax.set_title('Total Log Probability vs. p')
        ax.grid(True)
        if stored_lls.get():
            ax.legend()
        
        return fig

app = App(app_ui, server)</code></pre>
<p>Playing around with the app you should be able to see that the chart on the right, the ‘Total Log Probability vs p.’ has an obvious peak. What we want are parameter values that are the most probable, i.e.&nbsp;the ones with the largest total log probability. Here we can fairly easily pick random values, and maybe use some hints from the slope of the graph, to find a good estimate of <span class="math inline">\(p\)</span>. That strategy, which combines some guessing with observing the slope to find the most probable model is actually quite good - many state of the art methods are effectively very sophisticated versions of that same strategy.</p>
<section id="reinforcement-for-calculating-py_i-mid-theta" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-for-calculating-py_i-mid-theta">Reinforcement for Calculating <span class="math inline">\(P(y_i \mid \theta)\)</span></h3>
<p>Let’s again walkthrough calculating <span class="math inline">\(P(y_i \mid \theta)\)</span> for one data point but with more detail. If we can do this successfully the other calculations are generally straightforward (and in practice are taken care of by a software package). Let’s remind ourselves how to calculate the probability of an observation that follows the Binomial probability distribution:</p>
<p><span class="math display">\[
P(y_i = k | n, p) = \binom{n}{k} p^k (1 - p)^{n - k}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(k\)</span> is the observed number of successes in <span class="math inline">\(n\)</span> trials.</li>
<li><span class="math inline">\(p\)</span> is the probability of success in each trial.</li>
<li>(the n over k is ‘n choose k’, the binomial coefficient)</li>
</ul>
<p>Our first data point is 992 successes, which equals <span class="math inline">\(k\)</span>, out of 1,000 trials, which equals <span class="math inline">\(n\)</span>. The best value of <span class="math inline">\(p\)</span> is what we are unsure about, and our current strategy is to strategically try some values to try to understand the problem. Let’s assume we tried <span class="math inline">\(p\)</span> = 0.995 as a reasonable guess. The calculation is:</p>
<p><span class="math display">\[
P(y_i = 992 | n = 1000, p = 0.995) =
\binom{1000}{992} (0.995)^{992} (0.005)^8 = 0.0652
\]</span></p>
<p>The result is that the 992 data point has a probability of 0.0652, if we assume n=1000 and p=0.995. That value is about -2.73 using the natural log or -1.19 using log base 10. In the app above, if you select p=0.995 and select ‘Add to Plot’, the value of the first point/batch, which corresponds to k=992, has a value of about -2.73.</p>
</section>
<section id="other-things-to-note" class="level3">
<h3 class="anchored" data-anchor-id="other-things-to-note">Other Things to Note</h3>
<section id="difference-between-mathcallmd-and-pdm" class="level4">
<h4 class="anchored" data-anchor-id="difference-between-mathcallmd-and-pdm">Difference between <span class="math inline">\(\mathcal{L}(M|D)\)</span> and <span class="math inline">\(P(D|M)\)</span></h4>
<p>You may be asking yourself - what is the difference between what we are doing here and what we did in the first half of the primer when we calculated P(D|M)?</p>
<p>The difference is we have a goal. We use the probability of the data as a metric we can optimize to find the best model parameters. We optimize the parameters until we find the best model, i.e the one with the Maximum Likelihood Estimate. So in summary, the calculation of the probabilities is the same - the difference is we use an optimization strategy to find the best <span class="math inline">\(\mathcal{L}(M|D)\)</span>.</p>
</section>
<section id="fyi-on-the-analytical-solution" class="level4">
<h4 class="anchored" data-anchor-id="fyi-on-the-analytical-solution">FYI on the Analytical Solution</h4>
<p>Although we’ve mostly avoided them, analytical solutions can be very useful. For one they are typically very fast to compute. And secondly, they can be a nice verification that approximate/numerical methods converge to reasonable solutions, for at least some set of circumstances.</p>
<p>For the Binomial distribution there’s actually a simple analytical solution, and while it may seem simple in hindsight, the proper derivation is maybe more complex than you’d expect… The basic idea is that the derivative of the ‘Total Log Probability vs p.’ chart will equal zero at the maximum probability. After a brief drum roll we can reveal that the most likely estimate of p is ……. just the average success rate. More precisely, the successes divided by the total n, e.g.&nbsp;(992 + 995 + 989 + 998)/(1,000 + 1,000 + 1,000 + 1,000).</p>
</section>
</section>
</section>
<section id="computational-strategies-for-finding-the-maximum-likelihood-estimate" class="level2">
<h2 class="anchored" data-anchor-id="computational-strategies-for-finding-the-maximum-likelihood-estimate">Computational Strategies for finding the Maximum Likelihood Estimate</h2>
<p>Analytical solutions are not common for real-world problems, so we consider some computational strategies for finding the Maximum Likelihood Estimate.</p>
<section id="grid-search" class="level3">
<h3 class="anchored" data-anchor-id="grid-search">Grid Search</h3>
<p>Grid search is a straightforward optimization technique that divides the parameter space into a finite set of discrete values. For example, when estimating the parameter <span class="math inline">\(p\)</span> in a binomial model (where <span class="math inline">\(p\)</span> can range from 0 to 1), you might consider values such as 0.01, 0.02, …, 0.98, 0.99. For each candidate value, you compute the likelihood (or log-likelihood) of observing your data. The optimal parameter estimate, denoted as <span class="math inline">\(\hat{p}_{\text{grid}}\)</span>, is the value from the grid <span class="math inline">\(G\)</span> that maximizes the likelihood:</p>
<p><span class="math display">\[
\hat{p}_{\text{grid}} = \arg\max_{p_i \in G} \mathcal{L}(p_i \mid n, k)
\]</span></p>
<p>This notation means that <span class="math inline">\(\hat{p}_{\text{grid}}\)</span> is the value <span class="math inline">\(p_i\)</span> in <span class="math inline">\(G\)</span> for which the likelihood <span class="math inline">\(\mathcal{L}(p_i \mid n, k)\)</span> is highest—in other words, the optimum value derived from the grid search.</p>
<p>While grid search is attractive for its simplicity, it doesn’t scale well with multiple parameters. As you add more parameters, the number of parameter combinations increases exponentially (a phenomenon known as the “curse of dimensionality”). For instance, if you discretize each parameter into 100 values, two parameters would require checking <span class="math inline">\(100^2 = 10,000\)</span> combinations, and three parameters would involve <span class="math inline">\(100^3 = 1,000,000\)</span> combinations, quickly becoming computationally prohibitive.</p>
</section>
<section id="gradient-ascent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-ascent">Gradient Ascent</h3>
<p>To manually estimate the Maximum Likelihood for the binomial probability problem, we created a chart that had an obvious peak at the Maximum Likelihood Estimate. If we want to find that peak we simply need to walk/step up the hill. This is the idea of using gradients/derivatives to make our calculation more efficient.</p>
<p>This approach is excellent even for many parameters, so we jump right to the definition for multi-parameter spaces. We seek to compute the gradient of <span class="math inline">\(f(\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> so we can use it in an optimization algorithm:</p>
<p><span class="math display">\[
\nabla f(\theta) = \left[ \frac{\partial f}{\partial \theta_1}, \frac{\partial f}{\partial \theta_2}, \dots, \frac{\partial f}{\partial \theta_n} \right]
\]</span></p>
<p>We use the resulting gradient to take a step towards the nearest optimum with learning rate <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla f(\theta^{(t)})
\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the iteration index.</p>
<p>The use of gradients/derivatives forms a core piece of most modern optimization methods. The methods that use these gradients/derivatives are so effective, practitioners will go to great lengths to ensure that the gradients/derivatives can be calculated for their problem. This fact has given birth to many ‘autograd’ libraries that efficiently find derivatives in very complex problem domains. Modern neural networks are possible due to efficient matrix multiplication and gradient/derivative estimation (well, and GPU’s that go brrrrrrr).</p>
</section>
</section>
<section id="maximum-likelihood-estimate-with-continuous-data" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimate-with-continuous-data">Maximum Likelihood Estimate with Continuous Data</h2>
<p>We move on to continuous data and continuous probability distributions, but with the same goal of finding the model parameters that maximize the likelihood of the observed data. The methods remain the same. The only difference we should keep in mind is that when we calculate <span class="math inline">\(P(y_i \mid \theta)\)</span>, with a continuous probability distribution, it is a probability density, and not a true probability. However, since probability densities can still be used to compare relative probability, and we are only trying to find the model parameter values that are the most likely relative to all other possibilities, this nuance has little practical impact.</p>
<section id="normal-distribution-with-mle-estimate" class="level3">
<h3 class="anchored" data-anchor-id="normal-distribution-with-mle-estimate">Normal Distribution with MLE Estimate</h3>
<p>The following is a very similar app to what we saw in the chapter on Continuous Probability Distributions, where we just calculated the relative probability of continuous data. However, this time we also have ability to find the most likely model parameter values to explain the data, i.e the Maximimum Likelihood Estimate.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_sidebar(
    # Sidebar with user inputs (labels removed)
    ui.sidebar(
        ui.input_slider(
            "muInput", "Mean (μ):",
            min=90, max=110, value=103, step=0.1
        ),
        ui.input_slider(
            "varInput", "Variance (σ²):",
            min=1, max=20, value=7, step=0.1
        ),
        ui.hr(),
        ui.input_action_button("mleBtn", "MLE", class_="btn-primary"),
        ui.input_action_button("addToMlePlotBtn", "Add to MLE Plot", class_="btn-info"),
        ui.input_action_button("resetBtn", "Reset", class_="btn-warning"),
    ),
    
    # Main panel with tabs for plots
    ui.panel_title("Likelihood Calculation"),
    
    ui.navset_tab(
        ui.nav_panel("Normal PDF", 
            # Moved Current Data and Log-Likelihood here
            ui.row(
                ui.column(6,
                    ui.h4("Current Data (Y):"),
                    ui.output_text_verbatim("dataText")
                ),
                ui.column(6,
                    ui.h4("Current Log-Likelihood:"),
                    ui.output_text_verbatim("logLikelihoodText")
                )
            ),
            ui.output_plot("normalPlot", height="450px")
        ),
        ui.nav_panel("Cumulative Log Probability", 
            ui.output_plot("cumLogProbPlot", height="500px")
        ),
        ui.nav_panel("MLE Estimate",
            ui.output_plot("mlePlot", height="500px")
        )
    )
)

def server(input, output, session):
    # Fixed number of samples
    SAMPLE_SIZE = 7
    
    # Initialize data with 7 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=SAMPLE_SIZE)
    )
    
    # Store MLE points for the MLE plot - each point is (mu, var, likelihood)
    mle_points = reactive.Value([])

    # Generate a new sample and reset MLE points when 'Reset' is pressed
    @reactive.Effect
    @reactive.event(input.resetBtn)
    def _():
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=SAMPLE_SIZE)
        )
        mle_points.set([])

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # Show the log-likelihood
    @output
    @render.text
    def logLikelihoodText():
        ll = log_likelihood()
        return f"{round(ll, 2)}"

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Add current mu, var, and likelihood to MLE points
    @reactive.Effect
    @reactive.event(input.addToMlePlotBtn)
    def add_to_mle_plot():
        mu = input.muInput()
        var = input.varInput()
        ll = log_likelihood()
        
        # Create a new list with the current points plus the new one
        current_points = mle_points()
        new_points = current_points.copy()
        new_points.append((mu, var, ll))
        mle_points.set(new_points)

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var &lt;= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)
        plt.tight_layout()

        return fig

    # Plot the cumulative log probability
    @output
    @render.plot
    def cumLogProbPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        # Sort data points for plotting
        sorted_indices = np.argsort(y)
        sorted_y = y[sorted_indices]
        
        # Calculate log probabilities
        log_probs = -0.5 * np.log(2 * math.pi * var) - 0.5 * ((sorted_y - mu) / sigma)**2
        cum_log_probs = np.cumsum(log_probs)

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(sorted_y, cum_log_probs, 'b-', marker='o')
        ax.set_title("Cumulative Log Probability")
        ax.set_xlabel("Data Points")
        ax.set_ylabel("Cumulative Log Probability")
        ax.grid(True)
        plt.tight_layout()

        return fig
    
    # Plot the MLE estimates
    @output
    @render.plot
    def mlePlot():
        points = mle_points()
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        if points:
            mu_values = [p[0] for p in points]
            var_values = [p[1] for p in points]
            ll_values = [p[2] for p in points]
            
            # Create a scatter plot with color based on log-likelihood
            scatter = ax.scatter(mu_values, var_values, c=ll_values, cmap='viridis', 
                      s=100, alpha=0.7, edgecolors='k')
            
            # Add colorbar
            cbar = fig.colorbar(scatter)
            cbar.set_label('Log-Likelihood')
            
            # Add numbers to points
            for i, (x, y) in enumerate(zip(mu_values, var_values)):
                ax.text(x, y, str(i+1), color='white', ha='center', va='center', fontweight='bold')
            
            # Annotate the maximum likelihood point
            max_idx = np.argmax(ll_values)
            ax.scatter(mu_values[max_idx], var_values[max_idx], s=200, 
                      facecolors='none', edgecolors='red', linewidths=2)
            ax.text(mu_values[max_idx] + 2, var_values[max_idx], 
                   f"Max: ({mu_values[max_idx]:.1f}, {var_values[max_idx]:.1f}, L={ll_values[max_idx]:.2f})",
                   color='red', fontweight='bold')
        else:
            ax.text(0.5, 0.5, "Click 'Add to MLE Plot' to add points", 
                   ha='center', va='center', transform=ax.transAxes, fontsize=14)
        
        ax.set_title("MLE Estimate Plot")
        ax.set_xlabel("Mean (μ)")
        ax.set_ylabel("Variance (σ²)")
        ax.grid(True)
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
</code></pre>
<p>It’s quite a bit harder to optimize two parameters by hand, but you can always see the Maximium Likelihood Estimate by clicking the ‘MLE’ button. The true distribution from which the data was generated had a mean of 100 and a standard deviation of 10. You may notice that with just 7 data points, the Maximum Likelihood Estimate can vary notably from those values.</p>
<p>Also, while there are some good reasons in statistical analysis to default to using normal/gaussian distributions, it’s somewhat dangerous to the extent it has implicitly permeated many analyses. Please just keep this in mind when tackling ‘real-world’ problems.</p>
</section>
</section>
<section id="statistical-modelslinear-regression" class="level2">
<h2 class="anchored" data-anchor-id="statistical-modelslinear-regression">Statistical Models/Linear Regression</h2>
<p>So far this chapter we’ve focused on models/data generating processes that could be adequately described with just parametric probability distributions. Here we move on to statistical models conditioned on data, e.g.&nbsp;<span class="math inline">\(P(y_i | x_i)\)</span>, like we did in the first half of the primer. As a reminder, data allowed us to zoom in on a part of the sample space and make better estimates of the parameters of the probability distribution at that location. Again, as a reminder, here was our preferred notation for traditional (constant variance) linear regression:</p>
<p><span class="math display">\[
P(y_i \mid x_i) \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2)
\]</span></p>
<p>This notation makes it clear that our estimate for <span class="math inline">\(y_i\)</span> is a probability distribution centered around the linear predictor <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>. This formulation also highlights that while our model provides a deterministic estimate for the mean response at <span class="math inline">\(x_i\)</span>, individual observations vary due to randomness captured by the [constant] variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If we define the predicted value as <span class="math inline">\(\hat{y}_i = \beta_0 + \beta_1 x_i\)</span>, then the error for each observation is</p>
<p><span class="math display">\[
\epsilon_i = y_i - \hat{y}_i,
\]</span></p>
<p>Where <span class="math inline">\(y_i\)</span> is the observed data and <span class="math inline">\(\hat{y}_i\)</span> is the prediction. By our model assumption, these errors follow a normal distribution:</p>
<p><span class="math display">\[
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
\]</span></p>
<p>This simply means we assume the error (difference between data point and prediction) are centered at our prediction (the mean equals zero), and have a variance of <span class="math inline">\(\sigma^2\)</span>. We can now find <span class="math inline">\(P(y_i \mid \theta)\)</span> for each data point, which means we can also find the Maximum Likelihood Estimate of the model parameters (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>… <span class="math inline">\(\sigma\)</span>).</p>
<p>A visual here can be very helpful. For each data point we calculate its probability like in the chart below. Every observation has a ‘sideways’ normal distribution centered (<span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>) at the prediction, and the probability of the point is based on how far out it would land on that normal distribution.</p>
<div id="f97a3848" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3.1-without_priors_files/figure-html/cell-2-output-1.png" width="855" height="662" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="maximum-likelihood-with-constant-normal-variance" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-with-constant-normal-variance">Maximum Likelihood with Constant Normal Variance</h3>
<p>We take an unusual detour for the primer and derive the solution for maximizing likelihood with constant normal variance. This is important because of the result, as indicated by the callout at the end of the derivation. Also, rest assured that derivations are not needed to solve practical problems, solutions can be found numerically by just maximizing the results of the raw/original likelihood calculation (like you did in earlier examples).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sum of Squared Errors Derivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The probability density function (PDF) of a normal distribution is:</p>
<p><span class="math display">\[
p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x_i - \mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>The likelihood function for the entire dataset is the product of individual probabilities:</p>
<p><span class="math display">\[
L(\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x_i - \mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>By taking the natural logarithm and rearraging we can arrive at:</p>
<p><span class="math display">\[
\log L(\mu, \sigma^2) = -\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2
\]</span></p>
<p>If we assume constant variance the first term, <span class="math inline">\(-\frac{n}{2} \log (2\pi\sigma^2)\)</span> and <span class="math inline">\(\frac{1}{2\sigma^2}\)</span> is constant. So we simply need to minimize the second term, which is the <strong>sum of squared errors (SSE)</strong>:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} (x_i - \mu)^2
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is very common to see optimizations that minimize the squared error. You should recognize that this is implicitly assuming that the errors are normally distributed.</p>
</div>
</div>
<section id="example-app" class="level4">
<h4 class="anchored" data-anchor-id="example-app">Example App</h4>
<p>The app below allows you to visualize the sum of squares error. You can try minimizing the total squared error by altering the parameters of the regression. When you are ready you can see the parameter values that maximizes the likelihood by clicking on the MLE button.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 800

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_sidebar(
    # Sidebar with all user inputs
    ui.sidebar(
        ui.h3("Model Parameters"),
        ui.input_slider(
            "alphaInput", "Intercept (α):",
            min=-10, max=10, value=0, step=0.1
        ),
        ui.input_slider(
            "betaInput", "Slope (β):",
            min=-5, max=5, value=1, step=0.1
        ),
        ui.input_slider(
            "varInput", "Variance (σ²):",
            min=0.01, max=2, value=1, step=0.01
        ),
        ui.input_slider(
            "nInput", "Number of samples:",
            min=5, max=20, value=10, step=1
        ),
        ui.input_checkbox("showResiduals", "Show residuals as squares", value=True),
        ui.br(),
        ui.row(
            ui.column(
                6,
                ui.input_action_button("mleBtn", "MLE", class_="btn-primary"),
            ),
            ui.column(
                6,
                ui.input_action_button("newSampleBtn", "NEW SAMPLE", class_="btn-secondary"),
            ),
        ),
        width="300px"
    ),
    
    # Main content
    ui.h2("Linear Regression Likelihood"),
    
    ui.row(
        ui.column(
            6,
            ui.h4("Current Data (X, Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            6,
            ui.h4("Model Statistics:"),
            ui.output_text("statsOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("regressionPlot", height="500px"),
)

def server(input, output, session):
    # Reactive value to store X and Y
    data_vals = reactive.Value(None)

    # Function to generate linear-regression data
    def generate_data(n, alpha, beta, var):
        # For simplicity, let X be a random uniform(0, 10)
        X = np.random.uniform(0, 10, size=n)
        # Y = alpha + beta*X + noise
        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)
        return X, Y

    # Initialize data once
    data_vals.set(generate_data(10, 0, 1, 1))

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()
        data_vals.set(generate_data(n, alpha, beta, var))

    # Display the current data
    @output
    @render.text
    def dataText():
        X, Y = data_vals()
        # Show a few decimal places
        pairs_str = [
            f"({round(xi,1)}, {round(yi,1)})" for xi, yi in zip(X, Y)
        ]
        return ", ".join(pairs_str)

    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        X, Y = data_vals()
        n = len(Y)

        # Compute MLE (which in classical linear regression is the OLS solution)
        X_mean = np.mean(X)
        Y_mean = np.mean(Y)

        # beta_hat = Cov(X,Y)/Var(X)
        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)

        # alpha_hat = mean(Y) - beta_hat*mean(X)
        alpha_hat = Y_mean - beta_hat*X_mean

        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)
        residuals = Y - (alpha_hat + beta_hat*X)
        var_hat = np.sum(residuals**2) / n

        # Update the UI sliders
        session.send_input_message("alphaInput", {"value": alpha_hat})
        session.send_input_message("betaInput", {"value": beta_hat})
        session.send_input_message("varInput", {"value": var_hat})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        X, Y = data_vals()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()
        n = len(Y)

        if var &lt;= 0:
            return float("nan")

        # Compute sum of squared residuals
        residuals = Y - (alpha + beta*X)
        ssr = np.sum(residuals**2)

        # log-likelihood for linear regression
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * (ssr / var)
        return term1 + term2

    # Calculate sum of squared errors
    @reactive.Calc
    def sum_squared_errors():
        X, Y = data_vals()
        alpha = input.alphaInput()
        beta = input.betaInput()
        
        residuals = Y - (alpha + beta*X)
        return np.sum(residuals**2)

    # Show the log-likelihood and SSE
    @output
    @render.text
    def statsOutput():
        ll = log_likelihood()
        sse = sum_squared_errors()
        return ui.HTML(f"Log-Likelihood: {round(ll, 2)}, and the Sum of Squared Errors: {round(sse, 2)}")

    # Plot the data and the regression line
    @output
    @render.plot
    def regressionPlot():
        X, Y = data_vals()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()
        show_residuals = input.showResiduals()

        fig, ax = plt.subplots(figsize=(10, 6))

        # Plot data points
        ax.scatter(X, Y, color="blue", alpha=0.7, label="Data")

        # Plot regression line from min(X) to max(X)
        x_min, x_max = np.min(X), np.max(X)
        x_vals = np.linspace(x_min, x_max, 100)
        y_vals = alpha + beta * x_vals
        ax.plot(x_vals, y_vals, color="red", label=f"Line (α={round(alpha,2)}, β={round(beta,2)})")

        # Draw residual squares if enabled
        if show_residuals:
            y_pred = alpha + beta * X
            residuals = Y - y_pred
            
            for x, y, res in zip(X, Y, residuals):
                y_on_line = y - res  # Point on the regression line
                
                # Create square vertices
                if res &gt;= 0:  # Point above the line (positive residual)
                    square = plt.Rectangle((x, y_on_line), abs(res), abs(res), 
                                        fill=False, color='green', alpha=0.7)
                else:  # Point below the line (negative residual)
                    square = plt.Rectangle((x, y), abs(res), abs(res), 
                                        fill=False, color='green', alpha=0.7)
                ax.add_patch(square)

        ax.set_title("Linear Regression Fit")
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.legend()
        ax.grid(True)

        return fig

app = App(app_ui, server)</code></pre>
</section>
</section>
</section>
<section id="motivation-for-bayesian-with-prior-methods" class="level2">
<h2 class="anchored" data-anchor-id="motivation-for-bayesian-with-prior-methods">Motivation for Bayesian (With Prior) Methods</h2>
<p>If you’ve followed the general development of topics from the first half of the primer, this is where the discussion on Bayesian models slots in. We have, however, dedicated a separate chapter to this topic which is finding the likelihood of models ‘With Priors’.</p>
<p>Regardless, we stop a moment to contemplate the need for these <em>Bayesian</em> methods. ‘Priors’ refers to having prior information (or beliefs) about the parameter values. This can be very useful, especially if you understand the data generating process but have not been able to collect a large amount of data. They can be controversial, however, if not well defended. These points are long discussed in statistical literature.</p>
<p>The point we would like to make here is that once you get past some reasonable starting point for the prior parameter values, you open up an extremely attractive computational framework. In this framework the model parameter values are sampled probabilistically - you do not simply find a single most likely value at the top of the hill - you find a range of parameter values with different plausibilities <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. A major advantage is the methods to find optimum parameter values end up being <em>extremely general</em>. You can define all sorts of custom models, and modern Bayesian sampling will give you plausible parameter values for your models, including the spread/confidence in those parameter values.</p>
<p>But again, we wait a chapter.</p>
</section>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine Learning</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section is incomplete. It’s interesting material, but for the core purpose of the primer, it’s hard to prioritize for a v0 release.</p>
</div>
</div>
<p>We again visit machine learning, but now have additional background/terminology for distinguishing it from the statistical models we’ve otherwise discussed.</p>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>The parameters of a machine learning model are optimized via a <strong>loss function</strong>. A loss function, denoted as <span class="math inline">\(L\)</span>, is typically computed as the difference between the true target values <span class="math inline">\(y\)</span> (it’s machine learning so we say target instead of observed…) and the predicted values <span class="math inline">\(\hat{y}\)</span>. The generic form of a loss function can be expressed as:</p>
<p><span class="math display">\[
L(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} \ell(y_i, \hat{y}_i)
\]</span></p>
<section id="variables" class="level4">
<h4 class="anchored" data-anchor-id="variables">Variables:</h4>
<ul>
<li><span class="math inline">\(y_i\)</span>: The true value for the (i)-th data point.</li>
<li><span class="math inline">\(\hat{y}_i\)</span>: The predicted value for the (i)-th data point.</li>
<li><span class="math inline">\(\ell(y_i, \hat{y}_i)\)</span>: The individual error for the (i)-th data point defined by the loss function.</li>
<li><span class="math inline">\(N\)</span>: The total number of data points.</li>
</ul>
<p>The function <span class="math inline">\(\ell(y_i, \hat{y}_i)\)</span> depends on the task.</p>
</section>
</section>
<section id="comparison-to-likelihood-function" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-likelihood-function">Comparison to Likelihood Function</h3>
<p>Loss functions are more general than the Maximum Likelihood Estimate usually used in statistical models. They may or may not be based on probabilities and may or may not be equivalent to the Maximum Likelihood Estimate.</p>
</section>
<section id="training-a-simple-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="training-a-simple-neural-network">Training a Simple Neural Network</h3>
<p>In the first half we developed the structure of a neural network and let it generate data, but we did not train it. Training a neural network, is unfortunately, probably the hardest part of the whole package.</p>
<section id="model-architecture" class="level4">
<h4 class="anchored" data-anchor-id="model-architecture">Model Architecture</h4>
<p>The neural network we’ll eventually train in the app further below consists of:</p>
<ul>
<li>Input layer: 1 neuron (diamond carat)</li>
<li>Hidden layer 1: <span class="math inline">\(h_1\)</span> neurons (user-configurable)</li>
<li>Hidden layer 2: <span class="math inline">\(h_2\)</span> neurons (user-configurable)</li>
<li>Output layer: 1 neuron (predicted price)</li>
</ul>
</section>
<section id="loss-function-1" class="level4">
<h4 class="anchored" data-anchor-id="loss-function-1">Loss Function</h4>
<p>The model minimizes the Mean Squared Error (MSE) loss:</p>
<p><span class="math display">\[\mathcal{L}(\mathbf{W}, \mathbf{b}) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2\]</span></p>
<p>Where: - <span class="math inline">\(m\)</span> is the number of training examples - <span class="math inline">\(\hat{y}^{(i)}\)</span> is the predicted value for the <span class="math inline">\(i\)</span>-th example - <span class="math inline">\(y^{(i)}\)</span> is the actual value for the <span class="math inline">\(i\)</span>-th example</p>
</section>
<section id="parameter-estimation-process" class="level4">
<h4 class="anchored" data-anchor-id="parameter-estimation-process">Parameter Estimation Process</h4>
<section id="gradient-descent" class="level5">
<h5 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h5>
<p>The parameters are updated iteratively using gradient descent:</p>
<p><span class="math display">\[\mathbf{W}^{[l]} = \mathbf{W}^{[l]} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}}\]</span> <span class="math display">\[\mathbf{b}^{[l]} = \mathbf{b}^{[l]} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}}\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is the learning rate (fixed at 0.01 in our implementation).</p>
</section>
<section id="activation-function-derivatives" class="level5">
<h5 class="anchored" data-anchor-id="activation-function-derivatives">Activation Function Derivatives</h5>
<ol type="1">
<li><p><strong>ReLU Derivative</strong>: <span class="math display">\[g^{\prime}_{\text{ReLU}}(z) =
\begin{cases}
1 &amp; \text{if } z &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p></li>
<li><p><strong>Logistic Sigmoid Derivative</strong>: <span class="math display">\[g^{\prime}_{\text{sigmoid}}(z) = \sigma(z)(1-\sigma(z))\]</span> where <span class="math inline">\(\sigma(z) = \frac{1}{1+e^{-z}}\)</span></p></li>
</ol>
</section>
</section>
<section id="initialization-strategy" class="level4">
<h4 class="anchored" data-anchor-id="initialization-strategy">Initialization Strategy</h4>
<p>The weights are initialized using the initialization for ReLU or standard normal scaled by the square root of layer size:</p>
<p><span class="math display">\[\mathbf{W}^{[l]} \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n^{[l-1]}}}\right)\]</span></p>
<p>Where <span class="math inline">\(n^{[l-1]}\)</span> is the number of neurons in the previous layer.</p>
</section>
<section id="training-process" class="level4">
<h4 class="anchored" data-anchor-id="training-process">Training Process</h4>
<ol type="1">
<li><strong>Standardization</strong>: Input and output data are standardized to have zero mean and unit variance</li>
<li><strong>Iterations</strong>: The model is trained for a user-specified number of epochs</li>
<li><strong>Parameter Updates</strong>: After each forward pass, parameters are updated via backpropagation</li>
<li><strong>Convergence</strong>: Training continues until the specified number of epochs is completed</li>
</ol>
<p>The final parameter values displayed in the “Model Parameters” tab represent the optimized weights and biases that minimize the MSE loss on the training data.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# Data (an excerpt from the diamond dataset)
data_str = """ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67
32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69
16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66
32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69
3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48
16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61
5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48
34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67
28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54
32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57
16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71
31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63
51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64
27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71
37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69
16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69
35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71
30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64
36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69
30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69
32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7
35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72
20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64
34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69
33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69
32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68
32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71
44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68
38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74
43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67
33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68
35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72
37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7
33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7
26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71
33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7
36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69
36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74
13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69
30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74
29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73
30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71
46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71
32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7
20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75
34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67
27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72
29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7
30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72
31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74
35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74
50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73
32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75
29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74
37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64
42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75
30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71
23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79
18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78
31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78
34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8
13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79
36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75
37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77
30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73
36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77
34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7
30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9
33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81
26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76
27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76
34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74
28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93
39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96
33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83
36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9
37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87
41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86
35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87
27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83
41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93
37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86
36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88
37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92
38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85
35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89
31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81
30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96
39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85
30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93
17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9
35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88
33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93
39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96
31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94
33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01
41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95
42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88
30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87
41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89
35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98
24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97
35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98
37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98
36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97
28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96
43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91
41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99
37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98
40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94
37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92
39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98
46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15
38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03
41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1
50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13
43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14
35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05
41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01
37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1
37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16
38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16
33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88
43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09
44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13
32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02
39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17
42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05
49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22
39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16
47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05
38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05
49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19
39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16
44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22
47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19
44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23
49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21
41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11
47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15
48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16
41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21
48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17
43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26
46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19
43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25
36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22
44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18
37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17
48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2
44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17
46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16
46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23
50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24
43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38
47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25
41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23
42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35
48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28
52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23
49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28
52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25
50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24
46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22
46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19
45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3
46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31
41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35
45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35
43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26
45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29
42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24
3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47
51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54
50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51
46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32
53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5
1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53
2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47
52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43
52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4
52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48
44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63
46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4
47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5
2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37
48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48
51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57
46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64
49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47
52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46
2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51
277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53
809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47
52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53
946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56
51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48
48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6
51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57
3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59
51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51
694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56
50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57
45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48
49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54
2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55
1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58
50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55
53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55
48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61
3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51
51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53
53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56
7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82
7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8
8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79
1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73
53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66
1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52
580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67
47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59
1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63
53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63
4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74
4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62
1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62
682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6
113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87
3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88
9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86
53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71
4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61
4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89
9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94
8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82
14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97
12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02
3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93
6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7
9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87
4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89
10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93
9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86
13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75
18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81
3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79
5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68
15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98
10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0
5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94
8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91
4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8
9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01
10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83
15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03
5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86
9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04
15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95
7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05
9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94
15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92
11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06
15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95
10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89
4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99
18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0
15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99
16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0
11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94
16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91
11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99
13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95
13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91
9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96
9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02
10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05
12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98
15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0
8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06
17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03
7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03
16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0
11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92
5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03
15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03
7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04
14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0
8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01
7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99
21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04
48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22
16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01
15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08
18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11
7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08
8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07
20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15
13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18
11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06
19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0
16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0
15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05
17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11
14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1
21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04
4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12
16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09
11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05
16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07
21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1
8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11
18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13
13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13
20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16
12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08
14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22
22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23
8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2
18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21
26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22
20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27
12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27
15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11
18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15
16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23
10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24
12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19
7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21
23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22
20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24
20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24
15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23
24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57
26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53
25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49
6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36
10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25
14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39
15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32
12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32
21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61
22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6
20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53
19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52
16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59
19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49
19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31
15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36
24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42
16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38
20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58
21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29
25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77
24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62
22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63
22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55
19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54
20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54
24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59
21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65
23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67
19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45
19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58
25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53
27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75
27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66
25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69
24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64
15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7
25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57
20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6
17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58
24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14
24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69
22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93
27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2
26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""

# --- Activation Functions ---
def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x &gt; 0).astype(float)

def logistic(x):
    return 1 / (1 + np.exp(-x))

def logistic_deriv(x):
    sig = logistic(x)
    return sig * (1 - sig)

# --- Neural Network Class ---
# Now accepts an 'activation' parameter ("relu" or "logistic") and hidden layer size.
class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, activation="relu"):
        self.activation = activation  # "relu" or "logistic"
        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size1))
        
        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)
        self.b2 = np.zeros((1, hidden_size2))
        
        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)
        self.b3 = np.zeros((1, output_size))
    
    def forward(self, X):
        self.Z1 = np.dot(X, self.W1) + self.b1
        if self.activation == "relu":
            self.A1 = relu(self.Z1)
        else:
            self.A1 = logistic(self.Z1)
        
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        if self.activation == "relu":
            self.A2 = relu(self.Z2)
        else:
            self.A2 = logistic(self.Z2)
        
        self.Z3 = np.dot(self.A2, self.W3) + self.b3
        return self.Z3
    
    def compute_loss(self, y_pred, y_true):
        return np.mean((y_pred - y_true)**2)
    
    def backward(self, X, y_true, y_pred, learning_rate):
        m = y_true.shape[0]
        dZ3 = (2.0 / m) * (y_pred - y_true)
        dW3 = np.dot(self.A2.T, dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        
        dA2 = np.dot(dZ3, self.W3.T)
        if self.activation == "relu":
            dZ2 = dA2 * relu_deriv(self.Z2)
        else:
            dZ2 = dA2 * logistic_deriv(self.Z2)
        dW2 = np.dot(self.A1.T, dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        
        dA1 = np.dot(dZ2, self.W2.T)
        if self.activation == "relu":
            dZ1 = dA1 * relu_deriv(self.Z1)
        else:
            dZ1 = dA1 * logistic_deriv(self.Z1)
        dW1 = np.dot(X.T, dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)
        
        # Update parameters.
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
    
    def train(self, X, y, epochs, learning_rate):
        losses = []
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = self.compute_loss(y_pred, y)
            losses.append(loss)
            self.backward(X, y, y_pred, learning_rate)
        return losses
    
    def get_parameters(self):
        """Return a dictionary of model parameters"""
        return {
            "W1": self.W1,
            "b1": self.b1,
            "W2": self.W2,
            "b2": self.b2,
            "W3": self.W3,
            "b3": self.b3
        }

# --- Shiny UI ---
app_ui = ui.page_fluid(
    ui.h2("Neural Network Training"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("epochs", "Number of Training Epochs", min=100, max=5000, value=1000, step=100),
            ui.input_select("activation", "Activation Function", choices=["ReLu", "Logistic"], selected="Logistic"),
            ui.input_slider("hidden_neurons", "Number of Neurons in Hidden Layers", min=1, max=20, value=4, step=1)
        ),
        ui.navset_tab(
            ui.nav_panel("Training Loss",
                ui.output_plot("lossPlot", height="400px")
            ),
            ui.nav_panel("Predictions",
                ui.output_plot("predictionPlot", height="400px")
            ),
            ui.nav_panel("Model Parameters",
                ui.row(
                    ui.column(4, 
                        ui.h4("Layer 1 Weights"),
                        ui.output_table("w1Table")
                    ),
                    ui.column(4, 
                        ui.h4("Layer 2 Weights"),
                        ui.output_table("w2Table")
                    ),
                    ui.column(4, 
                        ui.h4("Layer 3 Weights"),
                        ui.output_table("w3Table")
                    )
                ),
                ui.row(
                    ui.column(4, 
                        ui.h4("Layer 1 Biases"),
                        ui.output_table("b1Table")
                    ),
                    ui.column(4, 
                        ui.h4("Layer 2 Biases"),
                        ui.output_table("b2Table")
                    ),
                    ui.column(4, 
                        ui.h4("Layer 3 Biases"),
                        ui.output_table("b3Table")
                    )
                )
            )
        )
    )
)

# --- Shiny Server ---
def server(input, output, session):
    
    @reactive.Calc
    def train_model():
        # Read the diamond dataset.
        df = pd.read_csv(StringIO(data_str))
        X = df["carat"].values.reshape(-1, 1)
        y = df["price"].values.reshape(-1, 1)
        
        # Standardize the data.
        X_mean, X_std = X.mean(), X.std()
        X_norm = (X - X_mean) / X_std
        
        y_mean, y_std = y.mean(), y.std()
        y_norm = (y - y_mean) / y_std
        
        # Choose the activation function.
        activation = input.activation().lower()
        
        # Get the number of neurons for the hidden layers.
        hidden_neurons = input.hidden_neurons()
        
        # Create and train the neural network.
        nn = NeuralNetwork(input_size=1, hidden_size1=hidden_neurons, hidden_size2=hidden_neurons, 
                           output_size=1, activation=activation)
        epochs = input.epochs()
        learning_rate = 0.01
        losses = nn.train(X_norm, y_norm, epochs, learning_rate)
        
        # Generate predictions.
        y_pred_norm = nn.forward(X_norm)
        y_pred = y_pred_norm * y_std + y_mean
        
        return {
            "losses": losses, 
            "X": X, 
            "y": y, 
            "y_pred": y_pred, 
            "model": nn,
            "data_stats": {
                "X_mean": X_mean,
                "X_std": X_std,
                "y_mean": y_mean,
                "y_std": y_std
            }
        }
    
    @output
    @render.plot
    def lossPlot():
        result = train_model()
        losses = result["losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="blue")
        ax.set_title("Training Loss over Epochs")
        ax.set_xlabel("Epoch")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def predictionPlot():
        result = train_model()
        X = result["X"]
        y = result["y"]
        y_pred = result["y_pred"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(X, y, color="blue", label="Actual Price")
        ax.scatter(X, y_pred, color="red", label="Predicted Price")
        ax.set_title("Actual vs. Predicted Diamond Price")
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.legend()
        return fig
    
    @output
    @render.table
    def w1Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["W1"])
        return df.round(4)

    @output
    @render.table
    def w2Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["W2"])
        return df.round(4)

    @output
    @render.table
    def w3Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["W3"])
        return df.round(4)

    @output
    @render.table
    def b1Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["b1"])
        return df.round(4)

    @output
    @render.table
    def b2Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["b2"])
        return df.round(4)

    @output
    @render.table
    def b3Table():
        result = train_model()
        model = result["model"]
        params = model.get_parameters()
        df = pd.DataFrame(params["b3"])
        return df.round(4)

app = App(app_ui, server)</code></pre>
<p>In training a ‘standard’ neural network, we hope to obtain a point estimate of the parameter values that will have the most accuracy in predicting data.</p>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>From the Frequentist viewpoint you can also calculate uncertainty in the models parameter values - however the assumption is that a single model with a single set of parameters creates the data. The uncertainty in the parameter values stems from the fact that the model generated data randomly, and we have only a limited number of samples with which we ‘backcalculate’ the parameters.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>