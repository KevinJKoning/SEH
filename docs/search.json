[
  {
    "objectID": "1.3-sine_test_shiny.html",
    "href": "1.3-sine_test_shiny.html",
    "title": "Sine function",
    "section": "",
    "text": "The plot below allows you to control parameters used in the sine function. Experiment with the period, amplitude, and phase shift to see how they affect the graph.\nIt usually takes about 10 seconds for the app to below to load (your browser is loading a web assembly version of python, pyodide, lauching a shiny server, and then serving an app…)\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 420\n\nfrom shiny import App, render, ui\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"period\", \"Period\", 0.5, 2, 1, step=0.5),\n            ui.input_slider(\"amplitude\", \"Amplitude\", 0, 2, 1, step=0.25),\n            ui.input_slider(\"shift\", \"Phase shift\", 0, 2, 0, step=0.1),\n        ),\n        ui.output_plot(\"plot\"),\n    ),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"Sine function\")\n    def plot():\n        t = np.arange(0.0, 4.0, 0.01)\n        s = input.amplitude() * np.sin(\n            (2 * np.pi / input.period()) * (t - input.shift() / 2)\n        )\n        fig, ax = plt.subplots()\n        ax.set_ylim([-2, 2])\n        ax.plot(t, s)\n        ax.grid()\n\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Sine function"
    ]
  },
  {
    "objectID": "1-data_generation.html",
    "href": "1-data_generation.html",
    "title": "Untitled",
    "section": "",
    "text": "Create the data generating process\nIt’s easy to find the likelihood/probability of the outcomes\nNote how large number of dice is looking a lot like a normal/gaussian distribution\n\n\n\n\n\nEach distribution has assumptions about the data generating process\n\nNormal assumes many additive processes\nLognormal assumes many multiplicative processes\n\nAre idealized real process, but the tradeoff is simplicity of description with only a few parameters\n\n\n\n\n\nHistogams can help us approximate likelihood of continuous data\n\nOr we can understand probability density and cumulative distributions\n\nInclude log likelihood and the need for indepedence (not a time series)\nImportance of statistical assumptions",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "1-data_generation.html#data-generation-processes",
    "href": "1-data_generation.html#data-generation-processes",
    "title": "Untitled",
    "section": "",
    "text": "Create the data generating process\nIt’s easy to find the likelihood/probability of the outcomes\nNote how large number of dice is looking a lot like a normal/gaussian distribution\n\n\n\n\n\nEach distribution has assumptions about the data generating process\n\nNormal assumes many additive processes\nLognormal assumes many multiplicative processes\n\nAre idealized real process, but the tradeoff is simplicity of description with only a few parameters\n\n\n\n\n\nHistogams can help us approximate likelihood of continuous data\n\nOr we can understand probability density and cumulative distributions\n\nInclude log likelihood and the need for indepedence (not a time series)\nImportance of statistical assumptions",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "1.4-dice_test_shiny.html",
    "href": "1.4-dice_test_shiny.html",
    "title": "Shiny Live Dice Example",
    "section": "",
    "text": "We start assuming that we know the process generating the data we examine. In reality we usually have only clues, however, we start here as it allows us to quickly examine the probability/likelihood of the data generated by the process. Once we have a good understanding of probability/likelihood, it will allow us to work in ‘reverse’ and to use the data to estimate the properties/parameters of the data generating process.\nThe process is summarized in ?@fig-process.\n\n\n\n\n\n//| label: fig-process\n//| fig-cap: \"Statistical Process\"\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Likelihood \\n of Data| B[\"Data\"]\n    B --&gt;|Likelihood of \\n Data Generation| A",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#introoverview",
    "href": "1.4-dice_test_shiny.html#introoverview",
    "title": "Shiny Live Dice Example",
    "section": "",
    "text": "We start assuming that we know the process generating the data we examine. In reality we usually have only clues, however, we start here as it allows us to quickly examine the probability/likelihood of the data generated by the process. Once we have a good understanding of probability/likelihood, it will allow us to work in ‘reverse’ and to use the data to estimate the properties/parameters of the data generating process.\nThe process is summarized in ?@fig-process.\n\n\n\n\n\n//| label: fig-process\n//| fig-cap: \"Statistical Process\"\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Likelihood \\n of Data| B[\"Data\"]\n    B --&gt;|Likelihood of \\n Data Generation| A",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#data-generation-dice-example",
    "href": "1.4-dice_test_shiny.html#data-generation-dice-example",
    "title": "Shiny Live Dice Example",
    "section": "Data Generation Dice Example",
    "text": "Data Generation Dice Example\nWe’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\nPlease note it usually takes about 10 seconds for the app below to load.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#dice-likelihood",
    "href": "1.4-dice_test_shiny.html#dice-likelihood",
    "title": "Shiny Live Dice Example",
    "section": "Dice Likelihood",
    "text": "Dice Likelihood\nTo find the probability/likelihood of an event in our data generating process of dice rolls, we simply need to add up the frequency of that event, say how often there was total of twelve, and divide that by the total number of events. For example if we saw a total of twelve in 40 out of 5,000 roll-totals, the probability of rolling twelve is approximately 40/5,000=0.0008.\nNow I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techiques that work well in reality.\nIf you are concerned with the quality of our estimate, just rerun the data generating process and see if the outcome changes meaningfully - if it does, increase the number of times we run the data generating process until the output is stable.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#more-data-generating-processes-probability-distributions",
    "href": "1.4-dice_test_shiny.html#more-data-generating-processes-probability-distributions",
    "title": "Shiny Live Dice Example",
    "section": "More Data Generating Processes (Probability Distributions)",
    "text": "More Data Generating Processes (Probability Distributions)\nWhen we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called probability distributions.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and typically observe failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\nThe point here is that each probability distribution is the basic form of: A) Each data generating process has inherent characteristics B) These characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#normalgaussian-likelihood",
    "href": "1.4-dice_test_shiny.html#normalgaussian-likelihood",
    "title": "Shiny Live Dice Example",
    "section": "Normal/Gaussian Likelihood",
    "text": "Normal/Gaussian Likelihood\nHopefully it was fairly intuitive how to find the likelihood of a certain dice-total. One advantage of that data generating process is the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One example is height.\nLikelihood app\nA simple way to understand the likelihood of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.\nIt will be useful for us to also estimate the probability density of exact values in continuous distributions…",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#likelihood-of-multiple-events",
    "href": "1.4-dice_test_shiny.html#likelihood-of-multiple-events",
    "title": "Shiny Live Dice Example",
    "section": "Likelihood of Multiple Events",
    "text": "Likelihood of Multiple Events\nSo far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes.\nEquation for multiple outcomes, and the useful alternative of log-probabilty\nWhen determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is [mostly] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.\nThis is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#statistical-tests",
    "href": "1.4-dice_test_shiny.html#statistical-tests",
    "title": "Shiny Live Dice Example",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nTraditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance’ tests is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get the significance testing wrong, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#arbitrarily-complicated-processes",
    "href": "1.4-dice_test_shiny.html#arbitrarily-complicated-processes",
    "title": "Shiny Live Dice Example",
    "section": "Arbitrarily Complicated Processes",
    "text": "Arbitrarily Complicated Processes\nWe are not limited to simple processes that create known probability distributions…\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, and modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more it is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.4-dice_test_shiny.html#parameters",
    "href": "1.4-dice_test_shiny.html#parameters",
    "title": "Shiny Live Dice Example",
    "section": "Parameters",
    "text": "Parameters\nEnd with a discussion of parameters as they appear in probability distributions. Next we will move on to estimating these parameters in the second half of the book.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Dice Example"
    ]
  },
  {
    "objectID": "1.2-dice_test_obs.html",
    "href": "1.2-dice_test_obs.html",
    "title": "Observable Dice Rolling",
    "section": "",
    "text": "viewof numDice = Inputs.number({ value: 2, min: 1, max: 10, label: \"Number of Dice:\" });\nviewof numRolls = Inputs.slider({ value: 100, min: 1, max: 10000, label: \"Number of Rolls:\" });\nviewof rollButton = Inputs.button(\"Roll the Dice!\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```{ojs}\n// Reactive Data (using a reactive value for better practice)\nconst diceSums = reactive(null);\n\n// Roll Logic (using an observer to react to button clicks)\nconst runtime = new observablehq.Runtime();\nconst module = runtime.module();\n\nmodule.input(\"numDice\", numDice);\nmodule.input(\"numRolls\", numRolls);\n\nmodule.define(\"diceSums\", [\"numDice\", \"numRolls\", \"rollButton\"], (numDice, numRolls, rollButton) =&gt; {\n  if (rollButton) { // Check if the button was clicked\n    const rolls = Array.from({ length: numRolls });\n    rolls.fill(0).forEach((_, i) =&gt; rolls[i] = Array.from({ length: numDice })\n      .map(() =&gt; Math.floor(Math.random() * 6) + 1)\n      .reduce((a, b) =&gt; a + b));\n    diceSums.set(rolls);\n  }\n  return diceSums();\n});\n\n// Display Functions\nfunction displayTextOutput() {\n  const sums = diceSums();\n  return sums === null ? \"No dice rolled yet!\" : sums.join(\", \");\n}\n\nfunction plotDiceDistribution() {\n  const sums = diceSums();\n  if (sums === null) return null;\n  const data = sums.map(sum =&gt; ({ total: sum }));\n\n  return Plot.plot({\n    x: { domain: [Math.min(...sums), Math.max(...sums)]},\n    y: { grid: true},\n    marks: [\n      Plot.barY(data, Plot.binX({y: \"count\"}, {x: \"total\", fill: \"steelblue\"})),\n      Plot.ruleY([0])\n    ],\n    marginTop: 30,\n    marginLeft: 50,\n    marginBottom: 30\n  });\n}\n\n// Render the inputs\nnumDice\nnumRolls\nrollButton\n\n// Reactivity for Display Update\nconst updateDisplay = viewof(() =&gt; {\n  sumsText.textContent = displayTextOutput();\n  const plot = plotDiceDistribution();\n  dicePlot.innerHTML = \"\"; // Clear the previous plot\n  if (plot) dicePlot.appendChild(plot);\n});\n\ndiceSums.subscribe(updateDisplay); // Subscribe to automatically update\n```\n\n\n\n\n\n\n\nOJS Syntax Error (line 17, column 1)Unexpected token",
    "crumbs": [
      "Home",
      "Data Generation",
      "Observable Dice Rolling"
    ]
  },
  {
    "objectID": "2-parameter_estimation.html",
    "href": "2-parameter_estimation.html",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "2-parameter_estimation.html#parameter-estimation-from-data",
    "href": "2-parameter_estimation.html#parameter-estimation-from-data",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "2-parameter_estimation.html#conclusion",
    "href": "2-parameter_estimation.html#conclusion",
    "title": "Untitled",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf it’s not obvious, this is a loop, and you move to each part of the loop",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "1.1-penguin_test.html",
    "href": "1.1-penguin_test.html",
    "title": "Penguins",
    "section": "",
    "text": "A simple example based on Allison Horst’s Palmer Penguins dataset. Here we look at how penguin body mass varies across both sex and species (use the provided inputs to filter the dataset by bill length and island):\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotData\n\n\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\n\n\nInputs.table(filtered)\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nfiltered = data.filter(function(penguin) {\n  return bill_length_min &lt; penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})",
    "crumbs": [
      "Home",
      "Data Generation",
      "Penguins"
    ]
  },
  {
    "objectID": "1.5-normal_likelihood_shiny.html",
    "href": "1.5-normal_likelihood_shiny.html",
    "title": "Shiny Live Likelihood Example",
    "section": "",
    "text": "Testing the likelihood app.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nSecond version\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n    \n    # Row 1: Data preview + mean/variance sliders + sample-size slider\n    ui.row(\n        ui.column(3,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(3,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Action buttons + log-likelihood text\n    ui.row(\n        ui.column(3,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(9,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot below\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize with 10 random points (just a default)\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join([str(round(val, 1)) for val in y])\n\n    # When 'MLE' is clicked, update muInput and varInput to the MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        # Update sliders\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF for the chosen mu/var, and show data as points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        # Build a grid of x values around the data\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot the PDF line\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Plot the data points at y=0 (with some jitter)\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        # Draw vertical line at mu\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Shiny Live Likelihood Example"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html",
    "href": "2.3-tests_processes_parameters.html",
    "title": "Wrap Up",
    "section": "",
    "text": "Traditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#statistical-tests",
    "href": "2.3-tests_processes_parameters.html#statistical-tests",
    "title": "Wrap Up",
    "section": "",
    "text": "Traditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "href": "2.3-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "title": "Wrap Up",
    "section": "Arbitrarily Complicated Processes",
    "text": "Arbitrarily Complicated Processes\nWe are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#parameters",
    "href": "2.3-tests_processes_parameters.html#parameters",
    "title": "Wrap Up",
    "section": "Parameters",
    "text": "Parameters\nEnd with a discussion of parameters as they appear in probability distributions. Next we will move on to estimating these parameters in the second half of the book… (note that the second half will eventually need to cover MLE).",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2-data_generation.html",
    "href": "2-data_generation.html",
    "title": "Untitled",
    "section": "",
    "text": "Select a subchapter from the left hand menu\nTODO: NEED TO CLEAN UP USE OF LIKELIHOOD AND PROBABILITY",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "2-data_generation.html#data-generation-processes-temporary-outline",
    "href": "2-data_generation.html#data-generation-processes-temporary-outline",
    "title": "Untitled",
    "section": "Data Generation Processes TEMPORARY OUTLINE",
    "text": "Data Generation Processes TEMPORARY OUTLINE\n\nDice Rolls\n\nCreate the data generating process\nIt’s easy to find the likelihood/probability of the outcomes\nNote how large number of dice is looking a lot like a normal/gaussian distribution\n\n\n\nNormal and Other Distributions\n\nEach distribution has assumptions about the data generating process\n\nNormal assumes many additive processes\nLognormal assumes many multiplicative processes\n\nAre idealized real process, but the tradeoff is simplicity of description with only a few parameters\n\n\n\nHistograms and Likelihood\n\nHistogams can help us approximate likelihood of continuous data\n\nOr we can understand probability density and cumulative distributions\n\nInclude log likelihood and the need for indepedence (not a time series)\nImportance of statistical assumptions",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Shiny Live Dice Example",
    "section": "",
    "text": "At its heart, I believe statistics is a looping process with only two steps: A) Describing the likelihood of data from a ‘known’ data generating process. B) Using data to estimate the likelihood of the parameters and form of a data generating process.\nThe process can start with A) or B). To really drive it home the process is summarized in ?@fig-process below.\n\n\n\n\n\n//| label: fig-process\n//| fig-cap: \"Statistical Process\"\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Likelihood \\n of Data| B[\"Data\"]\n    B --&gt;|Likelihood of \\n Data Generation| A\n\n\n\n\n\n\nFor convenience, we’ll start assuming that we know the process generating the data we examine. In reality we usually have only clues. Regardless, starting here allows us to quickly examine the core concept of probability/likelihood. Once we have a good understanding of probability/likelihood, it will allow us to work in ‘reverse’ and to use the data to estimate the properties/parameters of the data generating process.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#introoverview",
    "href": "1-intro.html#introoverview",
    "title": "Shiny Live Dice Example",
    "section": "",
    "text": "At its heart, I believe statistics is a looping process with only two steps: A) Describing the likelihood of data from a ‘known’ data generating process. B) Using data to estimate the likelihood of the parameters and form of a data generating process.\nThe process can start with A) or B). To really drive it home the process is summarized in ?@fig-process below.\n\n\n\n\n\n//| label: fig-process\n//| fig-cap: \"Statistical Process\"\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Likelihood \\n of Data| B[\"Data\"]\n    B --&gt;|Likelihood of \\n Data Generation| A\n\n\n\n\n\n\nFor convenience, we’ll start assuming that we know the process generating the data we examine. In reality we usually have only clues. Regardless, starting here allows us to quickly examine the core concept of probability/likelihood. Once we have a good understanding of probability/likelihood, it will allow us to work in ‘reverse’ and to use the data to estimate the properties/parameters of the data generating process.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html",
    "href": "3-parameter_estimation.html",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "href": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#conclusion",
    "href": "3-parameter_estimation.html#conclusion",
    "title": "Untitled",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf it’s not obvious, this is a loop, and you move to each part of the loop",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html",
    "href": "2.2-probability_distributions.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called probability distributions.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\nThe point here is that each probability distribution is the basic form of: A) Each data generating process has inherent characteristics B) These characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#more-data-generating-processes-probability-distributions",
    "href": "2.2-probability_distributions.html#more-data-generating-processes-probability-distributions",
    "title": "Probability Distributions",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called probability distributions.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\nThe point here is that each probability distribution is the basic form of: A) Each data generating process has inherent characteristics B) These characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "href": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "title": "Probability Distributions",
    "section": "Normal/Gaussian Likelihood",
    "text": "Normal/Gaussian Likelihood\nHopefully it was fairly intuitive how to find the likelihood of a certain dice-roll total. One advantage of that data generating process is the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One common example is height.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nA simple way to understand the likelihood of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.\nIt will be useful for us to also estimate the probability density of exact values in continuous distributions…",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "href": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "title": "Probability Distributions",
    "section": "Likelihood of Multiple Events",
    "text": "Likelihood of Multiple Events\nSo far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes.\nEquation for multiple outcomes, and the useful alternative of log-probabilty\nWhen determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is [mostly] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.\nThis is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html",
    "href": "2.1-simple_data_generation.html",
    "title": "Data Generation",
    "section": "",
    "text": "We’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\nPlease note it usually takes about 10 seconds for the app below to load.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#data-generation-dice-example",
    "href": "2.1-simple_data_generation.html#data-generation-dice-example",
    "title": "Data Generation",
    "section": "",
    "text": "We’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\nPlease note it usually takes about 10 seconds for the app below to load.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#dice-likelihood",
    "href": "2.1-simple_data_generation.html#dice-likelihood",
    "title": "Data Generation",
    "section": "Dice Likelihood",
    "text": "Dice Likelihood\nTo find the probability/likelihood of an event in our data generating process of dice rolls, we simply need to add up the frequency of that event, say how often there was total of twelve, and divide that by the total number of events. For example if we saw a total of twelve in 40 out of 5,000 roll-totals, the probability of rolling a total of twelve is approximately 40/5,000=0.0008.\nNow I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techiques that work well in reality.\nIf you are concerned with the quality of our estimate, just rerun the data generating process and see if the outcome changes meaningfully - if it does, increase the number of times we run the data generating process until the output is stable.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  }
]