[
  {
    "objectID": "2.4-tests_processes_parameters.html",
    "href": "2.4-tests_processes_parameters.html",
    "title": "Wrap Up",
    "section": "",
    "text": "It’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.4-tests_processes_parameters.html#statistical-tests",
    "href": "2.4-tests_processes_parameters.html#statistical-tests",
    "title": "Wrap Up",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nTraditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.4-tests_processes_parameters.html#parameters",
    "href": "2.4-tests_processes_parameters.html#parameters",
    "title": "Wrap Up",
    "section": "Parameters",
    "text": "Parameters\nEnd with a discussion of parameters as they appear in probability distributions. Next we will move on to estimating these parameters in the second half of the book… (note that the second half will eventually need to cover MLE).",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "3.2-probability_distributions_reverse.html",
    "href": "3.2-probability_distributions_reverse.html",
    "title": "Distribution Likelihood",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Distribution Likelihood"
    ]
  },
  {
    "objectID": "3.2-probability_distributions_reverse.html#text",
    "href": "3.2-probability_distributions_reverse.html#text",
    "title": "Distribution Likelihood",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Distribution Likelihood"
    ]
  },
  {
    "objectID": "2-data_generation.html",
    "href": "2-data_generation.html",
    "title": "Untitled",
    "section": "",
    "text": "In the first half of the book,\nSelect a subchapter from the left hand menu…\nThere are two recommended ways to work your way through this primer. A) Just go in top down order, this will introduce you first to the probability of data given a model of a data generating process, and it will work into models of greater and greater complexity. B) Do the first chapter in probability of data, then do the first chapter of probability of model, then the second chapter in both, etc. This bounces between the probability of the data and the likelihood of the model.\nTODO: BOLD words that end up in a glossary TODO: NEED TO CLEAN UP USE OF LIKELIHOOD AND PROBABILITY - PROBABILITY GOING FORWARD, LIKELIHOOD GOING BACKWARD TODO: Need to only have data generation in the first normal likelihood app, MLE and changing the data generating distribution in the second. TODO: APP FOR DIFFERENT PROBABILITY DISTRIBUTIONS, MAKE IT ITS OWN SUBCHAPTER? TODO: Do we have a description between statistics, machine learning, and AI in a good spot?",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2-data_generation.html#data-generation-processes-temporary-outline",
    "href": "2-data_generation.html#data-generation-processes-temporary-outline",
    "title": "Untitled",
    "section": "Data Generation Processes TEMPORARY OUTLINE",
    "text": "Data Generation Processes TEMPORARY OUTLINE\n\nDice Rolls\n\nCreate the data generating process\nIt’s easy to find the likelihood/probability of the outcomes\nNote how large number of dice is looking a lot like a normal/gaussian distribution\n\n\n\nNormal and Other Distributions\n\nEach distribution has assumptions about the data generating process\n\nNormal assumes many additive processes\nLognormal assumes many multiplicative processes\n\nAre idealized real process, but the tradeoff is simplicity of description with only a few parameters\n\n\n\nHistograms and Likelihood\n\nHistogams can help us approximate likelihood of continuous data\n\nOr we can understand probability density and cumulative distributions\n\nInclude log likelihood and the need for indepedence (not a time series)\nImportance of statistical assumptions",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2-data_generation.html#other-notes",
    "href": "2-data_generation.html#other-notes",
    "title": "Untitled",
    "section": "Other Notes",
    "text": "Other Notes\nData Generation - Dice Rolls - Probability Distributions for Data Generation - Linear Regression\nParameter Estimation - Dice Rolls (guess and check app) - Probability Distribution (enhanced normal app with MLE and adjustment of data generation) - Linear Regression (both frequentist with underlying assumption and bayesian)\nConclusion.",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…\n\n\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!\n\n\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#introoverview",
    "href": "1-intro.html#introoverview",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…\n\n\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!\n\n\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html",
    "href": "3.1-data_generation_reverse.html",
    "title": "Dice Model Likelihood",
    "section": "",
    "text": "In part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we want to take the data and find the most likely model of the data generating process. It’s reasonable to think of this as the ‘reverse’ of our previous approach in part one.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#data-generation-reverse",
    "href": "3.1-data_generation_reverse.html#data-generation-reverse",
    "title": "Dice Model Likelihood",
    "section": "",
    "text": "In part one we wanted to understand the likelihood of the data from a set/known data generating process. In part two we want to take the data and find the most likely data generating process.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed: 7 Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html",
    "href": "3-parameter_estimation.html",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "href": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#conclusion",
    "href": "3-parameter_estimation.html#conclusion",
    "title": "Untitled",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf it’s not obvious, this is a loop, and you move to each part of the loop",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html",
    "href": "2.3-linear_data_generation.html",
    "title": "Linear Regression",
    "section": "",
    "text": "At some point you have probably been exposed to linear regression… You may roll your eyes at being asked to warp back to your early education… But linear regression, and it’s generalized cousins, are an incredibly useful set of techniques in themselves, and truly understanding them is an enormous boon for understanding the entire suite of statistical and machine learning methods.\nWe’re also going to approach this in a way that is likely backwards from your previous introduction. Typically you’ll be presented with some data and the task will be to find a best fit line through the data. Often this starts with a single explanatory variable, but can it can easily be extended to many more. The actual algorithm of the best fit line is usually hand waved as some sort of magic - tackling that magic, i.e. finding the data generating process that best fits the data, is saved for the second half of this primer. Here we will think about it in the forward direction, if we already have a linear regression model, what data does it generate? And how likely were we to see that data once generated?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Regression"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#intro",
    "href": "2.3-linear_data_generation.html#intro",
    "title": "Linear Regression",
    "section": "",
    "text": "At some point you have probably been exposed to linear regression… You may roll your eyes at being asked to warp back to your early education… But linear regression, and it’s generalized cousins, are an incredibly useful set of techniques in themselves, and truly understanding them is an enormous boon for understanding the entire suite of statistical and machine learning methods.\nWe’re also going to approach this in a way that is likely backwards from your previous introduction. Typically you’ll be presented with some data and the task will be to find a best fit line through the data. Often this starts with a single explanatory variable, but can it can easily be extended to many more. The actual algorithm of the best fit line is usually hand waved as some sort of magic - tackling that magic, i.e. finding the data generating process that best fits the data, is saved for the second half of this primer. Here we will think about it in the forward direction, if we already have a linear regression model, what data does it generate? And how likely were we to see that data once generated?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Regression"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#data-generation",
    "href": "2.3-linear_data_generation.html#data-generation",
    "title": "Linear Data Generation",
    "section": "Data Generation",
    "text": "Data Generation\nThe great thing about data generation with a model is that it is going to show you what it knows and not what you think it knows. Let’s walk through an example with a common dataset made available with the R package ggplot2, the diamonds dataset.\nWe’re going to take a look at the relationship between size/carats and price. Here’s a simply X-Y plot that shows us that relationship.\n\n\n\n\n\n\n\n\n\nWe’re going to fit the data with a linear regression model, it will have this mathematical form:\n\\[\ny_i = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the the slope, and \\(\\epsilon\\) is the error.\nWe’re leaving all the model fitting until part two of this primer, but suffice it to say I fit a linear model using standard techniques and the outputs included a common metric for accuracy called R-squared with a value of about 0.84. It ranges from 0 to 1 and values close to 1 indicate the model is good at explaining the variability in the data, so this is a reasonably good model.\nYou’ve probably seen a linear regression line fit to data many times in your life, and I’m not going to bother you with one more. Instead I’m going to generate data based on what the model has ‘learned’, which I think will be more useful and interesting.\n\n\n\n\n\n\n\n\n\nOK, let’s point out a few things this model is doing wrong:\n\nThe model produces a decent number of negative values at small carats. I don’t think this is real because no one has paid me to take a diamond before.\nThe variation around the mean value is the same regardless of whether it is a 0.3 carat diamond or a 1.5 carat diamond. What’s actually happening in the data is called Heteroscedasticity, a fine English word, and actually quite common.\nThere are some important ‘break points’ in the data that a linear model is not going to capture, like the extra premium placed on a 1.00 carat diamond vs 0.99 carats.\nIt seems large diamonds, of presumably poor or average quality, are not as valuable as the model seems to think.\n\nLinear models are very interpretable\nNow you’re thinking, well you just didn’t use a complicated enough model… Maybe throw a neural network at it. And while you aren’t all wrong - you are also on the path to the dark side.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Linear Data Generation"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#more-models",
    "href": "2.3-linear_data_generation.html#more-models",
    "title": "Linear Models",
    "section": "More Models",
    "text": "More Models\nWe are gradually making more ‘bespoke’ models… There tend to be common types of models because they can be applied so generally, these include the generalized linear models and other ‘machine learning’ types of models like decision trees. However, there’s nothing in statistics that limits us to just these… If we have a specialized case, like modeling the motion of a spring, we can generate a specialized model.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Engineers in a Hurry (a Primer)",
    "section": "",
    "text": "Please use the Github repo for the book to make corrections or contributions: https://github.com/Bearsetc/SEH"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Although I already had a rough outline and some content, once I had a decent first draft of the primer’s introduction I asked ChatGPT to sketch out a table of contents. Even with the the introduction giving a very clear direction, ChatGPT’s table of contents mirrored every other statistical book I have read, and was completely different from what I was envisioning. And that’s when I began to think I may be onto something meaningful.\nIt should be no surprise that various large language models helped me write some of the more boilerplate tidbits of the primer and helped mash the code heavy parts into shape, I think that’s now just modern life. However, I grimice when folks suggest using AI to write for them - for example give it a one sentence prompt and have it crank out a 4 paragraph work e-mail. Once AI has enormous context about your life, this may make sense. However, today, it is just a way to sound like the trillions of words already written by the rest of humanity, however polished. There’s no reason to write something a large language model with no context can generate on the spot.\nI hope this helps you avoid a maze of confusion that I wandered through as I tried to grasp statistics. First as a graduate engineering student, and later as a professional engineer working in a risk management program of an energy utility. An even grander vision would be that someday, maybe I’ll be able to feed in my intro to ChatGPT, and it will spit out something much closer to my table of contents.\n-Kevin"
  },
  {
    "objectID": "2.2-probability_distributions.html",
    "href": "2.2-probability_distributions.html",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so statisticians have named them and [mostly] standardized their inputs/parameters. We can think of probability distributions as models of common data generating processes. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you’ll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of dice rolls earlier.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding enough dice in enough rolls, the outcome looks awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\n\n\n\n\n\n\nWarning\n\n\n\nSHORTEN THIS.\nThere is a little slight of hand that statisticians use without making it explicit - they use probability distributions in different contexts with different meanings. No doubt this is part of the confusing welcome. The other use of probability distributions, to describe uncertainty, we will touch on at the end of this chapter.\nIf teaching someone a new language and using a single word repeatedly with two different meanings - it would be confusing. We can learn faster when things like this are explicitly disambiguated. I’ll do that with the uses of probability distributions. When we use it in the context of data generation, I will refer to it as a probability distribution model. When we use it in the context of uncertainty, I will call it an uncertainty distribution. Just like the a single word with one spelling and two meanings, the math of the two cases will be exactly the same, but by calling out it’s meaning explicitly I hope to aid your understanding substantially.\nIf you want to a more detailed description of the two use cases, just briefly scroll to the end of this chapter.\n\n\nThe point here is that each probability distribution model has the basic form of:\n\nThe model is mimicing a [idealized] process with inherent characteristics\nThese characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#more-data-generating-processes-probability-distribution-as-models",
    "href": "2.2-probability_distributions.html#more-data-generating-processes-probability-distribution-as-models",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so statisticians have named them and [mostly] standardized their inputs/parameters. We can think of probability distributions as models of common data generating processes. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you’ll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of dice rolls earlier.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding enough dice in enough rolls, the outcome looks awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\n\n\n\n\n\n\nWarning\n\n\n\nSHORTEN THIS.\nThere is a little slight of hand that statisticians use without making it explicit - they use probability distributions in different contexts with different meanings. No doubt this is part of the confusing welcome. The other use of probability distributions, to describe uncertainty, we will touch on at the end of this chapter.\nIf teaching someone a new language and using a single word repeatedly with two different meanings - it would be confusing. We can learn faster when things like this are explicitly disambiguated. I’ll do that with the uses of probability distributions. When we use it in the context of data generation, I will refer to it as a probability distribution model. When we use it in the context of uncertainty, I will call it an uncertainty distribution. Just like the a single word with one spelling and two meanings, the math of the two cases will be exactly the same, but by calling out it’s meaning explicitly I hope to aid your understanding substantially.\nIf you want to a more detailed description of the two use cases, just briefly scroll to the end of this chapter.\n\n\nThe point here is that each probability distribution model has the basic form of:\n\nThe model is mimicing a [idealized] process with inherent characteristics\nThese characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#arbitrarily-complicated-processes",
    "href": "2.2-probability_distributions.html#arbitrarily-complicated-processes",
    "title": "Probability Distributions",
    "section": "Arbitrarily Complicated Processes",
    "text": "Arbitrarily Complicated Processes\nIt’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "href": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "title": "Continuous Probability Distributions",
    "section": "Normal/Gaussian Likelihood",
    "text": "Normal/Gaussian Likelihood\nTODO: We should not be able to change the data generating mechanism and only be able to change the data. Save changing the data generating mechanism for part 2.\nHopefully it was fairly intuitive how to find the probability of a certain dice-roll total. One advantage of that data generating process and model is that the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One common example is height.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nA simple way to understand the probability of outcomes of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.\nHistogram of continuous data??\nIt’s tempting to leave the discussion here and to essentially just use histograms of the continuous data and our relative frequency technique to find the approximate probability of a value. However, there are unfortunately problems. The first one that comes to mind is the arbitrary size of the bins you choose, although for practical problems we can probably get around it. The final straw though is that having a robust understanding of probability distributions will pay big dividends later (so we can use standard likelihood techniques), so we slip a little further into the statistical void.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "href": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "title": "Probability Distributions",
    "section": "Likelihood of Multiple Events",
    "text": "Likelihood of Multiple Events\nSo far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes. Let’s say we have \\(n\\) independent events \\(E\\). The probability of all these events occurring together is the product of their individual probabilities:\n\\[\\prod_{i=1}^{n} P(E_i)\\]\nFor example, if we flip a fair coin twice, the probability of getting heads both times is (1/2) * (1/2) = 1/4. When dealing with many events or very small probabilities, multiplying probabilities can lead to numerical instability. A clever and useful trick is to use the logarithm of the probability product: \\[\\sum_{i=1}^{n} log(P(E_i))\\]\nYou will find adding logarithms to be the standard for likelihood calculations.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#assumptions",
    "href": "2.2-probability_distributions.html#assumptions",
    "title": "Continuous Probability Distributions",
    "section": "Assumptions",
    "text": "Assumptions\n\n\n\n\n\n\nWarning\n\n\n\nWhen determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is [mostly] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.\nThis is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#always-a-data-generating-process",
    "href": "2.2-probability_distributions.html#always-a-data-generating-process",
    "title": "Probability Distributions",
    "section": "Always a Data Generating Process?",
    "text": "Always a Data Generating Process?\nWhile I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.\nThe second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.\nIn the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).\n\n\n\n\n\nflowchart LR\n    subgraph DGD[Data Generating Distribution]\n        subgraph UD[Uncertainty Distributions]\n            GammaMean[Gamma Distribution]\n            GammaVar[Gamma Distribution]\n        end\n        Mean[Mean]\n        Variance[Variance]\n        GammaMean --&gt; Mean\n        GammaVar --&gt; Variance\n        Mean --&gt; Normal\n        Variance --&gt; Normal\n        Normal[Normal Distribution]\n    end\n    Normal --&gt; Height[Height Observations]",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html",
    "href": "2.1-simple_data_generation.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "Here we begin the first half of the primer, which focuses on \\(P(D|M)\\), the probability of the data given a model of a data generating process. Before we can talk about data from a model, we need to introduce models. The first chapter considers only models with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce ‘discrete probability distributions’ as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. We will start with the dice example and use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. We then show how discrete probability distributions can generate samples similar to our dice rolling model but for other processes. We then show how we could use the relative frequency technique with discrete probability distributions to find approximate probabilities. We then point out, like someone already performing an infinite number of samples, the exact probabilities have already been calculated, if we need to use themß.\nFinally, we show how we can use computation to calculate the approximate probability of multiple events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#data-generation-with-dice",
    "href": "2.1-simple_data_generation.html#data-generation-with-dice",
    "title": "Discrete Probability Distributions",
    "section": "Data Generation with Dice",
    "text": "Data Generation with Dice\nWe’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process. It is also convenient that the mathematical model we’ll use is an awfully good approximation of the real data generating process, so long as you’re OK with ignoring all the physical bouncing of the dice and are content with just the result after a roll.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#dice-totals-probability",
    "href": "2.1-simple_data_generation.html#dice-totals-probability",
    "title": "Discrete Probability Distributions",
    "section": "Dice Totals Probability",
    "text": "Dice Totals Probability\nOur goal here is not to think too much about the data generating process or the model (that comes later!), what we really want to know now is the likelihood of a particular dice total given a dice model. To estimate the probability \\(P(E)\\) of an event \\(E\\), we can use the relative frequency approach. This involves counting the number of occurrences of the event E and dividing it by the total number of trials. For example, if we observed a total of twelve occur in 40 out of 5,000 dice rolls, the probability estimate is:\n\\[\nP(E) \\approx \\frac{\\text{Number of times event } E \\text{ occurs}}{\\text{Total number of trials}} = \\frac{40}{5000} = 0.008\n\\]\nThe accuracy of this estimate depends on the total number of trials as governed by the Law of Large Numbers. The standard error, which gives a measure of uncertainty in the estimate of a mean value, is proportional to one over the square root of the number of samples:\n\\[\nP_{\\text{error}} \\propto \\frac{1}{\\sqrt{N_{\\text{total}}}}\n\\]\nWhich indicates there are diminishing returns to just making the sample size larger. Now I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techiques that work well for real problems. In general, if you are concerned with the quality of an estimate with this approach, just rerun the model and see if the outcome changes meaningfully - if it does, increase the number of times we run the model until the output is stable enough for your application. If that’s still not enough, dig into exact/analytic methods.\n\n\n\n\n\n\nNote\n\n\n\nThroughout this primer there will be several opportunities for exact/analytical solutions, but we will generally ignore them. Instead we will choose computation whenever it is plausible. Why? It’s simply a lot faster than figuring out some unknown depth of analysis, and we can usually have more confidence in the results because we’ve reduced the human error element. (Some may know an essay called the ‘The Bitter Lesson’ by Rich Sutton, which I think has a strong engineering corollary…). Be assured that solving problems through raw computation has it’s limits - there’s no way to compute through a bad algorithm, and often the right next step in that situation is to gain a more fundamental understanding through analysis. But we’re professionals in a hurry, we choose computation when it’s plausible.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#summary",
    "href": "2.1-simple_data_generation.html#summary",
    "title": "Discrete Probability Distributions",
    "section": "Summary",
    "text": "Summary\nWhat we intended to show here is we can build a model of a process, and use that model to determine how probable any outcome of the process is. We also see that the reliability of that estimate is related to the total number of samples from the process, with more samples leading to more reliable estimates.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#wait-not-so-fast",
    "href": "2.1-simple_data_generation.html#wait-not-so-fast",
    "title": "Discrete Probability Distributions",
    "section": "Wait Not so Fast",
    "text": "Wait Not so Fast\nYou may have a thought lingering in the back of your brain - why am I trusting you that your dice rolling model represents reality? Well, in general you shouldn’t trust any model. This is where real data is incredibly important. Preferably we’d have the data from real dice rolls, and although we shouldn’t expect a perfect match, we’d be able to at least get an intuition for whether the model was reasonable. Since this problem was so simple, and is relatively easy to verify analytically, we skipped this otherwise important step.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html",
    "href": "3.x-reverse_it.html",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "href": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.3-linear-regression.html",
    "href": "3.3-linear-regression.html",
    "title": "Test of new Linear Regression App",
    "section": "",
    "text": "Text description here.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\n\nOrdinary Least Squares via Analytical Solution\nThe closed-form solution for linear regression minimizes the sum of squared residuals directly. Given a dataset with feature matrix X and target vector y :\n\\(\\beta = (X^T X)^{-1} X^T y\\)\nWe can directly compute the coefficients, however, we are assuming normally distributed errors…\n\n\nOther Methods\nA model of our data generating process will essentially try to predict our response variable.",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Linear Regression App"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#wait-one-more",
    "href": "2.1-simple_data_generation.html#wait-one-more",
    "title": "Discrete Probability Distributions",
    "section": "Wait, One More!",
    "text": "Wait, One More!\nYou may also have asked yourself - why can’t (or when should) I just use the real data to find the probability of an event? This would use the same technique as before, except the data would give count/frequency of the event divided by the total number of observations.\nThe answer to that is - if you have enough data, you should do it that way! For example, if you’ve observed something about a dozen or more times in a dataset, dividing the count/frequency of that event by the total number of observations will give you quite a good estimate for its probability, given the same conditions as the dataset. It’s more complicated if you’ve only observed the event a couple times, and obviously impossible if you’ve observed it zero times. It’s also not possible if you want to make a change to the conditions that were used to generate the dataset… (discussion of uncertainty in those estimates at low counts????????, appendix material????)\nIt’s quite common to be interested in the likelihood of an event in the tails (the far ends) of a distribution, however this is also where we need the most caution. Generally we will use some knowledge about the problem and the kind of data it is likely to produce combined with some sample of data to ground-truth the model parameters… However, since the tails of the distribution are rarely or never observed, we need a lot of caution and humility if we try to predict them.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#section",
    "href": "2.3-linear_data_generation.html#section",
    "title": "Linear Data Generation",
    "section": "",
    "text": "OK, so we got all this fancy output, including a decently high R-squared value of 0.843. Since an R-squared value of 0 is a model that explains none of the variability in the result, and a value of 1 is that it explains all the variability, we conclude we’ve done a fairly good job at 0.843.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = \"../datasets/diamonds.csv\"\ndiamonds = pd.read_csv(file_path)\ndiamonds = diamonds[(diamonds['cut'] == 'Premium') & (diamonds['color'] == 'E')]\n\n# Plot 1: Relationship between Carat and Price\nintercept = -2564.4751\nslope = 8503.5628\nresidual_std_dev = 1503\n\n# Generate new data points for carat\nX_new = np.linspace(0.2, 3, 1000)  # Example carat values\n\n# Simulate random noise\nrandom_noise = np.random.normal(loc=0, scale=residual_std_dev, size=len(X_new))\n\n# Generate new price values with variability\nY_new = intercept + slope * X_new + random_noise\n\n# Create a DataFrame to store the new data\nnew_data = pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n# Plot both side by side\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Original Data\naxes[0].scatter(diamonds['carat'], diamonds['price'], alpha=0.6, color='blue')\naxes[0].set_title('Original Data: Carat vs Price')\naxes[0].set_xlabel('Carat')\naxes[0].set_ylabel('Price')\naxes[0].grid(True)\n\n# Simulated Data\naxes[1].scatter(new_data['carat'], new_data['price'], alpha=0.6, color='orange')\naxes[1].set_title('Simulated Data: Carat vs Price')\naxes[1].set_xlabel('Carat')\naxes[1].set_ylabel('Price')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOK, let’s point out a few things this model is doing wrong (and by the way, this is much easier to see when generating data): -The model produces a decent number of negative values at small carats. I don’t think this is real because noone has paid me to take a diamond before. -The variation around the mean value is the same regardless of whether it is a 0.3 carat diamond or a 1.5 carat diamond.\nThe second one listed about is Heteroscedasticity, one of the funnest words in the english language.\nObservations about what’s wrong, heteroscadicity, negative values…\nNow you’re thinking, well you just didn’t use a complicated enough model… Maybe throw a neural network at it. And while you aren’t all wrong - you ware also on the path to the dark side.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Linear Data Generation"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#methods",
    "href": "3.1-data_generation_reverse.html#methods",
    "title": "Dice Model Likelihood",
    "section": "Methods",
    "text": "Methods\nBefore we dive too deep into the details, we are going to take a moment to reflect on what we’re attempting to do and common techniques to do it…\nWhen we were just generating data, it was obvious what variables went into the model, and what data came out of the model. However, we should do a better job of defining that now. Part of the issue here is that many names are used for many applications. We will call inputs predictor/feature variables, and we will call the outputs the outcome/target variables…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "href": "3.1-data_generation_reverse.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "title": "Dice Model Likelihood",
    "section": "Note in 3.2 we will go into the likelihood calculation itself",
    "text": "Note in 3.2 we will go into the likelihood calculation itself",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#single-variable-regression",
    "href": "2.3-linear_data_generation.html#single-variable-regression",
    "title": "Linear Regression",
    "section": "Single Variable Regression",
    "text": "Single Variable Regression\nThe great thing about data generation with a model is that it is going to show you what it knows and not what you think it knows. Let’s walk through an example with a common dataset made available with the R package ggplot2 or the Python package Seaborn - it is the diamonds dataset which show sale price based on diamond characteristics.\nHere’s a simply X-Y plot that shows us the relationship between diamond size (carats) and price:\n\n\n\n\n\n\n\n\n\nWe’re going to fit the data with a linear regression model, it will have this mathematical form:\n\\[\ny_i = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the the slope, and \\(\\epsilon\\) is the error. The variable x will be the diamond size/carat and y will be the price.\nWe’re leaving all the model fitting until part two of this primer, but suffice it to say I fit a linear model using standard techniques and the outputs included a common metric for accuracy called R-squared with a value of about 0.84. It ranges from 0 to 1 and values close to 1 indicate the model is good at explaining the variability in the data, so this is a reasonably good model.\nYou’ve probably seen a linear regression line fit to data many times in your life, and I’m not going to bother you with one more. Instead I’m going to generate data based on what the model has ‘learned’, which I think will be more useful and interesting.\n\n\n\n\n\n\n\n\n\n\nModel Flaws\nOK, let’s point out a few things this model is doing wrong:\n\nThe model produces a decent number of negative values at small carats. I don’t think this is real because no one has paid me to take a diamond before.\nThe variation around the mean value is the same regardless of whether it is a 0.3 carat diamond or a 1.5 carat diamond. What’s actually happening in the data is called Heteroscedasticity, a fine English word, and actually quite common.\nThere are some important ‘break points’ in the data that a linear model is not going to capture, like the extra premium placed on a 1.00 carat diamond vs 0.99 carats.\nIt seems large diamonds, of presumably poor or average quality, are not as valuable as the model seems to think.\n\nLinear models are very interpretable, so someone familiar with them probably could have pointed out these flaws very early. However, when more variables are introduced or we use a more flexible modeling technique, data generation is going to be even more valuable.\n\n\nProbability of the Data\nWhile we can expect all models to be flawed to some extent, using a model with some very obvious flaws makes estimating the probability of data quite dubious. For example, we have some data that is 5+ standard deviations away, which is rarer than one in a million. I don’t think the data is that rare, I think the model is just not very good.\nNEEDS MORE MATH",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Regression"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#multi-variable-regression",
    "href": "2.3-linear_data_generation.html#multi-variable-regression",
    "title": "Linear Regression",
    "section": "Multi Variable Regression",
    "text": "Multi Variable Regression\nThere are actually some more columns/attributes/variables in the diamond dataset we didn’t use. Often in life we know things like this exist but they are just not available. However, right now we’ll use them and see if we can improve the model.\nNow you’re thinking, well you just didn’t use a complicated enough model… Maybe throw a neural network at it. And while you aren’t all wrong - you are also on the path to the dark side.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Regression"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#references",
    "href": "2.3-linear_data_generation.html#references",
    "title": "Linear Models",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#pro",
    "href": "2.3-linear_data_generation.html#pro",
    "title": "Linear Data Generation",
    "section": "Pro",
    "text": "Pro",
    "crumbs": [
      "Home",
      "Data Generation",
      "Linear Data Generation"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "href": "2.3-linear_data_generation.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "title": "Linear Models",
    "section": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "text": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Linear Models"
    ]
  },
  {
    "objectID": "3.2-probability_distributions_reverse.html#likelihood",
    "href": "3.2-probability_distributions_reverse.html#likelihood",
    "title": "Distribution Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nWe have finally worked our way up to what I consider to be one of the most important topics… Likelihood…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Distribution Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#arbitrarily-complicated-processes-remove",
    "href": "2.2-probability_distributions.html#arbitrarily-complicated-processes-remove",
    "title": "Probability Distributions",
    "section": "Arbitrarily Complicated Processes (REMOVE?)",
    "text": "Arbitrarily Complicated Processes (REMOVE?)\nIt’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#arbitrarily-complicated-processes-move-to-later",
    "href": "2.2-probability_distributions.html#arbitrarily-complicated-processes-move-to-later",
    "title": "Probability Distributions",
    "section": "Arbitrarily Complicated Processes (MOVE TO LATER)",
    "text": "Arbitrarily Complicated Processes (MOVE TO LATER)\nIt’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#probability-of-continuous-data",
    "href": "2.2-probability_distributions.html#probability-of-continuous-data",
    "title": "Continuous Probability Distributions",
    "section": "“Probability” of Continuous Data",
    "text": "“Probability” of Continuous Data\nWhen we plot a continuous probability distribution, we refer to any point as the probability density, and the area under the curve as the cumulative probability, where all the area is equal to one. An important point is the probability of an exact value is effectively zero.\n\n\n\n\n\n\n\n\n\nAlthough we shouldn’t intepret a point on the graph as a probability, we can interpret it as the relative probability of that location compared to another. For example if a point has a probability density of 0.4 and another at 0.2, we can conclude the point at 0.4 has twice the relative probability.\n\n\n\n\n\n\nWarning\n\n\n\nI’ve reserved the use of likelihood until the second half of this primer during which we’ll find the best model based on the data from the data generating process. However, in other texts you will find also find descriptions of the relative probability of continuous distributions called the relative likelihood.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#probability-of-multiple-events",
    "href": "2.2-probability_distributions.html#probability-of-multiple-events",
    "title": "Continuous Probability Distributions",
    "section": "Probability of Multiple Events",
    "text": "Probability of Multiple Events\nMaybe not get too sucked into the probability of multiple events when we really want to get to likelihood???\nSo far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes. Let’s say we have \\(n\\) independent events \\(E\\). The probability of all these events occurring together is the product of their individual probabilities:\n\\[\\prod_{i=1}^{n} P(E_i)\\]\nFor example, if we flip a fair coin twice, the probability of getting heads both times is (1/2) * (1/2) = 1/4. When dealing with many events or very small probabilities, multiplying probabilities can lead to numerical instability. A clever and useful trick is to use the logarithm of the probability product: \\[\\sum_{i=1}^{n} log(P(E_i))\\]\nYou will find adding logarithms to be the standard for likelihood calculations.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#probability-distribution-or-uncertainty",
    "href": "2.2-probability_distributions.html#probability-distribution-or-uncertainty",
    "title": "Continuous Probability Distributions",
    "section": "Probability Distribution or Uncertainty?",
    "text": "Probability Distribution or Uncertainty?\nWhile I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.\nThe second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.\nIn the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).\n\n\n\n\n\nflowchart LR\n    subgraph DGD[Data Generating Distribution]\n        subgraph UD[Uncertainty Distributions]\n            GammaMean[Gamma Distribution]\n            GammaVar[Gamma Distribution]\n        end\n        Mean[Mean]\n        Variance[Variance]\n        GammaMean --&gt; Mean\n        GammaVar --&gt; Variance\n        Mean --&gt; Normal\n        Variance --&gt; Normal\n        Normal[Normal Distribution]\n    end\n    Normal --&gt; Height[Height Observations]",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.4-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "href": "2.4-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "title": "Wrap Up",
    "section": "",
    "text": "It’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#probability-of-a-single-event",
    "href": "2.2-probability_distributions.html#probability-of-a-single-event",
    "title": "Continuous Probability Distributions",
    "section": "Probability of a Single Event",
    "text": "Probability of a Single Event\nWhen discussing continuous probability distributions, the probability of a specific value is effectively zero, so talking about the probability of that value is meaningless. Instead, we generally talk about the probability of getting a value as extreme or more extreme than the value that we observed.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Probability Calculator\"),\n\n    # Row 1: Parameters and input value\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_numeric(\n                \"xInput\", \"Value (x):\",\n                value=110, min=0, max=200\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Probability output\n    ui.row(\n        ui.column(\n            12,\n            ui.h4(\"Probability (P(X ≥ x)):\"),\n            ui.output_text(\"probOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Calculate probability\n    @reactive.Calc\n    def calculate_probability():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n        return 1 - stats.norm.cdf(x, mu, sigma)\n\n    # Show the probability\n    @output\n    @render.text\n    def probOutput():\n        prob = calculate_probability()\n        return f\"{prob:.4f}\"\n\n    # Plot the normal PDF with shaded area\n    @output\n    @render.plot\n    def normalPlot():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n\n        # Create x values for plotting\n        x_min = mu - 4 * sigma\n        x_max = mu + 4 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot the PDF\n        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')\n        \n        # Shade the area for P(X ≥ x)\n        x_shade = x_vals[x_vals &gt;= x]\n        y_shade = stats.norm.pdf(x_shade, mu, sigma)\n        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, \n                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')\n\n        # Add vertical line at x\n        ax.axvline(x, color='red', linestyle='--', alpha=0.5)\n\n        ax.set_title(f\"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        return fig\n\napp = App(app_ui, server)\nHERE WE WANT TO VISUALIZE THE AREA UNDER THE CURVE.\nMathematically, what we just did is this:\nFor a continuous random variable \\(X\\) with a PDF \\(f(x)\\), the probability of observing a value within a range \\([a, b]\\) is calculated as:\n\\[\nP(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\n\\]\nThis integral represents the area under the curve of the PDF between \\(a\\) and \\(b\\). When we refer to probabilities of “as extreme or more extreme” values, we typically compute tail probabilities using similar integrals. For example, for a right-tailed probability beyond a point \\(c\\), we evaluate:\n\\[\nP(X \\geq c) = \\int_c^\\infty f(x) \\, dx\n\\]",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#loss-functions",
    "href": "3.1-data_generation_reverse.html#loss-functions",
    "title": "Dice Model Likelihood",
    "section": "Loss Functions",
    "text": "Loss Functions\nLikelihood is a specific form of a loss function. Likelihood is rooted in probability, but a loss function does not need to be. Loss functions in machine learning are what likelihood functions are in statistics. In generic form it looks like this:\n\\[\n\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\n\\(\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\\): This represents the overall loss function, which measures the difference between the true values \\(\\mathbf{y}\\) and the predicted values \\(\\hat{\\mathbf{y}}\\). It aggregates the individual losses across all data points.\n\\(\\ell(y_i, \\hat{y}_i)\\): This is the individual loss for a single data point \\(i\\). It can take different forms depending on the type of problem. We want to pay particular attention to how the errors are distributed when selecting the loss function.\n\\(\\frac{1}{N} \\sum_{i=1}^{N}\\): This is the averaging operation, which sums up the individual losses \\(\\ell(y_i, \\hat{y}_i)\\) across all \\(N\\) data points in the dataset and divides by \\(N\\) to compute the average loss. This helps ensure that the loss function is independent of the dataset size.\n\nWe introduce loss functions here but will apply them next chapter??",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#data-generation-reversed",
    "href": "3.1-data_generation_reverse.html#data-generation-reversed",
    "title": "Dice Model Likelihood",
    "section": "",
    "text": "In part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we want to take the data and find the most likely model of the data generating process. It’s reasonable to think of this as the ‘reverse’ of our previous approach in part one.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#likelihood",
    "href": "3.1-data_generation_reverse.html#likelihood",
    "title": "Dice Model Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\nIn the previous dice example, we were just eyeballing the right model parameters to maximize the probability of observing our data under the model. We’d like a more consistent and mathematical way to achieve the same intent, which is called a likelihood function. In generic form, it looks like this:\n\\[\n\\mathcal{L}(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{N} P(y_i \\mid \\theta)\n\\]\n\n\\(\\mathcal{L}(\\theta \\mid \\mathbf{y})\\): This represents the overall likelihood function, which measures the likelihood of the model parameters \\(\\theta\\) given the observed data \\(\\mathbf{y}\\). It aggregates the individual probability densities for continuous functions and probabilities for discrete functions across all data points, reflecting how well the parameters \\(\\theta\\) explain the entire dataset.\n\\(P(y_i \\mid \\theta)\\): This is the individual probability density for continuous functions or probability for discrete functions for a single data point \\(i\\). It represents the probability of observing the specific outcome \\(y_i\\) under the model parameterized by \\(\\theta\\). The form of this probability depends on the assumed probability distribution of the data (e.g., binomial for dice rolls, Gaussian for continuous data).\n\\(\\prod_{i=1}^{N}\\): This is the product operation, which multiplies the individual probabilities \\(P(y_i \\mid \\theta)\\) across all \\(N\\) data points in the dataset. This multiplication reflects the assumption that the data points are independent and identically distributed (i.i.d.), meaning the probability of the entire dataset is the product of the probabilities of each individual data point.\n\nThe specific form of the likelihood function is important and can change meaningfully depending on the problem… (MAYBE SAY LATER) If you have fit a regression line to data you probably used a loss function without knowing it, and it was almost certainly base don least squares, which happens to be appropriate when the errors in your data are normally distributed…\n\n\n\n\n\n\nLoss Functions\n\n\n\nLikelihood is a specific form of a loss function. Likelihood is rooted in probability, but a loss function does not need to be. Loss functions in machine learning are what likelihood functions are in statistics. In generic form they look like this:\n\\[\n\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\n\\(\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\\): This represents the overall loss function, which measures the difference between the true values \\(\\mathbf{y}\\) and the predicted values \\(\\hat{\\mathbf{y}}\\). It aggregates the individual losses across all data points.\n\\(\\ell(y_i, \\hat{y}_i)\\): This is the individual loss for a single data point \\(i\\). It can take different forms depending on the type of problem.\n\\(\\frac{1}{N} \\sum_{i=1}^{N}\\): This is the averaging operation, which sums up the individual losses \\(\\ell(y_i, \\hat{y}_i)\\) across all \\(N\\) data points in the dataset and divides by \\(N\\) to compute the average loss. This helps ensure that the loss function is independent of the dataset size.\n\n\n\nWe introduce loss functions here but will apply them next chapter??",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Dice Model Likelihood"
    ]
  },
  {
    "objectID": "1-intro.html#glossary",
    "href": "1-intro.html#glossary",
    "title": "Introduction",
    "section": "Glossary",
    "text": "Glossary\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#welcome",
    "href": "1-intro.html#welcome",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…\n\n\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!\n\n\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "2.5-tests_processes_parameters.html",
    "href": "2.5-tests_processes_parameters.html",
    "title": "Wrap Up",
    "section": "",
    "text": "It’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.5-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "href": "2.5-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "title": "Wrap Up",
    "section": "",
    "text": "It’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.5-tests_processes_parameters.html#statistical-tests",
    "href": "2.5-tests_processes_parameters.html#statistical-tests",
    "title": "Wrap Up",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nTraditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.5-tests_processes_parameters.html#parameters",
    "href": "2.5-tests_processes_parameters.html#parameters",
    "title": "Wrap Up",
    "section": "Parameters",
    "text": "Parameters\nEnd with a discussion of parameters as they appear in probability distributions. Next we will move on to estimating these parameters in the second half of the book… (note that the second half will eventually need to cover MLE).",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html",
    "href": "2.4-bespoke_models.html",
    "title": "Bespoke Models",
    "section": "",
    "text": "Before we get on with creating bespoke models… we should mention all the generic statistical and machine learning models that may meet your needs. If you are interested in the many statistical/machine learning tools, I suggest you refer to An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\nThe various statistical/machine learning tools which include generalized linear regression, decision/regression trees, and neural nets have a distinct advantage and disadvantage: 1) So long as some basic criteria are met, you just apply the model to your data. 2) So long as some basic criteria are met, you just apply the model to your data.\nThat was not a typo. The advantage is both in the speed in which you can perform an analysis as well as the simplicity in communicating how you performed the analysis. I say it is a disadvantage because you are not forced to think through your problem, especially the assumptions that each tool is making. For example, in linear regression, you have a implicit prior assumption that the error term is constant and normally distributed.\nIn this primer we will leave all these other tools to other books, and we will instead focus on techniques that allow us to mold the model to our situation. This should only be done when you have at least some knowledge about the data generation process you are modeling - but it doesn’t take much and who tries to model data for a situation they know nothing about?",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#the-rainbow",
    "href": "2.4-bespoke_models.html#the-rainbow",
    "title": "Bespoke Models",
    "section": "",
    "text": "Although this chapter is about creating bespoke models… we should mention all the generic statistical and machine learning models that may meet your needs.\nWe are gradually making more ‘bespoke’ models… There tend to be common types of models because they can be applied so generally, these include the generalized linear models and other ‘machine learning’ types of models like decision trees. However, there’s nothing in statistics that limits us to just these… If we have a specialized case, like modeling the motion of a spring, we can generate a specialized model.\nIf you are interested in the many statistical/machine learning tools, I suggest you refer to An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\nThe various statistical/machine learning tools which include generalized linear regression and decision/regression trees have a distinct advantage and disadvantage: 1) So long as some basic criteria are met, you just apply the model to your data. 2) So long as some basic criteria are met, you just apply the model to your data.\nThat was not a typo. The advantage is communicating how you performed the analysis mostly requires just naming the tool. You only need to ensure it is applied correctly.\nI say it is a disadvantage because you do not really think through your problem, and you do not really think through the assumptions that each tool is making. For example, in linear regression, you have a implicit prior assumption that the error term is constant and normally distributed.\nIn this primer we will leave all the other tools to other books, and we will instead use techniques that allow us to mold the model to our situation. This should only be done when you have at least some knowledge about the data generation process you are modeling - but it doesn’t take much and who tries to model data for a situation they know nothing about?\nThink bayes Allen Downey, Statistical Rethinking..\nRefer to statistical learning for more…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "href": "2.4-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "title": "Bespoke Models",
    "section": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "text": "Causal Model with 1.0, 1.5, and 2.0 Cut Points\nIf we reason about what is influencing the price of a diamond, once we remove the artificial value that is placed for passing the 1.0, 1.5, and 2.0 carat thresholds, the value appears to be a somewhat continuous and rational combination of the relevant characteristics, specifically color, cut, clarity, and size. If we seem to iron out that one wrinkle, we could have a very good model.\nWe summarize our knowledge of the real data generating process in a causal diagram:\n\n\n\n\n\ngraph LR\n    cut --&gt; linear_model\n    color --&gt; linear_model\n    clarity --&gt; linear_model\n    size --&gt; linear_model\n    size --&gt; carat_threshold\n    linear_model --&gt; price\n    carat_threshold --&gt; price\n\n\n\n\n\n\nThis shows that while we have two mechanisms influencing diamond price. While the cut, color, clarity, and size can be combined in a linear fashion to determine price, the size is also an input to a carat threshold model that will influence price.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points-1",
    "href": "2.4-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points-1",
    "title": "Bespoke Models",
    "section": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "text": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#all-the-models-under-the-rainbow",
    "href": "2.4-bespoke_models.html#all-the-models-under-the-rainbow",
    "title": "Bespoke Models",
    "section": "",
    "text": "Before we get on with creating bespoke models… we should mention all the generic statistical and machine learning models that may meet your needs. If you are interested in the many statistical/machine learning tools, I suggest you refer to An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\nThe various statistical/machine learning tools which include generalized linear regression, decision/regression trees, and neural nets have a distinct advantage and disadvantage: 1) So long as some basic criteria are met, you just apply the model to your data. 2) So long as some basic criteria are met, you just apply the model to your data.\nThat was not a typo. The advantage is both in the speed in which you can perform an analysis as well as the simplicity in communicating how you performed the analysis. I say it is a disadvantage because you are not forced to think through your problem, especially the assumptions that each tool is making. For example, in linear regression, you have a implicit prior assumption that the error term is constant and normally distributed.\nIn this primer we will leave all these other tools to other books, and we will instead focus on techniques that allow us to mold the model to our situation. This should only be done when you have at least some knowledge about the data generation process you are modeling - but it doesn’t take much and who tries to model data for a situation they know nothing about?",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#revisiting-the-diamond",
    "href": "2.4-bespoke_models.html#revisiting-the-diamond",
    "title": "Bespoke Models",
    "section": "Revisiting the Diamond",
    "text": "Revisiting the Diamond",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#revisiting-the-diamond-dataset",
    "href": "2.4-bespoke_models.html#revisiting-the-diamond-dataset",
    "title": "Bespoke Models",
    "section": "Revisiting the Diamond Dataset",
    "text": "Revisiting the Diamond Dataset\nIn the diamond dataset there was something quite obvious to humans that is impossible for many statistical or machine learning tools to learn - the price premium when passing the 1.0, 1.5, and 2.0 carat thresholds.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#b",
    "href": "2.4-bespoke_models.html#b",
    "title": "Bespoke Models",
    "section": "B",
    "text": "B",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#bu",
    "href": "2.4-bespoke_models.html#bu",
    "title": "Bespoke Models",
    "section": "Bu",
    "text": "Bu",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#building-bespoke-models",
    "href": "2.4-bespoke_models.html#building-bespoke-models",
    "title": "Bespoke Models",
    "section": "Building Bespoke Models",
    "text": "Building Bespoke Models\nWhile",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.4-bespoke_models.html#model-flexibility",
    "href": "2.4-bespoke_models.html#model-flexibility",
    "title": "Bespoke Models",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nEngineers are typically familiar with solving systems of equations and the necessity that the system has the appropriate degrees of freedom, such that it is not overly constrained so there is no solution, and it is not underconstrained such that there are an infinite number of solutions.\nStatistical models that have too much flexibility can be optimized into all sorts of nonsensical solutions. Allowing a negative intercept in a linear regression model of the diamond dataset is one example, as it indicates customers will be paid to take small diamonds. I like to visualize this as a truss with beams/members that can contort into all sorts of nonsensical shapes. What we really want are beams/members that bend in reasonable ways.\nWe can use our knowledge of the data generating process to create reasonable priors for our model. This is simply saying that we already know some reasonable range of values, or even a most likely value, for the parameters of the model. Common examples are not letting a parameter take a negative value, or that previous experience suggest the parameter is near a certain value. Setting a prior does not mean we force a parameter value on the model, it just eliminates the impossible values and nudges the model towards our prior knowledge. If the data is different…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke Models"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#discrete-probability-distributions",
    "href": "2.1-simple_data_generation.html#discrete-probability-distributions",
    "title": "Discrete Probability Distributions",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nSome kinds of",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#preview",
    "href": "2.1-simple_data_generation.html#preview",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "Here we begin the first half of the primer, which focuses on \\(P(D|M)\\), the probability of the data given a model of a data generating process. Before we can talk about data from a model, we need to introduce models. The first chapter considers only models with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce ‘discrete probability distributions’ as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. We will start with the dice example and use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. We then show how discrete probability distributions can generate samples similar to our dice rolling model but for other processes. We then show how we could use the relative frequency technique with discrete probability distributions to find approximate probabilities. We then point out, like someone already performing an infinite number of samples, the exact probabilities have already been calculated, if we need to use themß.\nFinally, we show how we can use computation to calculate the approximate probability of multiple events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#discrete-probability-distributions-as",
    "href": "2.1-simple_data_generation.html#discrete-probability-distributions-as",
    "title": "Discrete Probability Distributions",
    "section": "Discrete Probability Distributions as",
    "text": "Discrete Probability Distributions as\nSome kinds of",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#discrete-probability-distributions-as-data-generating-models",
    "href": "2.1-simple_data_generation.html#discrete-probability-distributions-as-data-generating-models",
    "title": "Discrete Probability Distributions",
    "section": "Discrete Probability Distributions as Data Generating Models",
    "text": "Discrete Probability Distributions as Data Generating Models\nIn the last section, we used dice rolling as our data generating process, however there are other discrete processes, such as flipping a coin. As you can imagine, when you change the data generating process, the relative frequency of the possible events/outcomes changes. There are a number of discrete processes that have been interesting to statisticians, and they have been formalized into mathematical models called discrete probability distributions.\n\nBinomial Distribution\nWe use coin flipping as the basis of the binomial distribution, but be aware that the ‘coin’ does not need to be fair, or even close to fair.\nSome kinds of\n\n\nPoisson Distribution",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  }
]