[
  {
    "objectID": "2.3-tests_processes_parameters.html",
    "href": "2.3-tests_processes_parameters.html",
    "title": "Wrap Up",
    "section": "",
    "text": "Traditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#statistical-tests",
    "href": "2.3-tests_processes_parameters.html#statistical-tests",
    "title": "Wrap Up",
    "section": "",
    "text": "Traditional statistical education is somewhat obsessed with testing if two data generating processes are different. (Our approach not only allows for determining if two data generating processes are different, but also making predictions about future data from the process). A common example of these ‘significance tests’ is a human’s likelihood of dying while taking a medication vs not taking the medication. Obviously we prefer not to give people medications that are ineffective, so this is a reasonable question.\nThe typical approach goes something like this - if I assume I know the characteristics of data generating process A, and I know the data generated by a possibly different data generating process we’ll call A/B - then how likely is the data, or even more extreme data/values, to come from A. If it is very unlikely to come from A, we’ll conclude it is coming from another process, B.\nThat probably sounded somewhat confusing, and if you are confused, you are in good company. Many intelligent people publishing smart papers in reputable journals get significance testing not-quite-right, not to mention the arbitrary standard of rejecting a null hypothesis if p &lt; 0.05, which should also be adjusted to the situation. Subsequently this text will spend no time on hypothesis testing, other than to say if it’s important for you to determine if two data generating processes are different, I’m sure you can devise a method that makes sense and you can understand.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "href": "2.3-tests_processes_parameters.html#arbitrarily-complicated-processes",
    "title": "Wrap Up",
    "section": "Arbitrarily Complicated Processes",
    "text": "Arbitrarily Complicated Processes\nWe are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "2.3-tests_processes_parameters.html#parameters",
    "href": "2.3-tests_processes_parameters.html#parameters",
    "title": "Wrap Up",
    "section": "Parameters",
    "text": "Parameters\nEnd with a discussion of parameters as they appear in probability distributions. Next we will move on to estimating these parameters in the second half of the book… (note that the second half will eventually need to cover MLE).",
    "crumbs": [
      "Home",
      "Data Generation",
      "Wrap Up"
    ]
  },
  {
    "objectID": "3.1-reverse_it.html",
    "href": "3.1-reverse_it.html",
    "title": "Untitled",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\nOnce we get close though, it’s much harder to tell if the model can be even more accurate…"
  },
  {
    "objectID": "3.1-reverse_it.html#probability-of-a-data-generating-process",
    "href": "3.1-reverse_it.html#probability-of-a-data-generating-process",
    "title": "Untitled",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\nOnce we get close though, it’s much harder to tell if the model can be even more accurate…"
  },
  {
    "objectID": "2.2-probability_distributions.html",
    "href": "2.2-probability_distributions.html",
    "title": "Likelihood",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called probability distributions. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you’ll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of dice rolls earlier.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\n\n\n\n\n\n\nImportant\n\n\n\nThere is a little slight of hand that statisticians use without making it explicit - they use probability distributions in different contexts with different meanings. No doubt this is part of the confusing welcome. The other use of probability distributions, to describe uncertainty, we will touch on at the end of this chapter.\nIf teaching someone a new language and using a single word repeatedly with two different meanings - it would be confusing. We can learn faster when things like this are explicitly disambiguated. I’ll do that with the uses of probability distributions. When we use it in the context of data generation, I will refer to it as a data generating distribution. When we use it in the context of uncertainty, I will call it an uncertainty distribution. Just like the a single word with one spelling and two meanings, the math of the two cases will be exactly the same, but by calling out it’s meaning explicitly I hope to aid your understanding substantially.\nIf you want to a more detailed description of the two use cases, just briefly scroll to the end of this chapter.\n\n\nSome kind of graphic here?. an example of a bayesian distribution with uncertainty in inputs producing a data generating distribution\nKEEP?: The point here is that each data generating distribution is the basic form of: A) Each data generating process has inherent characteristics B) These characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#more-data-generating-processes-probability-distributions",
    "href": "2.2-probability_distributions.html#more-data-generating-processes-probability-distributions",
    "title": "Likelihood",
    "section": "",
    "text": "When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called probability distributions. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you’ll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of dice rolls earlier.\nA ‘coin-flip’ process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn’t have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there’s no fixed maximum.\nAdditive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn’t noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you’ll get the log-normal distribution.\n\n\n\n\n\n\nImportant\n\n\n\nThere is a little slight of hand that statisticians use without making it explicit - they use probability distributions in different contexts with different meanings. No doubt this is part of the confusing welcome. The other use of probability distributions, to describe uncertainty, we will touch on at the end of this chapter.\nIf teaching someone a new language and using a single word repeatedly with two different meanings - it would be confusing. We can learn faster when things like this are explicitly disambiguated. I’ll do that with the uses of probability distributions. When we use it in the context of data generation, I will refer to it as a data generating distribution. When we use it in the context of uncertainty, I will call it an uncertainty distribution. Just like the a single word with one spelling and two meanings, the math of the two cases will be exactly the same, but by calling out it’s meaning explicitly I hope to aid your understanding substantially.\nIf you want to a more detailed description of the two use cases, just briefly scroll to the end of this chapter.\n\n\nSome kind of graphic here?. an example of a bayesian distribution with uncertainty in inputs producing a data generating distribution\nKEEP?: The point here is that each data generating distribution is the basic form of: A) Each data generating process has inherent characteristics B) These characteristics determine the shape/frequency of the outcomes",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "href": "2.2-probability_distributions.html#normalgaussian-likelihood",
    "title": "Likelihood",
    "section": "Normal/Gaussian Likelihood",
    "text": "Normal/Gaussian Likelihood\nTODO: We should not be able to change the data generating mechanism and only be able to change the data. Save changing the data generating mechanism for part 2.\nHopefully it was fairly intuitive how to find the probability/likelihood of a certain dice-roll total. One advantage of that data generating process is the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One common example is height.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nA simple way to understand the likelihood of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.\nIt will be useful for us to also estimate the likelihood/density of exact values in continuous distributions… I think it’s best I only hand-wave at that now - but suffice it to say there are functions into which we can enter a value and a probability distribution and we receive a relative likelihood/density of that point vs other points.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "href": "2.2-probability_distributions.html#likelihood-of-multiple-events",
    "title": "Likelihood",
    "section": "Likelihood of Multiple Events",
    "text": "Likelihood of Multiple Events\nSo far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes. Let’s say we have \\(n\\) independent events \\(E\\). The probability of all these events occurring together is the product of their individual probabilities:\n\\[\\prod_{i=1}^{n} P(E_i)\\]\nFor example, if we flip a fair coin twice, the probability of getting heads both times is (1/2) * (1/2) = 1/4. When dealing with many events or very small probabilities, multiplying probabilities can lead to numerical instability. A clever and useful trick is to use the logarithm of the probability product: \\[\\sum_{i=1}^{n} log(P(E_i))\\]\nYou will find adding logarithms to be the standard for likelihood calculations.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#assumptions",
    "href": "2.2-probability_distributions.html#assumptions",
    "title": "Likelihood",
    "section": "Assumptions",
    "text": "Assumptions\nWhen determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is [mostly] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.\nThis is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "3-parameter_estimation.html",
    "href": "3-parameter_estimation.html",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "href": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#conclusion",
    "href": "3-parameter_estimation.html#conclusion",
    "title": "Untitled",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf it’s not obvious, this is a loop, and you move to each part of the loop",
    "crumbs": [
      "Home",
      "Parameter Estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\nL(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!\n\n\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#introoverview",
    "href": "1-intro.html#introoverview",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\nL(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!\n\n\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "2-data_generation.html",
    "href": "2-data_generation.html",
    "title": "Untitled",
    "section": "",
    "text": "In the first half of the book,\nSelect a subchapter from the left hand menu\nTODO: NEED TO CLEAN UP USE OF LIKELIHOOD AND PROBABILITY - PROBABILITY GOING FORWARD, LIKELIHOOD GOING BACKWARD TODO: Need to only have data generation in the first normal likelihood app, MLE and changing the data generating distribution in the second. TODO: APP FOR DIFFERENT PROBABILITY DISTRIBUTIONS, MAKE IT ITS OWN SUBCHAPTER?",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "2-data_generation.html#data-generation-processes-temporary-outline",
    "href": "2-data_generation.html#data-generation-processes-temporary-outline",
    "title": "Untitled",
    "section": "Data Generation Processes TEMPORARY OUTLINE",
    "text": "Data Generation Processes TEMPORARY OUTLINE\n\nDice Rolls\n\nCreate the data generating process\nIt’s easy to find the likelihood/probability of the outcomes\nNote how large number of dice is looking a lot like a normal/gaussian distribution\n\n\n\nNormal and Other Distributions\n\nEach distribution has assumptions about the data generating process\n\nNormal assumes many additive processes\nLognormal assumes many multiplicative processes\n\nAre idealized real process, but the tradeoff is simplicity of description with only a few parameters\n\n\n\nHistograms and Likelihood\n\nHistogams can help us approximate likelihood of continuous data\n\nOr we can understand probability density and cumulative distributions\n\nInclude log likelihood and the need for indepedence (not a time series)\nImportance of statistical assumptions",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html",
    "href": "2.1-simple_data_generation.html",
    "title": "Data Generation",
    "section": "",
    "text": "We’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#data-generation-dice-example",
    "href": "2.1-simple_data_generation.html#data-generation-dice-example",
    "title": "Data Generation",
    "section": "",
    "text": "We’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process.\nPlay around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.1-simple_data_generation.html#dice-likelihood",
    "href": "2.1-simple_data_generation.html#dice-likelihood",
    "title": "Data Generation",
    "section": "Dice Likelihood",
    "text": "Dice Likelihood\nTo find the probability/likelihood of an event in our data generating process of dice rolls, we simply need to add up the frequency of that event, say how often there was total of twelve, and divide that by the total number of events. For example if we saw a total of twelve in 40 out of 5,000 roll-totals, the probability of rolling a total of twelve is approximately 40/5,000=0.0008.\nNow I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techiques that work well in reality.\nIf you are concerned with the quality of our estimate, just rerun the data generating process and see if the outcome changes meaningfully - if it does, increase the number of times we run the data generating process until the output is stable.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Data Generation"
    ]
  },
  {
    "objectID": "2.4-linear_data_generation.html",
    "href": "2.4-linear_data_generation.html",
    "title": "Linear Data Generation",
    "section": "",
    "text": "At some point you have probably been exposed to linear regression… You may roll your eyes at being asked to warp back to your early education… But linear regression, and it’s generalized cousins, are an incredibly useful set of techniques in themselves, and truly understanding them is an enormous boon for understanding the entire suite of machine learning methods.\nWe’re also going to approach this in a way that is likely backwards from your previous introduction. Typically you’ll be presented with some data and the task will be to find a best fit line through the data. Often this starts with a single explanatory variable, but can it can easily be extended to many more. The actual algorithm of the best fit line is usually hand waved as some sort of magic - tackling that magic, i.e. finding the data generating process that best fits the data, is saved for the second half of this primer. Here we will think about it in the forward direction, if we already have a linear regression model, what data does it generate? And how likely were we to see that data once generated?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "2.2-probability_distributions.html#always-a-data-generating-process",
    "href": "2.2-probability_distributions.html#always-a-data-generating-process",
    "title": "Likelihood",
    "section": "Always a Data Generating Process?",
    "text": "Always a Data Generating Process?\nWhile I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.\nHowever, it may still be possible to unify both these uses in the data generating process conceptual model. Consider the uncertainty about a value/parameter. A practitioner needs to choose the shape of this uncertainty. Do they believe the value is most likely some central value, but just not sure? Do they think it is equally likely to be any value, but with a set minimum or maximum? We could express these either with normal or uniform distributions, respectively. However, why did they choose normal or uniform? There must have been some aspect of a data generating process that led them to make that conclusion.\nThe second half of the book we will figure out how to best find the form and parameters of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.\nimage of an probability distribution that seperates out uncertainty in knowledge for inherent variability in the outputs\nIn the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).\n\n\n\n\n\nflowchart LR\n    subgraph DGD[Data Generating Distribution]\n        subgraph UD[Uncertainty Distributions]\n            GammaMean[Gamma Distribution]\n            GammaVar[Gamma Distribution]\n        end\n        Mean[Mean]\n        Variance[Variance]\n        GammaMean --&gt; Mean\n        GammaVar --&gt; Variance\n        Mean --&gt; Normal\n        Variance --&gt; Normal\n        Normal[Normal Distribution]\n    end\n    Normal --&gt; Height[Height Observations]",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2.2-probability_distributions.html#arbitrarily-complicated-processes",
    "href": "2.2-probability_distributions.html#arbitrarily-complicated-processes",
    "title": "Likelihood",
    "section": "Arbitrarily Complicated Processes",
    "text": "Arbitrarily Complicated Processes\nIt’s important to note that we are not limited to simple processes that create known probability distributions. We can combine any set of imaginable computational steps, including any probability distributions, together to form a more accurate estimate of the data generating process. A helpful visualization of a more complicated process is a directed acyclic graph (DAG).\nIt is worth noting that a small number of parameters is generally called ‘Statistics’, modest to large number of parameters is called ‘Machine Learning’, and somewhere around a billion or more is called ‘Artificial Intelligence’.",
    "crumbs": [
      "Home",
      "Data Generation",
      "Likelihood"
    ]
  },
  {
    "objectID": "2-data_generation.html#other-notes",
    "href": "2-data_generation.html#other-notes",
    "title": "Untitled",
    "section": "Other Notes",
    "text": "Other Notes\nData Generation - Dice Rolls - Probability Distributions for Data Generation - Linear Regression\nParameter Estimation - Dice Rolls (guess and check app) - Probability Distribution (enhanced normal app with MLE and adjustment of data generation) - Linear Regression (both frequentist with underlying assumption and bayesian)\nConclusion.",
    "crumbs": [
      "Home",
      "Data Generation"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html",
    "href": "3.x-reverse_it.html",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "href": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html",
    "href": "3.1-data_generation_reverse.html",
    "title": "Data Generation",
    "section": "",
    "text": "In part one we wanted to understand the likelihood of the data from a set/known data generating process. In part two we want to take the data and find the most likely data generating process.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model that is generating that data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed: 7 Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Data Generation"
    ]
  },
  {
    "objectID": "3.1-data_generation_reverse.html#data-generation-reverse",
    "href": "3.1-data_generation_reverse.html#data-generation-reverse",
    "title": "Data Generation",
    "section": "",
    "text": "In part one we wanted to understand the likelihood of the data from a set/known data generating process. In part two we want to take the data and find the most likely data generating process.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model that is generating that data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed: 7 Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Data Generation"
    ]
  },
  {
    "objectID": "3.2-probability_distributions_reverse.html",
    "href": "3.2-probability_distributions_reverse.html",
    "title": "Likelihood Reverse",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Likelihood Reverse"
    ]
  },
  {
    "objectID": "3.2-probability_distributions_reverse.html#text",
    "href": "3.2-probability_distributions_reverse.html#text",
    "title": "Likelihood Reverse",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Likelihood Reverse"
    ]
  },
  {
    "objectID": "3.3-linear-regression.html",
    "href": "3.3-linear-regression.html",
    "title": "Test of new Linear Regression App",
    "section": "",
    "text": "Text description here.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Parameter Estimation",
      "Test of new Linear Regression App"
    ]
  }
]