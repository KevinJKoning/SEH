[
  {
    "objectID": "3-parameter_estimation.html",
    "href": "3-parameter_estimation.html",
    "title": "Likelihood of Model",
    "section": "",
    "text": "Please select a chapter in the left hand menu.",
    "crumbs": [
      "Home",
      "Likelihood of Model"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html",
    "href": "2.3-statistical_models.html",
    "title": "Statistical Models",
    "section": "",
    "text": "To learn about probability distributions, we first thought about the data generating process that made them, and then simulated data from models of those processes. This reflects the reality of an infinite number of possible probability distributions. We then focused on some named parametric probability distributions, which have names due to having several nice properties, which include the ability to calculate P(D|M) with [mostly] analytic solutions, and to simulate common processes, including some that nature tends to ‘converge to’, like the normal distribution.\nAs we proceed, we will mostly work with parametric probability distributions - but let’s not make the mistake of believing reality is only composed of these named parametric probability distributions.\nLet’s also remember that the variability we describe with statistics has two sources, Aleatoric Uncertainty (Randomness) and Epistemic Uncertainty (Lack of Knowledge). When we simulate data from ‘real life’ with a probability distribution, we almost always lump Aleatoric and Epistemic Uncertainty together. However, we now start to use models that incorporate data to reduce the Epistemic Uncertainty.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#reflections",
    "href": "2.3-statistical_models.html#reflections",
    "title": "Statistical Models",
    "section": "",
    "text": "To learn about probability distributions, we first thought about the data generating process that made them, and then simulated data from models of those processes. This reflects the reality of an infinite number of possible probability distributions. We then focused on some named parametric probability distributions, which have names due to having several nice properties, which include the ability to calculate P(D|M) with [mostly] analytic solutions, and to simulate common processes, including some that nature tends to ‘converge to’, like the normal distribution.\nAs we proceed, we will mostly work with parametric probability distributions - but let’s not make the mistake of believing reality is only composed of these named parametric probability distributions.\nLet’s also remember that the variability we describe with statistics has two sources, Aleatoric Uncertainty (Randomness) and Epistemic Uncertainty (Lack of Knowledge). When we simulate data from ‘real life’ with a probability distribution, we almost always lump Aleatoric and Epistemic Uncertainty together. However, we now start to use models that incorporate data to reduce the Epistemic Uncertainty.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#preview",
    "href": "2.3-statistical_models.html#preview",
    "title": "Statistical Models",
    "section": "Preview",
    "text": "Preview\nPreviously we had models that sampled from across the whole sample space, now we look at models given conditions from a subset of the sample space. This conditioning essentially narrows our view, allowing us to ‘zoom in’ on an area of interest. However, variability still exists, and it depends on the problem whether the shape of the uncertainty has been modified by the conditioning.\nJust like in previous chapters, we focus on generating data to understand our models. This will allow us to quickly see the result of the underlying model assumptions that we would have been likely to miss otherwise.\nWe focus on model ‘errors’ and understanding how probable an error is (we will actually move away from using the word ‘error’ at all). This is what makes our models fundamentally statistical. Different models of data generating processes will have errors with different distributions, and it’s important we get the probability of our errors correct if we want accurate models. We build this on the foundation we laid in the previous two chapters on probability distributions.\nThis is in contrast to machine learning models, which we’ll tackle in the next chapter, which rely on loss functions, which may or may not be probabilistic.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#selecting-a-probability-distribution-by-conditioning-on-data",
    "href": "2.3-statistical_models.html#selecting-a-probability-distribution-by-conditioning-on-data",
    "title": "Statistical Models",
    "section": "Selecting a Probability Distribution by Conditioning on Data",
    "text": "Selecting a Probability Distribution by Conditioning on Data\nLet’s say we drive a car and each trip record the fuel use at different speeds. After many trips we have the following data:\n\nCar Speed vs Average Gas Mileage\n\n\n\n\n\n\n\nMiles Per Hour\nMeasured Miles Per Gallon (Sorry Metric System)\nStandard Deviation (\\(\\sigma\\))\n\n\n\n\n30\n32\n2\n\n\n45\n36\n3\n\n\n60\n37\n3\n\n\n75\n33\n2\n\n\n90\n26\n2\n\n\nGrand Mean\n34\n5\n\n\n\nIt’s important to note that the average miles per gallon will have some expected variability due to traffic, wind speed/direction, stopping frequency, etc. Let’s assume that variability is normally distributed, centered at the Measured Miles Per Gallon, and has a standard deviation as shown in the table. If we compile all our data of gas mileage for all speeds we get a grand mean of 31 mpg with a standard deviation of 5 miles per gallon.\nThe probability distributions from previous chapters are like the grand mean, they only knew about the whole sample space. However, now that we have more data, we can also get a conditional probability, \\(P(GasMileage \\mid Speed)\\), which stated out loud is the ‘probability of gas mileage given speed’. To be complete we should include all the variables, \\(P(GasMileage \\mid MilesPerHour, MeasuredMPG, \\sigma)\\).\nTo describe mileage at a specific speed, we can then write something like this \\(P(GasMileage \\mid Speed=45)\\), where we go back to the shorthand because we are implicitly referencing the table. Now we can look at the table and see at 45 Miles per Hour we measured 36 miles per gallon and 3 \\(\\sigma\\). Let’s point out that we are not ignorant enough to believe if we drove at 45 miles per hour again, that we would get exactly 36 miles per gallon - we expect to get about 36 miles per gallon, give or take based on the probabilities of the normal distribution.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#linear-regression",
    "href": "2.3-statistical_models.html#linear-regression",
    "title": "Statistical Models",
    "section": "Linear Regression",
    "text": "Linear Regression\nArmed with everything we learned so far, we turn to something something familiar but aim for a deeper understanding. We examine the perpetually useful topic of linear regression, but do not start by finding a best fit line through data. Instead, we assume a linear regression model and examine the data it generates.\n\nSingle Variable Linear Regression\nThe great thing about data generation with a model is that it is going to show you what it knows and not what you think it knows. Let’s walk through an example with a common dataset made available with the R package ggplot2 or the Python package Seaborn - it is the diamonds dataset which includes size/carats and sale price, among other diamond characteristics. The dataset displayed below as been made smaller via stratified sampling based on carats.\nPlay around with the app below based on linear regression and make some notes-to-self about the differences between the original and generated data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the code for simplicity\n# It is a stratified sample based on carats\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Diamond Price Modeling\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"num_points\", \"Number of Simulated Points\", \n                           min=100, max=1000, value=200, step=100),\n            ui.input_slider(\"noise_level\", \"Noise Level\", \n                           min=500, max=3000, value=1503, step=100),\n            ui.input_slider(\"slope\", \"Slope\", \n                           min=1000, max=10000, value=5000, step=500),\n            ui.input_slider(\"intercept\", \"Intercept\", \n                           min=-2000, max=2000, value=0, step=100),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_simulated\", \"Show Simulated Data\", True),\n            width=250\n        ),\n        ui.output_plot(\"diamondPlot\", height=\"500px\"),\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def generate_simulated_data():\n        # Use the user-defined slope and intercept\n        X_new = np.linspace(diamonds['carat'].min(), diamonds['carat'].max(), input.num_points())\n        slope = input.slope()\n        intercept = input.intercept()\n        \n        random_noise = np.random.normal(0, input.noise_level(), len(X_new))\n        Y_new = intercept + slope * X_new + random_noise\n        return pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n    @output\n    @render.plot\n    def diamondPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111)\n        \n        if input.show_original():\n            ax.scatter(diamonds['carat'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_simulated():\n            simulated_data = generate_simulated_data()\n            ax.scatter(simulated_data['carat'], simulated_data['price'], \n                      alpha=0.6, color='orange', label='Simulated Data')\n            \n            # Add the regression line\n            X_line = np.array([diamonds['carat'].min(), diamonds['carat'].max()])\n            Y_line = input.slope() * X_line + input.intercept()\n            ax.plot(X_line, Y_line, 'r--', \n                   label=f'y = {input.slope()}x + {input.intercept()}')\n        \n        ax.set_title('Diamond Carat vs Price: Original and Simulated Data')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Price')\n        ax.grid(True)\n        ax.legend()\n        \n        fig.canvas.draw_idle()\n        plt.close()\n        \n        return fig\n\napp = App(app_ui, server)\n\nModel Flaws\nAfter seeing it work, let’s point out some defencies in this model:\n\nIt seems any reasonable setting produces some negative values at small carats. We don’t think this is real because no one has paid us to take a diamond before.\nThere are some important ‘break points’ in the data that the linear model is not capturing, like the extra premium placed on a 1.00 carat diamond vs 0.99 carats.\nThe variation around the mean value in the generated data is the same regardless of whether it is 0.3 or 1.5 carats, but that doesn’t match the data.\n\nWe will work to correct these problems, but we will start on the last bullet first. The change in variability from small carat values to large values is called Heteroscedasticity, a fine word for a dinner party, and actually quite common. However, when a model underpredicts variation at extreme values, points in those areas will have far too much ‘leverage’ on the model fit. Practically all linear models assume constant variance, which is frequently an erroneous assumption. To understand what’s happening, we need some math.\n\n\nLinear Regression, Bad, Good, and Even Better Notation\n\nThe Bad\nSingle variable linear regression model has this classical mathematical form:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the the slope, and \\(\\epsilon\\) is the error. In the diamond price model the \\(x_i\\) variable is a diamond size/carat and \\(y_i\\) is the predicted price. We’d like to take this opportunity to nitpick the classical form, as it implies that we are modeling a deterministic process, and that variations from that deterministic process constitute ‘errors’.\n\n\nThe Good\nA more statistically genuine, and equivalent, way to think about linear regression is:\n\\[\ny_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)\n\\]\nThis notation says that \\(y_i\\), the predicted price, is normally distributed (the script N) with a mean of \\(\\beta_0 + \\beta_1 x_i\\) and a standard deviation of \\(\\sigma\\). This makes it obvious that each prediction has some expected variation, and that variation is the same, i.e. constant, regardless the value of x or y. In the generated data in the app above, this is why the ‘noise’ around the line was always constant along the line.\n\n\nEven Better\nThere’s another level of insight to gain as well which is related to our earlier example of Probability Distributions Conditioned on Data. We can modify the notation we use for our linear regression model to this:\n\\[\nP(y_i \\mid x_i) \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)\n\\]\nWhich states that the probability of \\(y_i\\) given \\(x_i\\) is normally distributed as previously described. (Stating \\(P(y_i \\mid \\beta_0, \\beta_1, x_i, \\sigma)\\) is more explicit, but since everything but \\(x_i\\) is constant, we use the implicit notation to make the point more clearly). The important insight with this slight change in notation is that the predicted value is conditioned on the input data. This is just like the gas mileage example given at the beginning of the chapter.\n\n\n\nKey Takeaway\nWhen we give a specific input, such as the size of the diamond in carats, we essentially ignore all other carat sizes, and get a probability distribution at just that point - just like when we chose a specific speed for gas mileage, we ignored all the other speeds and got the probability distribution for that row.\nThis is what data does in our models - it narrows the sample space so we can examine a more focused probability distribution. Of course, how well this more focused probability distribution represents reality is very much related to how ‘good’ our model is. And we’ve already stated that our current linear regression model has some flaws.\n\n\nAccounting for Heteroscadicity in a Linear Model\nLet’s modify the model to tackle the issue of heteroscadicity. It will not become obvious why until the second half of the primer, but this will break the standard assumptions of linear regression models and the ability to fit it using standard analytical solutions. (Thus the reason so many linear models exist with this flaw regardless of how common heteroscadicity is).\nHere’s a new mathematical model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + x_i \\epsilon\n\\]\nOr in the notation we prefer:\n\\[\nP(y_i \\mid x_i) \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2 x_i)\n\\]\nHere’s the new app.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the code for simplicity\n# It is a stratified sample based on carats\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Diamond Price Modeling\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"num_points\", \"Number of Simulated Points\", \n                           min=100, max=1000, value=200, step=100),\n            ui.input_slider(\"noise_level\", \"Noise Level\", \n                           min=500, max=3000, value=1503, step=100),\n            ui.input_slider(\"slope\", \"Slope\", \n                           min=1000, max=10000, value=5000, step=500),\n            ui.input_slider(\"intercept\", \"Intercept\", \n                           min=-2000, max=2000, value=0, step=100),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_simulated\", \"Show Simulated Data\", True),\n            width=250\n        ),\n        ui.output_plot(\"diamondPlot\", height=\"500px\"),\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def generate_simulated_data():\n        # Use the user-defined slope and intercept\n        X_new = np.linspace(diamonds['carat'].min(), diamonds['carat'].max(), input.num_points())\n        slope = input.slope()\n        intercept = input.intercept()\n        \n        random_noise = np.random.normal(0, input.noise_level(), len(X_new))\n        Y_new = intercept + slope * X_new + X_new * random_noise\n        return pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n    @output\n    @render.plot\n    def diamondPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111)\n        \n        if input.show_original():\n            ax.scatter(diamonds['carat'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_simulated():\n            simulated_data = generate_simulated_data()\n            ax.scatter(simulated_data['carat'], simulated_data['price'], \n                      alpha=0.6, color='orange', label='Simulated Data')\n            \n            # Add the regression line\n            X_line = np.array([diamonds['carat'].min(), diamonds['carat'].max()])\n            Y_line = input.slope() * X_line + input.intercept()\n            ax.plot(X_line, Y_line, 'r--', \n                   label=f'y = {input.slope()}x + {input.intercept()}')\n        \n        ax.set_title('Diamond Carat vs Price: Original and Simulated Data')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Price')\n        ax.grid(True)\n        ax.legend()\n        \n        fig.canvas.draw_idle()\n        plt.close()\n        \n        return fig\n\napp = App(app_ui, server)\nThat can fit the change in variance with price much better, although it could still use some improvements. We’d like to stress that very few people would have noticed the flaw in the previous linear regression model unless we had used it to generate data.\n\n\n\nMulti-Variable Linear Regression\nOur model accuracy is also suffering from some epistemic (knowledge) uncertainty. Initially we included only size/carats, which we intuited would be the most important predictor variable. However, there are other possibly useful predictor variables, including depth, cut, color, and clarity.\nHere’s a view of the raw data. We will not include all the variables in our multi-variable linear regression model, but it’s good to see what is avaialable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n51657\n0.3\nIdeal\nG\nVS2\n62.3\n58.0\n545\n4.26\n4.28\n2.66\n\n\n34838\n0.3\nPremium\nG\nVVS2\n60.8\n58.0\n878\n4.38\n4.34\n2.65\n\n\n9718\n0.3\nIdeal\nH\nVVS2\n62.1\n54.0\n590\n4.32\n4.35\n2.69\n\n\n46635\n0.3\nVery Good\nE\nSI1\n62.7\n60.0\n526\n4.24\n4.28\n2.67\n\n\n31852\n0.3\nPremium\nG\nVS1\n62.2\n59.0\n776\n4.28\n4.24\n2.65\n\n\n\nIt’s very hard to visualize and fit models by hand in multiple dimensions, so this is a rare instance in the first half of the primer where we show a pre-fitted model. Another advantage of doing so allows us to focus this app on just the residuals/errors, which are the difference between the model predicted values and the actual values. In the app below, observe how the R-squared accuracy metric changes as you include different variables.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the same format as before\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Multiple Linear Regression Diamond Price Modeling\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_checkbox_group(\n                \"predictors\",\n                \"Select Predictors:\",\n                choices=[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\"],\n                selected=[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\"]\n            ),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_predicted\", \"Show Predicted Values\", True),\n            width=250\n        ),\n        ui.output_plot(\"regressionPlot\"),\n        ui.output_text(\"modelSummary\"),\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def fit_multiple_regression():\n        # Convert input.predictors() to list and ensure at least one predictor is selected\n        predictors = list(input.predictors())\n        if not predictors:\n            predictors = [\"carat\"]  # default to carat if nothing is selected\n        \n        # Separate numerical and categorical columns\n        numerical_features = [col for col in predictors if col in ['carat', 'depth']]\n        categorical_features = [col for col in predictors if col in ['cut', 'color', 'clarity']]\n        \n        # Create preprocessing steps\n        numeric_transformer = StandardScaler()\n        categorical_transformer = OneHotEncoder(drop='first')\n        \n        # Combine preprocessing steps\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numeric_transformer, numerical_features),\n                ('cat', categorical_transformer, categorical_features)\n            ])\n        \n        # Create pipeline\n        model = Pipeline([\n            ('preprocessor', preprocessor),\n            ('regressor', LinearRegression())\n        ])\n        \n        # Fit the model\n        X = diamonds[predictors]\n        y = diamonds['price']\n        model.fit(X, y)\n        \n        # Get predictions\n        y_pred = model.predict(X)\n        \n        # Calculate R-squared\n        r2 = model.score(X, y)\n        \n        # Get feature names after transformation\n        feature_names = (\n            numerical_features +\n            sum([[f\"{feat}_{val}\" for val in diamonds[feat].unique()[1:]]\n                 for feat in categorical_features], [])\n        )\n        \n        # Get coefficients\n        coefficients = model.named_steps['regressor'].coef_\n        \n        return {\n            'predictions': y_pred,\n            'coefficients': dict(zip(feature_names, coefficients)),\n            'r2': r2,\n            'intercept': model.named_steps['regressor'].intercept_\n        }\n\n    @output\n    @render.plot\n    def regressionPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111)\n        \n        model_results = fit_multiple_regression()\n        \n        if input.show_original():\n            ax.scatter(diamonds['price'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_predicted():\n            ax.scatter(diamonds['price'], model_results['predictions'], \n                      alpha=0.6, color='red', label='Predicted Values')\n            \n        # Add perfect prediction line\n        min_price = min(diamonds['price'])\n        max_price = max(diamonds['price'])\n        ax.plot([min_price, max_price], [min_price, max_price], \n                'k--', label='Perfect Prediction')\n        \n        ax.set_title('Actual vs Predicted Diamond Prices')\n        ax.set_xlabel('Actual Price')\n        ax.set_ylabel('Predicted Price')\n        ax.grid(True)\n        ax.legend()\n        \n        return fig\n\n    @output\n    @render.text\n    def modelSummary():\n        model_results = fit_multiple_regression()\n        \n        # Calculate RMSE\n        predictions = model_results['predictions']\n        actual = diamonds['price']\n        rmse = np.sqrt(np.mean((predictions - actual) ** 2))\n        \n        summary = f\"R-squared: {model_results['r2']:.3f}\\n\"\n        # summary += f\"RMSE: ${rmse:.2f}\\n\\n\"\n        \n        return summary\n\napp = App(app_ui, server)\n\nYou should notice that the other variables are helpful in improving the fit of the model, including reducing the heteroscedasticity. However, some are more useful than others and none are close to the importance of size/carats.\nA discussion on including the ‘right’ variables is a very lengthy topic. We’ll summarize it by only saying the obvious - if the variable has no ability to make better predictions, it should not be included. However, even this statement has a nuance - we said predictions since even completely unrelated variables can make the model appear to fit the existing data better, but ultimately predict new data worse. A variable can also be not useful enough to warrant the additional model complexity of including it.\n\nMathematical Form\nThe multi-variate regression equation has this generic mathematical form:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_n x_{n,i} + \\epsilon\n\\]\nOr in the notation we prefer:\n\\[\nP(y_i \\mid x_{1,i}, x_{2,i} \\dots x_{3,i} ) \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_n x_{n,i}, \\sigma^2)\n\\]\nNote that we have gone back to the traditional form of multi-variate regression where the \\(\\epsilon\\)/\\(\\sigma\\) term is constant. Also, note that regarding the subscripts on the \\(x\\) variables, the first position refers to which predictor variable (like carats or clarity), and the second refers the i’th observation (like which row in a table). Now having seen the data and the equation above, you may be scratching your head as to how the non-numeric values in the data were incorporated into the model.\n\n\nNon-Numeric Inputs\n\nCategorical\nLinear Models have the ability to consume non-numeric data, although you may consider it to be inelegant. Effectively we need a parameter for every possible non-numeric data value, such as when cut = Ideal, cut = Premium, or cut = Good. As you can imagine this can lead to a lot of extra parameters, each of which you may call an indicator variable coefficient. Each parameter adjusts the output accordingly if the non-numeric data value is present in the input, otherwise it does nothing.\n\n\nOrdinal Data\nThere’s actually another option for non-numeric data, and that’s if the data is ordinal. Ordinal data is non-numeric, but it has a natural progression, such as Bad, OK, Good. If we expect that the impact of each step in our ordinal data is approximately the same (the impact of Bad to OK is about the same as OK to Good), we can significantly reduce the number of parameters needed to just one per category. The data becomes a set of integers that map to the qualitative value in the correct order, and the parameter is then predicting the impact of a step in the ordinal value.\n\n\nAn Aside on Overfitting\nMore parameters significantly increases the risk of overfitting, which is fitting to noise instead of signal. It is dangerous because a model can fit existing data extremely well but then do a terrible job when it matters most, at predicting future data.\nBecause of the many different combinations of diamond characteristics and the inherent variability in the sales price, a perculiarity of a particular dataset may create an indicator variable parameter where OK is better than Good. However, if we use ordinal values, this will not happen. This is an important case where a human with a causal understanding of the data generating process knows better than a statistical procedure.\nIncluding known structural constraints in the model and limiting the model parameters is a great way to minimize the risk of overfitting.\n\n\n\n\nGeneralized Linear Models\nWhat’s learned with linear regression can be extended to many datasets that are not linear, but can be transformed to look linear. Typically the transformation will also assume that the variability, \\(\\epsilon\\) or \\(\\sigma\\), follows another distribution. In linear regression the assumption is a normal distribution, but transformations can assume, for example Binomial, Poisson, or Bernoulli distributions.\nLogistic regression is useful for taking a set of predictor variables and using them to make a binary, e.g. yes or no, decision. If the impact of each predictor variable is consistently proportional to its value, i.e. the impact is linear, then the model form is a good fit.\nHere’s logistic regression in the log-odds form, which makes the linearity of the inputs obvious:\n\\[\n\\ln \\left( \\frac{P(y_i = 1 \\mid x_{1,i}, x_{2,i} \\dots x_{3,i})}{1 - P(y_i = 1 \\mid x_{1,i}, x_{2,i} \\dots x_{3,i})} \\right) = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_n x_{n,i}\n\\]\nHere’s the more natural form for evaluating the function:\n\\[\nP(y_i = 1 \\mid x_{1,i}, x_{2,i} \\dots x_{3,i}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_n x_{n,i})}}\n\\]\nwhere:\n\n\\(P(y_i = 1 \\mid x_{1,i}, x_{2,i} \\dots x_{3,i})\\) is the probability that \\(y_i\\) equals 1 given the predictor variables \\(x_{1,i}, x_{2,i} \\dots x_{3,i}\\).\n\\(\\beta_0\\) is the intercept term and \\(\\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients for the predictor variables.\nThe outputs vary from 0 to 1 and can be interpreted as probabilities from a Bernoulli distribution. There is no error term since the output is already intpreted as probabilistic.\n\nIn the app below, we have a predictor variable X, and we have some data that shows that typically when X is larger, we should predict 1, and when X is smaller, we should predict 0. There is some overlap in the middle but the signal is quite clear at large or small values of X. The app lets you choose a standard linear model or a logistic model, which is a generalized linear model.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear vs Logistic Regression Comparison\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"separation\", \"Class Separation\", \n                           min=0.5, max=5.0, value=2.0, step=0.5),\n            ui.input_slider(\"class_balance\", \"Class Balance (% of Class 1)\", \n                           min=10, max=90, value=50),\n            ui.input_slider(\"threshold\", \"Decision Threshold\", \n                           min=0.0, max=1.0, value=0.5, step=0.05),\n            ui.input_slider(\"noise\", \"Noise Level\", \n                           min=0.1, max=2.0, value=0.5, step=0.1),\n            ui.output_text(\"description\"),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Linear Regression\",\n                ui.output_plot(\"linearPlot\", height=\"400px\"),\n                ui.br(),\n                ui.output_text(\"linear_metrics\"),\n            ),\n            ui.nav_panel(\"Logistic Regression\",\n                ui.output_plot(\"logisticPlot\", height=\"400px\"),\n                ui.br(),\n                ui.output_text(\"logistic_metrics\"),\n            ),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def generate_data():\n        n = 40  # Fixed sample size\n        separation = input.separation()\n        class_1_prop = input.class_balance() / 100\n        noise_level = input.noise()\n        \n        # Generate balanced X values for each class\n        n1 = int(n * class_1_prop)\n        n0 = n - n1\n        \n        X0 = np.random.normal(-separation/2, 1, n0).reshape(-1, 1)\n        X1 = np.random.normal(separation/2, 1, n1).reshape(-1, 1)\n        X = np.vstack([X0, X1])\n        \n        # Generate labels\n        y = np.hstack([np.zeros(n0), np.ones(n1)])\n        \n        # Add noise to X\n        X = X + np.random.normal(0, noise_level, X.shape)\n        \n        return X, y\n\n    @output\n    @render.text\n    def description():\n        return (\n            f\"Model Settings:\\n\"\n            f\"• Fixed sample size: 40\\n\"\n            f\"• Separation between classes: {input.separation():.1f}\\n\"\n            f\"• Noise level: {input.noise():.1f}\\n\"\n            f\"• Decision threshold: {input.threshold():.2f}\"\n        )\n\n    @output\n    @render.plot\n    def linearPlot():\n        X, y = generate_data()\n        threshold = input.threshold()\n        \n        # Fit linear model\n        lr = LinearRegression()\n        lr.fit(X, y)\n        \n        # Generate predictions\n        X_pred = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n        y_lr_pred = lr.predict(X_pred)\n        \n        # Create plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.scatter(X[y==0], y[y==0], color='blue', alpha=0.5, label='Class 0')\n        ax.scatter(X[y==1], y[y==1], color='red', alpha=0.5, label='Class 1')\n        ax.plot(X_pred, y_lr_pred, 'g-', label='Linear Regression')\n        ax.axhline(y=threshold, color='purple', linestyle='--', \n                  label=f'Threshold ({threshold:.2f})')\n        \n        ax.set_title(\"Linear Regression for Classification\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Probability\")\n        ax.legend()\n        ax.set_ylim(-0.2, 1.2)\n        \n        return fig\n\n    @output\n    @render.plot\n    def logisticPlot():\n        X, y = generate_data()\n        threshold = input.threshold()\n        \n        # Fit logistic model\n        log_reg = LogisticRegression()\n        log_reg.fit(X, y)\n        \n        # Generate predictions\n        X_pred = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n        y_log_pred_prob = log_reg.predict_proba(X_pred)[:, 1]\n        \n        # Create plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.scatter(X[y==0], y[y==0], color='blue', alpha=0.5, label='Class 0')\n        ax.scatter(X[y==1], y[y==1], color='red', alpha=0.5, label='Class 1')\n        ax.plot(X_pred, y_log_pred_prob, 'g-', label='Logistic Regression')\n        ax.axhline(y=threshold, color='purple', linestyle='--', \n                  label=f'Threshold ({threshold:.2f})')\n        \n        ax.set_title(\"Logistic Regression\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Probability\")\n        ax.legend()\n        ax.set_ylim(-0.2, 1.2)\n        \n        return fig\n\n    @output\n    @render.text\n    def linear_metrics():\n        X, y = generate_data()\n        threshold = input.threshold()\n        \n        lr = LinearRegression()\n        lr.fit(X, y)\n        y_lr_pred_prob = lr.predict(X)\n        y_lr_pred = (y_lr_pred_prob &gt; threshold).astype(int)\n        \n        acc = accuracy_score(y, y_lr_pred)\n        prec = precision_score(y, y_lr_pred, zero_division=0)\n        rec = recall_score(y, y_lr_pred, zero_division=0)\n        \n        return (f\"Metrics:\\n\"\n                f\"• Accuracy: {acc:.3f}\\n\"\n                f\"• Precision: {prec:.3f}\\n\"\n                f\"• Recall: {rec:.3f}\")\n\n    @output\n    @render.text\n    def logistic_metrics():\n        X, y = generate_data()\n        threshold = input.threshold()\n        \n        log_reg = LogisticRegression()\n        log_reg.fit(X, y)\n        y_log_pred_prob = log_reg.predict_proba(X)[:, 1]\n        y_log_pred = (y_log_pred_prob &gt; threshold).astype(int)\n        \n        acc = accuracy_score(y, y_log_pred)\n        prec = precision_score(y, y_log_pred, zero_division=0)\n        rec = recall_score(y, y_log_pred, zero_division=0)\n        \n        return (f\"Metrics:\\n\"\n                f\"• Accuracy: {acc:.3f}\\n\"\n                f\"• Precision: {prec:.3f}\\n\"\n                f\"• Recall: {rec:.3f}\")\n\napp = App(app_ui, server)\nWhat we should notice is that the linear interpretaion does not make much sense, as it will eventually predict negative probabilities for small X and probabilities over 1 for large X. Logistic regression is a much more natural fit for the data and the domain of the problem.\nA quick aside you may find interesting - we choose to include logistic regression as it is a preview to a piece of the Bayesian Neural Networks we tackle in the next chapter. You may be surprised to find out how many key pieces you’ve already been introduced to.\n\n\nBespoke/Bayesian Models\nWhen we added the non-constant error term to our linear regression model, we headed off the well-trod path of standard statistical models with analytical solutions. Heading off that well-trod path often leads to Bayesian statistics and causal modeling. We have intentionally avoided a discussion of Bayesian and Frequentist statistics (a discussion of which is easily found elsewhere), but suffice it to say that the primer up until this point has effectively taken a Frequentist viewpoint. To oversimplify, since our discussion of parametric probability distributions considered no uncertainty in the parameters themselves, it was inherently a Frequentist introduction. Furthermore, we have not incorporated any prior knowledge when estimating the parameter values, although we will in the second half of the primer.\nHere are a couple key elements we want to include as we move toward models without analytical solutions:\n\nCausal diagrams. These can have more technical uses, but at the very least they communicate our model intent.\nA way to find likely parameter values for our models without analytical solutions. This will wait until the second half of the primer.\n\n\nCausal Diagrams\nAs mentioned, a causal diagram is a multi-use tool, but at the very least they will communicate our intent, and that often tends to also hone/refine thinking at the same time. Here’s what we want in a diamond model:\n\n\n\n\n\ngraph LR\n    cut --&gt; linear_regression\n    color --&gt; linear_regression\n    clarity --&gt; linear_regression\n    size --&gt; linear_regression\n    size --&gt; carat_threshold\n    linear_regression --&gt; price\n    carat_threshold --&gt; price\n    size --&gt; variance\n    variance --&gt; price\n\n\n\n\n\n\nThis clearly shows that we intend cut, color, clarity, and size to be used in a linear regression model for price, that size will also be used for a carat threshold model for price, and finally that size will also be used to model the variance for price. The structure of the diagram, i.e. the presence of colliders, forks, backdoor paths etc. can give clarity on dependence, independence, and spurious correleations, but for more information you should seek out other references.\n\n\n\nModel\nHere’s a model that includes the basic elements of the causal diagram, but only has carat size as a predictor to keep the user interface from being overwhelming. We trust you can imagine more inputs to adjust for other predictor variables.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the code for simplicity\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Diamond Price Modeling\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"noise_level\", \"Noise Level\", \n                           min=500, max=3000, value=1503, step=100),\n            ui.input_slider(\"slope\", \"Slope\", \n                           min=1000, max=10000, value=5000, step=500),\n            ui.input_slider(\"intercept\", \"Intercept\", \n                           min=-2000, max=2000, value=0, step=100),\n            ui.input_numeric(\"threshold_05\", \"Price Add at 0.5 Carats ($)\", \n                           value=0, min=0, max=10000),\n            ui.input_numeric(\"threshold_10\", \"Price Add at 1.0 Carats ($)\", \n                           value=0, min=0, max=10000),\n            ui.input_numeric(\"threshold_15\", \"Price Add at 1.5 Carats ($)\", \n                           value=0, min=0, max=10000),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_simulated\", \"Show Simulated Data\", True),\n            width=250\n        ),\n        ui.output_plot(\"diamondPlot\", height=\"500px\"),\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def generate_simulated_data():\n        # Use the user-defined slope and intercept\n        X_new = np.linspace(diamonds['carat'].min(), diamonds['carat'].max(), 400)\n        slope = input.slope()\n        intercept = input.intercept()\n        \n        random_noise = np.random.normal(0, input.noise_level(), len(X_new))\n        Y_new = intercept + slope * X_new + X_new * random_noise\n        \n        # Add price jumps at thresholds\n        Y_new += np.where(X_new &gt;= 0.5, input.threshold_05(), 0)\n        Y_new += np.where(X_new &gt;= 1.0, input.threshold_10(), 0)\n        Y_new += np.where(X_new &gt;= 1.5, input.threshold_15(), 0)\n        \n        return pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n    @output\n    @render.plot\n    def diamondPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111)\n        \n        if input.show_original():\n            ax.scatter(diamonds['carat'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_simulated():\n            simulated_data = generate_simulated_data()\n            ax.scatter(simulated_data['carat'], simulated_data['price'], \n                      alpha=0.6, color='orange', label='Simulated Data')\n            \n            # Add vertical lines at thresholds\n            if input.threshold_05() &gt; 0:\n                ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.3)\n            if input.threshold_10() &gt; 0:\n                ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.3)\n            if input.threshold_15() &gt; 0:\n                ax.axvline(x=1.5, color='gray', linestyle='--', alpha=0.3)\n        \n        ax.set_title('Diamond Carat vs Price: Original and Simulated Data')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Price')\n        ax.grid(True)\n        ax.legend()\n        \n        fig.canvas.draw_idle()\n        plt.close()\n        \n        return fig\n\napp = App(app_ui, server)\nWe’d say this is becoming a fairly good model for estimating diamond sale price.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#probability-of-the-data",
    "href": "2.3-statistical_models.html#probability-of-the-data",
    "title": "Statistical Models",
    "section": "Probability of the Data",
    "text": "Probability of the Data\nAs usual, we started by focusing on the data generated from our models to improve our intution. When writing the standard equation for single variable linear regression, it’s hard to understand what ‘probability of the data’ would even mean:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon\n\\]\nHowever, in our preferred notation, that should become quite a bit more obvious:\n\\[\nP(y_i \\mid x_i) \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)\n\\]\n\nProbability of Data with Initial Single Variable Linear Regression\nTODO: Probability of data for small and large carats, show how large carats is too unlikely.\nWe will consider the probability of data based on our original/classical linear regression model. Hopefully you remember how we can generate data from a statistical model and use that to approximate probabilities of the data - this is always a reasonable option - however, since we are explicitly assuming that the variability in our model follows a normal distribution, we can calculate probabilities faster by using existing analytical solutions.\nWe are still waiting until the second half to get optimally fitted models, but we can do a decent job by eyeballing it with the apps we used earlier. Playing around with the parameters of the first app, we think these are some reasonable settings, all things considered:\n\nNoise Level/Standard Deviation = 1,400\nSlope = 6,500\nIntercept = -1,200\n\nWe’ll use these settings to calculate the probability of certain sale prices.\n\nProbability of Greater Price at a Small Carat Value\nHere we can simplify our equation to get the parameters of the normal distribution:\n\\[\nP(y_i \\mid x_i=0.45) \\sim \\mathcal{N}(-1200 + 6500 \\cdot 0.45, 1400^2) = \\sim \\mathcal{N}(1725, 1400^2)\n\\]\nOnce we have those parameters, we can use any statistical software to calculate probabilities. Here are some probabilities for selling a 0.45 carat diamond at the given price or higher:\n\n$1,725: 50%\n$2,000: 42%\n$3,000: 18%\n\nHowever, if we look back at the app and eyeball the actual sale prices, nothing in that carat size sold for anything close to these dollar values. While this is partially due to a poor estimate of the average price, it is also very much due to an overestimated standard deviation at 0.45 carats as well.\n\n\nProbability of Greater Price at a Large Carat Value\nNow we try the same thing for a much larger diamond:\n\\[\nP(y_i \\mid x_i=1.55) \\sim \\mathcal{N}(-1200 + 6500 \\cdot 1.55, 1400^2) = \\sim \\mathcal{N}(8875, 1400^2)\n\\]\nHere are some probabilities for selling a 1.55 carat diamond at the given price or higher:\n\n$8,875: 50%\n$10,000: 21%\n$12,000: 1%\n\nAgain, if we look back at the sale prices, in a fairly small number of sales at that price, several are above $12,000, so these estimates again seem terrible. Here, the estimate of the average sale price was somewhat low, but also now due to an underestimate of the standard deviation at 1.55 carats.\n\n\nSummary\nDue to some poor model assumptions, the probabilities we calculated here do not seem very reasonable. Ultimately this makes both P(D|M) and P(M|D) non-credible. We’ll continue in the next section and see if we can do better.\n\n\n\nProbability of the Data with Revised Single Variable Linear Regression\nTODO: Probability of data for small and large carats, show how it seems more reasonable. Worth the space???",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#summary-1",
    "href": "2.3-statistical_models.html#summary-1",
    "title": "Statistical Models",
    "section": "Summary",
    "text": "Summary\nTODO: Revise this once finished with the chapter. Or discard altogether.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2.3-statistical_models.html#graveyard",
    "href": "2.3-statistical_models.html#graveyard",
    "title": "Statistical Models",
    "section": "Graveyard",
    "text": "Graveyard\nAnother try on the linear regression with more charts:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the code for simplicity\n# It is a stratified sample based on carats\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Diamond Price Modeling\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"num_points\", \"Number of Simulated Points\", \n                           min=100, max=1000, value=200, step=100),\n            ui.input_slider(\"noise_level\", \"Noise Level\", \n                           min=500, max=3000, value=1503, step=100),\n            ui.input_slider(\"slope\", \"Slope\", \n                           min=1000, max=10000, value=5000, step=500),\n            ui.input_slider(\"intercept\", \"Intercept\", \n                           min=-2000, max=2000, value=0, step=100),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_simulated\", \"Show Simulated Data\", True),\n            width=250\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Main Plot\",\n                ui.output_plot(\"diamondPlot\", height=\"400px\")\n            ),\n            ui.nav_panel(\"Residual Distribution\",\n                ui.output_plot(\"residualPlot\", height=\"250px\")\n            ),\n            ui.nav_panel(\"Residual vs Carat\",\n                ui.output_plot(\"residualScatter\", height=\"400px\")\n            )\n        )\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def generate_simulated_data():\n        X_new = np.linspace(diamonds['carat'].min(), diamonds['carat'].max(), input.num_points())\n        slope = input.slope()\n        intercept = input.intercept()\n        \n        random_noise = np.random.normal(0, input.noise_level(), len(X_new))\n        Y_new = intercept + slope * X_new + random_noise\n        return pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n    @output\n    @render.plot\n    def diamondPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 5))\n        ax = fig.add_subplot(111)\n        \n        if input.show_original():\n            ax.scatter(diamonds['carat'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_simulated():\n            simulated_data = generate_simulated_data()\n            ax.scatter(simulated_data['carat'], simulated_data['price'], \n                      alpha=0.6, color='orange', label='Simulated Data')\n            \n            # Add the regression line\n            X_line = np.array([diamonds['carat'].min(), diamonds['carat'].max()])\n            Y_line = input.slope() * X_line + input.intercept()\n            ax.plot(X_line, Y_line, 'r--', \n                   label=f'y = {input.slope()}x + {input.intercept()}')\n        \n        ax.set_title('Diamond Carat vs Price: Original and Simulated Data')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Price')\n        ax.grid(True)\n        ax.legend()\n        \n        return fig\n\n    @output\n    @render.plot\n    def residualPlot():\n        if not input.show_simulated():\n            return None\n            \n        # Calculate residuals for actual data\n        predicted_prices = input.slope() * diamonds['carat'] + input.intercept()\n        residuals = diamonds['price'] - predicted_prices\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 2.5))\n        fig.suptitle('Residual Distribution by Carat Range (Original Data)')\n        \n        # Define carat ranges\n        small_mask = diamonds['carat'] &lt;= 1.0\n        large_mask = diamonds['carat'] &gt; 1.0\n        \n        # Calculate common x-axis limits\n        x_min = min(residuals)\n        x_max = max(residuals)\n        x_range = x_max - x_min\n        x_min -= x_range * 0.05\n        x_max += x_range * 0.05\n        \n        # Plot for diamonds &lt;= 1 carat\n        small_residuals = residuals[small_mask]\n        if len(small_residuals) &gt; 0:\n            ax1.hist(small_residuals, bins=12, edgecolor='black')\n            ax1.set_title('Residuals for ≤ 1 carat')\n            ax1.set_xlabel('Residual')\n            ax1.set_ylabel('Count')\n            ax1.set_xlim(x_min, x_max)\n        else:\n            ax1.text(0.5, 0.5, 'No data in this range', \n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    transform=ax1.transAxes)\n        \n        # Plot for diamonds &gt; 1 carat\n        large_residuals = residuals[large_mask]\n        if len(large_residuals) &gt; 0:\n            ax2.hist(large_residuals, bins=12, edgecolor='black')\n            ax2.set_title('Residuals for &gt; 1 carat')\n            ax2.set_xlabel('Residual')\n            ax2.set_ylabel('Count')\n            ax2.set_xlim(x_min, x_max)\n        else:\n            ax2.text(0.5, 0.5, 'No data in this range', \n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    transform=ax2.transAxes)\n        \n        plt.tight_layout()\n        return fig\n\n    @output\n    @render.plot\n    def residualScatter():\n        if not input.show_simulated():\n            return None\n            \n        # Calculate residuals for actual data\n        predicted_prices = input.slope() * diamonds['carat'] + input.intercept()\n        residuals = diamonds['price'] - predicted_prices\n        \n        fig = plt.figure(figsize=(10, 5))\n        ax = fig.add_subplot(111)\n        \n        ax.scatter(diamonds['carat'], abs(residuals), \n                  alpha=0.6, color='blue', label='Absolute Residuals')\n        ax.set_title('Absolute Residuals vs Carat')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Absolute Residual')\n        ax.grid(True)\n        \n        return fig\n\napp = App(app_ui, server)\nRegression probability of data:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Statistical Models"
    ]
  },
  {
    "objectID": "2-data_generation.html",
    "href": "2-data_generation.html",
    "title": "Probability of Data",
    "section": "",
    "text": "Please select a chapter in the left hand menu.",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html",
    "href": "3.X-Naive_Monte_Carlo.html",
    "title": "Naive Monte Carlo",
    "section": "",
    "text": "We use the term naive to distinguish this method from more advanced Monte Carlo methods. The advantage here will be simplicity, but it will not be a good solution with many parameters (due to the spareseness of high dimensional space), or where we do not have good priors.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re wondering why generate data for examples instead of using ‘real’ data sets, it’s because it’s the only way to have certainty in the data generating process. This allows us to see how well we fit the model. Obviously the goal is real data - but unfortunately you’ll probably never know exactly what the data generating process was, and that makes understanding the accuracy of ‘new’ methods very difficult."
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html#intro",
    "href": "3.X-Naive_Monte_Carlo.html#intro",
    "title": "Naive Monte Carlo",
    "section": "",
    "text": "We use the term naive to distinguish this method from more advanced Monte Carlo methods. The advantage here will be simplicity, but it will not be a good solution with many parameters (due to the spareseness of high dimensional space), or where we do not have good priors.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re wondering why generate data for examples instead of using ‘real’ data sets, it’s because it’s the only way to have certainty in the data generating process. This allows us to see how well we fit the model. Obviously the goal is real data - but unfortunately you’ll probably never know exactly what the data generating process was, and that makes understanding the accuracy of ‘new’ methods very difficult."
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html#bayesian-methods",
    "href": "3.X-Naive_Monte_Carlo.html#bayesian-methods",
    "title": "Naive Monte Carlo",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\nFor determining the \\(P(M|D)\\), we will use Bayesian methods. There’s a whole historical debate you could research, but when it comes to most scientific and engineering subjects, you wouldn’t be studying the subject if you didn’t have some prior information, even if you can only describe it as intuition. This prior information is valuable, and it should be included in the model. Priors are also valueable in more bespoke models due to their ability to limit solutions to those known to be plausible…\nAllen Downey said it well when he stated in Think Bayes that if don’t have much data you should use Bayesian methods, and when you have lots of data, it dominates the priors so you can still use Bayesian methods… (research the real quote)."
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html#dataset",
    "href": "3.X-Naive_Monte_Carlo.html#dataset",
    "title": "Naive Monte Carlo",
    "section": "Dataset",
    "text": "Dataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# =========================\n# Step 1: Generate Data\n# =========================\n\n# True parameters\nmu_true = 5.5  # average height in feet\nsigma_true = 0.3  # standard deviation in feet\n\n# Generate dataset of 100 persons\ndataset_size = 100\nheights = np.random.normal(loc=mu_true, scale=sigma_true, size=dataset_size)\n\n# Plot the generated heights\nplt.figure(figsize=(10, 6))\nsns.histplot(heights, bins=15, kde=True, color='skyblue')\nplt.title('Histogram of Generated Heights (100 Persons)')\nplt.xlabel('Height (feet)')\nplt.ylabel('Frequency')\nplt.show()"
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html#priors",
    "href": "3.X-Naive_Monte_Carlo.html#priors",
    "title": "Naive Monte Carlo",
    "section": "Priors",
    "text": "Priors\n\n# =========================\n# Step 2: Define Priors (Corrected)\n# =========================\n\n# Define prior ranges\nmu_prior_min = 4.0  # feet\nmu_prior_max = 7.0  # feet\nsigma_prior_min = 0.0\nsigma_prior_max = 3 * sigma_true  # 0.9 feet\n\n# Plot the prior distributions\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Prior for mu\nmu_values = np.linspace(mu_prior_min, mu_prior_max, 1000)\nmu_prior = np.ones_like(mu_values) / (mu_prior_max - mu_prior_min)\nax[0].plot(mu_values, mu_prior, color='blue')\nax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\nax[0].set_xlabel(r'$\\mu$ (feet)')\nax[0].set_ylabel('Probability Density')\n\n# Prior for sigma\nsigma_values = np.linspace(sigma_prior_min, sigma_prior_max, 1000)\nsigma_prior_dist = np.ones_like(sigma_values) / (sigma_prior_max - sigma_prior_min)\nax[1].plot(sigma_values, sigma_prior_dist, color='green')\nax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\nax[1].set_xlabel(r'$\\sigma$ (feet)')\nax[1].set_ylabel('Probability Density')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 3: Sample from Priors\n# =========================\n\n# Number of samples\nnum_prior_samples = 10000\n\n# Sample mu and sigma from their priors\nmu_samples = np.random.uniform(mu_prior_min, mu_prior_max, num_prior_samples)\nsigma_samples = np.random.uniform(sigma_prior_min, sigma_prior_max, num_prior_samples)\n\n# To avoid sigma=0, set a minimum sigma\nsigma_samples[sigma_samples == 0] = 1e-6\n\n# Generate heights based on sampled mu and sigma\nheights_prior = np.random.normal(loc=mu_samples, scale=sigma_samples)\n\n# Plot the prior-generated heights\nplt.figure(figsize=(10, 6))\nsns.histplot(heights_prior, bins=50, kde=True, color='orange')\nplt.title('Histogram of Heights Generated from Priors (10,000 Persons)')\nplt.xlabel('Height (feet)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 4: Naive Monte Carlo (Corrected)\n# =========================\n\nimport pandas as pd\n\n# Precompute constants\nn = len(heights)\ndata = heights\n\n# Vectorized computation of log-likelihoods\n# There's a hell of a lot happening in this one line. For each of the 10,000 random samples\n# from the prior, it is using each as the loc and scale of a norm dist to find the \n# likelihood of getting the height data. Then summing the 100 to get a 10,000 lenth array\nlog_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n\n# To prevent numerical underflow, we'll work with log-likelihoods\n# Normalize log-likelihoods by subtracting the max\nmax_log_likelihood = np.max(log_likelihoods)\nnormalized_log_likelihood = log_likelihoods - max_log_likelihood\nlikelihoods = np.exp(normalized_log_likelihood)\n\n# Compute posterior probabilities (unnormalized)\n# Due to flat priors posterior is just equivalent to the likelihoods\nposterior = likelihoods\n\n# Normalize the posterior (so it sums to 1)\nposterior /= np.sum(posterior)\n\n# Verify that 'posterior' is one-dimensional\nprint(f\"Shape of posterior: {posterior.shape}\")\n\n# Create a DataFrame for easier handling\ndf = pd.DataFrame({\n    'mu': mu_samples,\n    'sigma': sigma_samples,\n    'posterior': posterior\n})\n\n# Plot the posterior distributions for mu and sigma using Seaborn\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot posterior for mu\nsns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\nax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\nax[0].set_title(r'Posterior Distribution for $\\mu$')\nax[0].set_xlabel(r'$\\mu$ (feet)')\nax[0].set_ylabel('Posterior Probability')\nax[0].legend()\n\n# Plot posterior for sigma\nsns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\nax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\nax[1].set_title(r'Posterior Distribution for $\\sigma$')\nax[1].set_xlabel(r'$\\sigma$ (feet)')\nax[1].set_ylabel('Posterior Probability')\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nShape of posterior: (10000,)\n\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 5: Visualize Parameter Likelihood (Log-Scaled Color)\n# =========================\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a small epsilon to avoid log(0)\nepsilon = 1e-10\n\n# Compute the logarithm of posterior probabilities\nlog_posterior = np.log(posterior + epsilon)\n\n# Create the scatter plot\nplt.figure(figsize=(10, 8))\n\n# Scatter plot using log_posterior for color scaling\nscatter = plt.scatter(\n    mu_samples,\n    sigma_samples,\n    c=log_posterior,          # Use log-transformed posterior\n    cmap='viridis',\n    alpha=0.5,\n    s=10\n)\n\n# Add a colorbar with appropriate labeling\ncbar = plt.colorbar(scatter)\ncbar.set_label('Log Posterior Probability')\n\n# Add reference lines for true parameter values using raw strings\nplt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\nplt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n\n# Set plot titles and labels using raw strings\nplt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\nplt.xlabel(r'$\\mu$ (feet)')\nplt.ylabel(r'$\\sigma$ (feet)')\n\n# Add legend\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nShiny Live!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Inference of Heights\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.h4(\"True Parameters\"),\n            ui.input_numeric(\n                \"mu_true\", \n                \"True Mean Height (μ_true) [feet]\", \n                value=5.5, \n                min=4.0, \n                max=7.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_true\", \n                \"True Std Dev (σ_true) [feet]\", \n                value=0.3, \n                min=0.1, \n                max=1.0, \n                step=0.05\n            ),\n            ui.hr(),\n            ui.h4(\"Dataset Generation\"),\n            ui.input_numeric(\n                \"dataset_size\", \n                \"Number of Persons\", \n                value=100, \n                min=10, \n                max=1000, \n                step=10\n            ),\n            ui.hr(),\n            ui.h4(\"Prior Ranges\"),\n            ui.input_numeric(\n                \"mu_prior_min\", \n                \"μ Prior Minimum [feet]\", \n                value=4.0, \n                min=0.0, \n                max=6.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"mu_prior_max\", \n                \"μ Prior Maximum [feet]\", \n                value=7.0, \n                min=5.0, \n                max=10.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_prior_min\", \n                \"σ Prior Minimum [feet]\", \n                value=0.0, \n                min=0.0, \n                max=1.0, \n                step=0.05\n            ),\n            ui.input_numeric(\n                \"sigma_prior_max\", \n                \"σ Prior Maximum [feet]\", \n                value=0.9, \n                min=0.5, \n                max=3.0, \n                step=0.1\n            ),\n            ui.hr(),\n            ui.h4(\"Sampling\"),\n            ui.input_numeric(\n                \"num_prior_samples\", \n                \"Number of Prior Samples\", \n                value=10000, \n                min=1000, \n                max=100000, \n                step=1000\n            ),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Generated Data\", ui.output_plot(\"generated_heights_plot\")),\n            ui.nav_panel(\"Prior Distributions\", ui.output_plot(\"prior_distributions_plot\")),\n            ui.nav_panel(\"Prior-Generated Heights\", ui.output_plot(\"prior_generated_heights_plot\")),\n            ui.nav_panel(\"Posterior Distributions\", ui.output_plot(\"posterior_distributions_plot\")),\n            ui.nav_panel(\"Parameter Scatter Plot\", ui.output_plot(\"parameter_scatter_plot\")),\n        )\n\n    )\n)\n\ndef server(input, output, session):\n    # Reactive expression to generate dataset\n    @reactive.Calc\n    def heights():\n        np.random.seed(42)  # For reproducibility\n        mu = input.mu_true()\n        sigma = input.sigma_true()\n        size = input.dataset_size()\n        return np.random.normal(loc=mu, scale=sigma, size=size)\n    \n    # Reactive expressions for prior ranges\n    @reactive.Calc\n    def mu_prior_range():\n        return (input.mu_prior_min(), input.mu_prior_max())\n    \n    @reactive.Calc\n    def sigma_prior_range():\n        return (input.sigma_prior_min(), input.sigma_prior_max())\n    \n    # Reactive expression to sample from priors\n    @reactive.Calc\n    def prior_samples():\n        num_samples = input.num_prior_samples()\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        mu_samples = np.random.uniform(mu_min, mu_max, num_samples)\n        sigma_samples = np.random.uniform(sigma_min, sigma_max, num_samples)\n        # Avoid sigma=0\n        sigma_samples[sigma_samples == 0] = 1e-6\n        return mu_samples, sigma_samples\n    \n    # Reactive expression to generate heights from priors\n    @reactive.Calc\n    def heights_prior_samples():\n        mu_samples, sigma_samples = prior_samples()\n        return np.random.normal(loc=mu_samples, scale=sigma_samples)\n    \n    # Reactive expression to compute posterior\n    @reactive.Calc\n    def posterior():\n        data = heights()\n        mu_samples, sigma_samples = prior_samples()\n        n = len(data)\n        \n        # Compute log-likelihoods\n        log_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n        \n        # Normalize log-likelihoods to prevent underflow\n        max_log_likelihood = np.max(log_likelihoods)\n        normalized_log_likelihood = log_likelihoods - max_log_likelihood\n        likelihoods = np.exp(normalized_log_likelihood)\n        \n        # Posterior probabilities (unnormalized)\n        posterior_probs = likelihoods\n        \n        # Normalize posterior\n        posterior_probs /= np.sum(posterior_probs)\n        \n        return posterior_probs\n    \n    # Reactive expression to create DataFrame for posterior\n    @reactive.Calc\n    def posterior_df():\n        mu_samples, sigma_samples = prior_samples()\n        posterior_probs = posterior()\n        return pd.DataFrame({\n            'mu': mu_samples,\n            'sigma': sigma_samples,\n            'posterior': posterior_probs\n        })\n    \n    # Plot 1: Generated Heights Histogram\n    @output\n    @render.plot\n    def generated_heights_plot():\n        data = heights()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(data, bins=15, kde=True, color='skyblue')\n        plt.title(f'Histogram of Generated Heights ({input.dataset_size()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 2: Prior Distributions for mu and sigma\n    @output\n    @render.plot\n    def prior_distributions_plot():\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Prior for mu\n        mu_values = np.linspace(mu_min, mu_max, 1000)\n        mu_prior = np.ones_like(mu_values) / (mu_max - mu_min)\n        ax[0].plot(mu_values, mu_prior, color='blue')\n        ax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Probability Density')\n        \n        # Prior for sigma\n        sigma_values = np.linspace(sigma_min, sigma_max, 1000)\n        sigma_prior_dist = np.ones_like(sigma_values) / (sigma_max - sigma_min)\n        ax[1].plot(sigma_values, sigma_prior_dist, color='green')\n        ax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Probability Density')\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 3: Heights Generated from Priors\n    @output\n    @render.plot\n    def prior_generated_heights_plot():\n        heights_prior = heights_prior_samples()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(heights_prior, bins=50, kde=True, color='orange')\n        plt.title(f'Histogram of Heights Generated from Priors ({input.num_prior_samples()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 4: Posterior Distributions for mu and sigma\n    @output\n    @render.plot\n    def posterior_distributions_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Posterior for mu\n        sns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\n        ax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        ax[0].set_title(r'Posterior Distribution for $\\mu$')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Posterior Probability')\n        ax[0].legend()\n        \n        # Posterior for sigma\n        sns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\n        ax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\n        ax[1].set_title(r'Posterior Distribution for $\\sigma$')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Posterior Probability')\n        ax[1].legend()\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 5: Scatter Plot of mu vs sigma with Log Posterior Probability\n    @output\n    @render.plot\n    def parameter_scatter_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        epsilon = 1e-10  # To avoid log(0)\n        log_posterior = np.log(df['posterior'] + epsilon)\n        \n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            df['mu'],\n            df['sigma'],\n            c=log_posterior,\n            cmap='viridis',\n            alpha=0.5,\n            s=10\n        )\n        cbar = plt.colorbar(scatter)\n        cbar.set_label('Log Posterior Probability')\n        \n        plt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        plt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n        \n        plt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\n        plt.xlabel(r'$\\mu$ (feet)')\n        plt.ylabel(r'$\\sigma$ (feet)')\n        plt.legend()\n        plt.tight_layout()\n        return plt.gcf()\n\napp = App(app_ui, server)\nv2\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Inference of Heights\"),\n    ui.row(\n        ui.column(3,\n            ui.h4(\"True Parameters\"),\n            ui.input_numeric(\n                \"mu_true\", \n                \"True Mean Height [feet]\", \n                value=5.5, \n                min=4.0, \n                max=7.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_true\", \n                \"True Std Dev (σ_true) [feet]\", \n                value=0.3, \n                min=0.1, \n                max=1.0, \n                step=0.05\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Dataset Generation\"),\n            ui.input_numeric(\n                \"dataset_size\", \n                \"Number of Persons\", \n                value=100, \n                min=10, \n                max=1000, \n                step=10\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Prior Ranges\"),\n            ui.input_numeric(\n                \"mu_prior_min\", \n                \"μ Prior Minimum [feet]\", \n                value=4.0, \n                min=0.0, \n                max=6.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"mu_prior_max\", \n                \"μ Prior Maximum [feet]\", \n                value=7.0, \n                min=5.0, \n                max=10.0, \n                step=0.1\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Prior Ranges & Sampling\"),\n            ui.input_numeric(\n                \"sigma_prior_min\", \n                \"σ Prior Minimum [feet]\", \n                value=0.0, \n                min=0.0, \n                max=1.0, \n                step=0.05\n            ),\n            ui.input_numeric(\n                \"sigma_prior_max\", \n                \"σ Prior Maximum [feet]\", \n                value=0.9, \n                min=0.5, \n                max=3.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"num_prior_samples\", \n                \"Number of Prior Samples\", \n                value=10000, \n                min=1000, \n                max=100000, \n                step=1000\n            ),\n        )\n    ),\n    ui.navset_tab(\n        ui.nav_panel(\"True Height Data\", ui.output_plot(\"generated_heights_plot\")),\n        ui.nav_panel(\"Prior\", ui.output_plot(\"prior_distributions_plot\")),\n        ui.nav_panel(\"Prior-Generated Heights\", ui.output_plot(\"prior_generated_heights_plot\")),\n        ui.nav_panel(\"Posterior\", ui.output_plot(\"posterior_distributions_plot\")),\n        ui.nav_panel(\"Parameter Plot\", ui.output_plot(\"parameter_scatter_plot\")),\n    )\n)\n\n\ndef server(input, output, session):\n    # Reactive expression to generate dataset\n    @reactive.Calc\n    def heights():\n        np.random.seed(42)  # For reproducibility\n        mu = input.mu_true()\n        sigma = input.sigma_true()\n        size = input.dataset_size()\n        return np.random.normal(loc=mu, scale=sigma, size=size)\n    \n    # Reactive expressions for prior ranges\n    @reactive.Calc\n    def mu_prior_range():\n        return (input.mu_prior_min(), input.mu_prior_max())\n    \n    @reactive.Calc\n    def sigma_prior_range():\n        return (input.sigma_prior_min(), input.sigma_prior_max())\n    \n    # Reactive expression to sample from priors\n    @reactive.Calc\n    def prior_samples():\n        num_samples = input.num_prior_samples()\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        mu_samples = np.random.uniform(mu_min, mu_max, num_samples)\n        sigma_samples = np.random.uniform(sigma_min, sigma_max, num_samples)\n        # Avoid sigma=0\n        sigma_samples[sigma_samples == 0] = 1e-6\n        return mu_samples, sigma_samples\n    \n    # Reactive expression to generate heights from priors\n    @reactive.Calc\n    def heights_prior_samples():\n        mu_samples, sigma_samples = prior_samples()\n        return np.random.normal(loc=mu_samples, scale=sigma_samples)\n    \n    # Reactive expression to compute posterior\n    @reactive.Calc\n    def posterior():\n        data = heights()\n        mu_samples, sigma_samples = prior_samples()\n        n = len(data)\n        \n        # Compute log-likelihoods\n        log_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n        \n        # Normalize log-likelihoods to prevent underflow\n        max_log_likelihood = np.max(log_likelihoods)\n        normalized_log_likelihood = log_likelihoods - max_log_likelihood\n        likelihoods = np.exp(normalized_log_likelihood)\n        \n        # Posterior probabilities (unnormalized)\n        posterior_probs = likelihoods\n        \n        # Normalize posterior\n        posterior_probs /= np.sum(posterior_probs)\n        \n        return posterior_probs\n    \n    # Reactive expression to create DataFrame for posterior\n    @reactive.Calc\n    def posterior_df():\n        mu_samples, sigma_samples = prior_samples()\n        posterior_probs = posterior()\n        return pd.DataFrame({\n            'mu': mu_samples,\n            'sigma': sigma_samples,\n            'posterior': posterior_probs\n        })\n    \n    # Plot 1: Generated Heights Histogram\n    @output\n    @render.plot\n    def generated_heights_plot():\n        data = heights()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(data, bins=15, kde=True, color='skyblue')\n        plt.title(f'Histogram of Generated Heights ({input.dataset_size()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 2: Prior Distributions for mu and sigma\n    @output\n    @render.plot\n    def prior_distributions_plot():\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Prior for mu\n        mu_values = np.linspace(mu_min, mu_max, 1000)\n        mu_prior = np.ones_like(mu_values) / (mu_max - mu_min)\n        ax[0].plot(mu_values, mu_prior, color='blue')\n        ax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Probability Density')\n        \n        # Prior for sigma\n        sigma_values = np.linspace(sigma_min, sigma_max, 1000)\n        sigma_prior_dist = np.ones_like(sigma_values) / (sigma_max - sigma_min)\n        ax[1].plot(sigma_values, sigma_prior_dist, color='green')\n        ax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Probability Density')\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 3: Heights Generated from Priors\n    @output\n    @render.plot\n    def prior_generated_heights_plot():\n        heights_prior = heights_prior_samples()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(heights_prior, bins=50, kde=True, color='orange')\n        plt.title(f'Histogram of Heights Generated from Priors ({input.num_prior_samples()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 4: Posterior Distributions for mu and sigma\n    @output\n    @render.plot\n    def posterior_distributions_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Posterior for mu\n        sns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\n        ax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        ax[0].set_title(r'Posterior Distribution for $\\mu$')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Posterior Probability')\n        ax[0].legend()\n        \n        # Posterior for sigma\n        sns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\n        ax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\n        ax[1].set_title(r'Posterior Distribution for $\\sigma$')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Posterior Probability')\n        ax[1].legend()\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 5: Scatter Plot of mu vs sigma with Log Posterior Probability\n    @output\n    @render.plot\n    def parameter_scatter_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        epsilon = 1e-10  # To avoid log(0)\n        log_posterior = np.log(df['posterior'] + epsilon)\n        \n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            df['mu'],\n            df['sigma'],\n            c=log_posterior,\n            cmap='viridis',\n            alpha=0.5,\n            s=10\n        )\n        cbar = plt.colorbar(scatter)\n        cbar.set_label('Log Posterior Probability')\n        \n        plt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        plt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n        \n        plt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\n        plt.xlabel(r'$\\mu$ (feet)')\n        plt.ylabel(r'$\\sigma$ (feet)')\n        plt.legend()\n        plt.tight_layout()\n        return plt.gcf()\n\napp = App(app_ui, server)"
  },
  {
    "objectID": "3.X-Naive_Monte_Carlo.html#less-naive-monte-carlo",
    "href": "3.X-Naive_Monte_Carlo.html#less-naive-monte-carlo",
    "title": "Naive Monte Carlo",
    "section": "Less Naive Monte Carlo",
    "text": "Less Naive Monte Carlo\nIf you paid attention to the Posterior Probability of \\(\\mu\\) and \\(\\sigma\\) plot, you probabiliy noticed that a lot of the points/space on the chart was consumed by areas of low probability. This was true even though we had few parameters (low dimensions) and good priors. The trick of the more advanced Monte Carlo techniques is how to find the areas of high likelihood and sample them efficiently. These include Hamiltonian Monte Carlo (HMC) and the No-U-Turn Sampler (NUTS). We’ll use em, and thankfully the details are easily researched if you want to know more…"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A note to readers and contributors alike:\nI think the primer is best viewed as a set of themes within a narrative thread. First, the themes:\n\nThat statistics is simply a loop of P(D|M) and P(M|D), where D is Data and M is Model.\nThat computation/simulation is ultimately more intuitive and useful than excessive analytical solutions.\nClosely related to the last bullet, that generating data from models makes it easy to:\n\nUnderstand what the model really does/knows, not what you think it does/knows.\nApproximate solutions that are very difficult to understand or solve analytically.\n\n\nNext the narrative thread:\n\nThat the world is full of data generating processes each creating unique probability distributions.\nFor any data generating process, if we can generate data from a model that approximates it, we can approximate P(D|M).\nThat statisticians have named some particularly useful probability distributuions, in large part because they have [mostly] analytic solutions to P(D|M).\nStatistical models, like linear regression, can’t be understood until you see the centrality of the error term, how it assumes a probability distribution, and how it shapes the fit of the rest of the model.\nStandard statistical models have extremely limiting assumptions, like constant variance in linear regression, and modifying the model to be more realistic quickly becomes analytically intractable.\nThat Machine Learning is not always statistical because it uses loss functions that may not be probabilistic, however some machine learning methods are still naturally probabilistic, like bayesian neural nets.\nWith all the discussion of P(D|M), it is now easier to understand P(M|D).\nThat we either fit models with or without priors.\n\nTo contributors, one of the most challenging aspects of creating the primer is balancing intuitive wordings with ‘statistical correctness’. If you see area for improvement that work with the themes and narrative thread - please contribute!!\nPersonal note:\nAlthough I already had a rough outline and some content, once I had a decent first draft of the primer’s introduction I asked ChatGPT to sketch out a table of contents. Even with the the introduction giving a very clear direction, ChatGPT’s table of contents mirrored every other statistical book I have read, and was completely different from what I was envisioning. And that’s when I began to think I may be on to something meaningful.\nI hope this helps you avoid a maze of confusion that I wandered through as I tried to grasp statistics. First as a graduate engineering student, and later as a professional engineer working in a risk management program of an energy utility. An even grander vision would be that someday, maybe I’ll be able to feed in my intro to ChatGPT, and it will spit out something much closer to my table of contents.\n-Kevin"
  },
  {
    "objectID": "3.2-with_priors.html",
    "href": "3.2-with_priors.html",
    "title": "With Priors",
    "section": "",
    "text": "We finally come to the last chapter, which is focused on P(M|D)…\nStatistics used to be a bit of a flame way between those who used priors and Bayesian statistics, and the ‘standard’ which was frequentism… I think it’s safe to say however, if you’re working on a subject in which you have a significant amount of expert knowledge with relatively complex models, you should use a Bayesian approach for two reasons:\n\nIf you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.\nIf you do have an overwhelming amount of data, the priors you set will have little to no difference in the final statistical model - although they may still help avoid nonsensical parameter values.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#preview",
    "href": "3.2-with_priors.html#preview",
    "title": "With Priors",
    "section": "",
    "text": "We finally come to the last chapter, which is focused on P(M|D)…\nStatistics used to be a bit of a flame way between those who used priors and Bayesian statistics, and the ‘standard’ which was frequentism… I think it’s safe to say however, if you’re working on a subject in which you have a significant amount of expert knowledge with relatively complex models, you should use a Bayesian approach for two reasons:\n\nIf you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.\nIf you do have an overwhelming amount of data, the priors you set will have little to no difference in the final statistical model - although they may still help avoid nonsensical parameter values.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#text",
    "href": "3.2-with_priors.html#text",
    "title": "With Priors",
    "section": "Text",
    "text": "Text\nText.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#likelihood",
    "href": "3.2-with_priors.html#likelihood",
    "title": "With Priors",
    "section": "Likelihood",
    "text": "Likelihood\nWe have finally worked our way up to what I consider to be one of the most important topics… Likelihood…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#probability-distribution-or-uncertainty",
    "href": "3.2-with_priors.html#probability-distribution-or-uncertainty",
    "title": "With Priors",
    "section": "Probability Distribution or Uncertainty?",
    "text": "Probability Distribution or Uncertainty?\nWhile I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.\nThe second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.\nIn the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).\n\n\n\n\n\nflowchart LR\n    subgraph DGD[Data Generating Distribution]\n        subgraph UD[Uncertainty Distributions]\n            GammaMean[Gamma Distribution]\n            GammaVar[Gamma Distribution]\n        end\n        Mean[Mean]\n        Variance[Variance]\n        GammaMean --&gt; Mean\n        GammaVar --&gt; Variance\n        Mean --&gt; Normal\n        Variance --&gt; Normal\n        Normal[Normal Distribution]\n    end\n    Normal --&gt; Height[Height Observations]",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html",
    "href": "2.1-discrete_probability_distributions.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "In the first half of the primer we focus on \\(P(D|M)\\), the probability of the data given a model of a data generating process1. This first chapter considers only processes with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce discrete probability distributions as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. For any model we can use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. Where an analytical solution of \\(P(D|M)\\) is available, we will also compare exact probabilities to those estimated from relative frequency.\nWe then shift to the probability of multiple events based on the laws of probability for independent events. We show how we can use computation to calculate the relative probability of a specific series of events by comparing it to a multitude of randomly generated series of events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls. This is our more intuitive approach to hypothesis testing.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#preview",
    "href": "2.1-discrete_probability_distributions.html#preview",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "In the first half of the primer we focus on \\(P(D|M)\\), the probability of the data given a model of a data generating process1. This first chapter considers only processes with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce discrete probability distributions as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. For any model we can use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. Where an analytical solution of \\(P(D|M)\\) is available, we will also compare exact probabilities to those estimated from relative frequency.\nWe then shift to the probability of multiple events based on the laws of probability for independent events. We show how we can use computation to calculate the relative probability of a specific series of events by comparing it to a multitude of randomly generated series of events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls. This is our more intuitive approach to hypothesis testing.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#models-of-discrete-data-generating-processes",
    "href": "2.1-discrete_probability_distributions.html#models-of-discrete-data-generating-processes",
    "title": "Discrete Probability Distributions",
    "section": "Models of Discrete Data Generating Processes",
    "text": "Models of Discrete Data Generating Processes\n\nA Dice Total Model\nWe’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process. It is also convenient that the mathematical model we’ll use is a good approximation of the real data generating process, so long as you’re OK with ignoring all the physical bouncing of the dice and are content with just the result after a roll.\nA dice model is built into the app displayed beneath this paragraph. It will simulate rolling the number of dice you specify, as if you threw them out of a cup all at once, and total the value on those dice from the cup, which is considered one roll. Additionally, it will repeat rolling that cup of dice the number of times you specify, and summarize the results on a histogram. Play around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling App\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax1 = plt.subplots()\n\n        # Calculate frequencies and relative frequencies\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        relative_freq = counts / len(current_sums)\n\n        # Create the frequency bars with darker blue\n        bars = ax1.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")  \n        ax1.set_xlabel(\"Dice Total\")\n        ax1.set_ylabel(\"Frequency\", color=\"steelblue\")\n        ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n\n        # Create secondary y-axis for relative frequency\n        ax2 = ax1.twinx()\n        markers = ax2.plot([str(s) for s in unique_sums], relative_freq, \n                          color=\"r\", marker='_', linestyle='None', label=\"Relative Frequency\")\n        ax2.set_ylabel(\"Relative Frequency\", color=\"red\")\n        ax2.tick_params(axis='y', labelcolor=\"red\")\n        \n        # Set y-axis limits to start at 0 for relative frequency\n        ax2.set_ylim(bottom=0)\n\n        # Set title\n        plt.title(\"Frequency and Relative Frequency of Dice Totals\", fontsize=14)\n        \n        # Add legend\n        lines = [bars.patches[0], markers[0]]\n        labels = [\"Frequency\", \"Relative Frequency\"]\n        ax1.legend(lines, labels, loc='upper left')\n\n        # Adjust layout to prevent label cutoff\n        plt.tight_layout()\n        \n        # Rotate x-axis labels after tight_layout\n        ax1.tick_params(axis='x', rotation=90)\n\n        return fig\n\napp = App(app_ui, server)\nHopefully you’ve noted how a larger number of rolls seems to give us smoother and more consistent results. We will revisit this point more precisely in a later section.\n\n\nDiscrete Probability Distributions as Models of Data Generating Processes\nIn the last section, we used dice rolling as our data generating process, however there are other discrete processes we may be interested in, such as testing 1,000 products that have a 0.999 probability of success each. As you can imagine, when you change the data generating process, the relative frequency (distribution) of the outcomes change. There are many important discrete processes that can be described with [mostly] analytical mathematical models involving a small set of parameters. These have been given names and we will refer to them as parametric discrete probability distributions. We include parametric to contrast with non-parametric distributions that cannot be described with a small set of parameters, and include discrete to constrast with continuous distributions that can take any real ℝ number value.\n\n\n\n\n\n\nNote\n\n\n\nIf you are familiar with discrete probability distributions, you may have expected them to be introduced as a way to find the exact probability of an event, instead of using them to generate data. We will eventually use this feature, but our goal is to more generally introduce models of data generating processes and their associated probability distributions. Only a small number have properties that allow for nice analytical solutions to the probabilities of their data, and we’d prefer not to constrain ourselves to thinking in only an analytical (as opposed to computational) framework.\n\n\nInitially we want to think about discrete probability distributions in the same way we thought about our dice model, that it is a model that will generate random events from a data generating process of interest. Below are a couple examples to illustrate the point.\n\nBinomial Distribution\nThe binomial distribution is a model that represents the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (commonly referred to as “success” and “failure”). The probability of success can range between 0 and 1. In the app below you can recreate something like the product testing scenario recently mentioned. Feel free to play around with the settings/parameters to get a feel for how the probability distribution behaves.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Binomial Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"numTrials\", \n                \"Number of Trials (n)\", \n                min=1, \n                max=10000, \n                value=100, \n                step=1\n            ),\n            ui.input_slider(\n                \"probSuccess\", \n                \"Probability of Success (p)\", \n                min=0.001, \n                max=0.999, \n                value=0.5, \n                step=0.001\n            ),\n        ),\n        ui.output_plot(\"binomPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    FIXED_NUM_SIMULATIONS = 10000\n\n    @reactive.Calc\n    def binomial_samples():\n        n = input.numTrials()\n        p = input.probSuccess()\n        size = FIXED_NUM_SIMULATIONS\n        return np.random.binomial(n, p, size)\n    \n    def determine_bin_width(data, max_bins=30):\n        data_min = data.min()\n        data_max = data.max()\n        data_range = data_max - data_min + 1\n        for bin_width in range(1, data_range + 1):\n            num_bins = math.ceil(data_range / bin_width)\n            if num_bins &lt;= max_bins:\n                return bin_width\n        return 1\n    \n    @output\n    @render.plot\n    def binomPlot():\n        samples = binomial_samples()\n        bin_width = determine_bin_width(samples, max_bins=30)\n        \n        data_min = samples.min()\n        data_max = samples.max()\n        bins = np.arange(data_min, data_max + bin_width, bin_width)\n        \n        counts, bin_edges = np.histogram(samples, bins=bins)\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        # Calculate relative frequencies\n        relative_freq = counts / len(samples)\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Plot absolute frequencies\n        bars = ax1.bar(bin_centers, counts, width=bin_width*0.9, \n                      color=\"steelblue\", alpha=0.6, edgecolor=\"black\", align='center')\n        \n        ax1.set_xlabel(\"Number of Successes\")\n        ax1.set_ylabel(\"Frequency\", color=\"steelblue\")\n        ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n        \n        # Create secondary y-axis for relative frequency\n        ax2 = ax1.twinx()\n        markers = ax2.plot(bin_centers, relative_freq, \n                          color=\"red\", marker='_', linestyle='None', \n                          markersize=10, markeredgewidth=2, label=\"Relative Frequency\")\n        ax2.set_ylabel(\"Relative Frequency\", color=\"red\")\n        ax2.tick_params(axis='y', labelcolor=\"red\")\n        ax2.set_ylim(bottom=0)\n        \n        # Set title\n        plt.title(\"Frequency and Relative Frequency of Successes\", fontsize=14)\n        \n        # Add legend\n        lines = [bars.patches[0], markers[0]]\n        labels = [\"Frequency\", \"Relative Frequency\"]\n        ax1.legend(lines, labels, loc='upper left')\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([int(x) for x in bin_centers], rotation=90)\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\n\n\nPoisson Distribution\nThe Poisson distribution is a model that describes the number of events occurring in a fixed interval of time or space, assuming that the events occur independently and at a constant average rate. For example, it might represent the number of phone calls received by a call center in an hour or the number of cars passing through a toll booth in a minute. The Poisson Distribution is similar to the Binomial distribution, except there are not a fixed number of trials, so there is not an upper limit to the number of events returned by a sample. However, values much larger than the average rate become incredibly unlikely.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\nimport math\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Poisson Distribution Simulation\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"rate_param\", \n                \"Rate Parameter (λ)\", \n                min=0.01, \n                max=10.0, \n                value=1.0, \n                step=0.01\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Times to Repeat Sample\",\n                min=100,\n                max=10000,\n                value=10000,\n                step=100\n            )\n        ),\n        ui.output_plot(\"poissonPlot\", height=\"400px\"),\n    )\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def num_simulations():\n        return input.num_trials()\n\n    @reactive.Calc\n    def poisson_samples():\n        lam = input.rate_param()\n        n_trials = num_simulations()\n        return np.random.poisson(lam, n_trials)\n    \n    def determine_bin_width(data, max_bins=30):\n        data_min = data.min()\n        data_max = data.max()\n        data_range = data_max - data_min + 1\n        for bin_width in range(1, data_range + 1):\n            num_bins = math.ceil(data_range / bin_width)\n            if num_bins &lt;= max_bins:\n                return bin_width\n        return 1\n\n    @output\n    @render.plot\n    def poissonPlot():\n        samples = poisson_samples()\n        lam = input.rate_param()\n        \n        bin_width = determine_bin_width(samples, max_bins=30)\n        \n        data_min = samples.min()\n        data_max = samples.max()\n        bins = np.arange(data_min, data_max + bin_width, bin_width)\n        \n        counts, bin_edges = np.histogram(samples, bins=bins)\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        # Calculate relative frequencies\n        relative_freq = counts / len(samples)\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Plot absolute frequencies\n        bars = ax1.bar(bin_centers, counts, width=bin_width*0.9, \n                      color=\"steelblue\", alpha=0.6, edgecolor=\"black\", align='center')\n        \n        ax1.set_xlabel(\"Number of Events\")\n        ax1.set_ylabel(\"Frequency\", color=\"steelblue\")\n        ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n        \n        # Create secondary y-axis for relative frequency\n        ax2 = ax1.twinx()\n        markers = ax2.plot(bin_centers, relative_freq, \n                          color=\"red\", marker='_', linestyle='None', \n                          markersize=10, markeredgewidth=2, label=\"Relative Frequency\")\n        ax2.set_ylabel(\"Relative Frequency\", color=\"red\")\n        ax2.tick_params(axis='y', labelcolor=\"red\")\n        ax2.set_ylim(bottom=0)\n        \n        # Set title\n        plt.title(f\"Frequency and Relative Frequency of Poisson Events\", \n                 fontsize=14)\n        \n        # Add legend\n        lines = [bars.patches[0], markers[0]]\n        labels = [\"Frequency\", \"Relative Frequency\"]\n        ax1.legend(lines, labels, loc='upper right')\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([int(x) for x in bin_centers], rotation=90)\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\n\n\nSummary\nWe keep this section brief as there are plenty of easily accessible references for discrete probability distributions. Hopefully the point was made though - that each discrete probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcomes of a process.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#probability-of-data",
    "href": "2.1-discrete_probability_distributions.html#probability-of-data",
    "title": "Discrete Probability Distributions",
    "section": "Probability of Data",
    "text": "Probability of Data\nHaving given some background on models, we can now focus on the probability of the data given the model, \\(P(D|M)\\).\n\nDice Totals Probability of Data\n\nRelative Frequency\nWe want to find the probability of a particular dice total (the data) given a dice model. To estimate the probability \\(P(E)\\) of an event \\(E\\), we can use the relative frequency approach. This involves counting the number of occurrences of the event E and dividing it by the total number of trials. For example, if we observed a total of twelve occur in 40 out of 5,000 dice rolls, the probability estimate is:\n\\[\nP(E) \\approx \\frac{\\text{Number of times event } E \\text{ occurs}}{\\text{Total number of trials}} = \\frac{40}{5000} = 0.008\n\\]\n\n\nLaw of Large Numbers\nThe accuracy of this estimate depends on the total number of trials as governed by the Law of Large Numbers. The standard error, which gives a measure of uncertainty in the estimate of a mean value, is proportional to one over the square root of the number of samples:\n\\[\nP_{\\text{error}} \\propto \\frac{1}{\\sqrt{N_{\\text{total}}}}\n\\]\nWhich indicates there are diminishing returns to just making the sample size larger. Now I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techniques that work well for real problems. In general, if you are concerned with the quality of an estimate with this approach, just rerun the model and see if the outcome changes meaningfully - if it does, increase the number of times we run the model until the output is stable enough for your application. If that’s still not enough, dig into exact/analytic methods.\n\n\nRevised Dice App\nHere’s another version of the Dice Total App that you saw earlier - except it now has additional functionality to calculate the approximate and exact probability of a certain dice total based on your inputs. It also shows the relative frequency of the dice sums, i.e. a true probability distribution where the sum of the probabilities equals one.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 560\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Utility function to compute exact distribution of sums for n dice ---\ndef dice_sum_distribution(n_dice):\n    \"\"\"\n    Return a list 'dist' where dist[s] = probability of sum s for n_dice dice.\n    Indices go from 0 up to 6*n_dice. Only sums in range [n_dice..6*n_dice]\n    have nonzero probabilities.\n    \"\"\"\n    # ways[s] = number of ways to get sum s\n    ways = [0] * (6*n_dice + 1)\n    ways[0] = 1  # base case\n\n    for _ in range(n_dice):\n        new_ways = [0] * (6*n_dice + 1)\n        for sum_val, count in enumerate(ways):\n            if count &gt; 0:\n                for face in range(1, 7):\n                    new_ways[sum_val + face] += count\n        ways = new_ways\n\n    total_outcomes = 6 ** n_dice\n    dist = [count / total_outcomes for count in ways]\n    return dist\n\n# -------------------------- UI Definition ---------------------------\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling App with Probability Mass Function\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n            ui.input_select(\n                \"selectedTotal\", \n                \"Select Dice Total\", \n                choices=[\"\"],   # initially empty, will be updated dynamically\n                multiple=False,\n            ),\n            ui.output_text(\"approxProbability\"),\n            ui.output_text(\"exactProbability\")\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    )\n)\n\n# -------------------------- Server Definition -------------------------\ndef server(input, output, session):\n    # Reactive: Generate random sums based on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Reactive: Exact distribution of sums for the current number of dice\n    @reactive.Calc\n    def exact_distribution():\n        return dice_sum_distribution(input.numDice())\n\n    # Dynamically update the choices in the 'selectedTotal' select input\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    # Plot the relative frequency of dice totals\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        \n        fig, ax = plt.subplots()\n        \n        # Get theoretical distribution first to set x-axis limits\n        dist = exact_distribution()\n        theoretical_sums = range(input.numDice(), 6*input.numDice()+1)\n        theoretical_probs = [dist[i] for i in theoretical_sums]\n        \n        # Create dictionary to store empirical probabilities for all possible sums\n        empirical_dict = {i: 0 for i in theoretical_sums}\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        for sum_val, count in zip(unique_sums, counts):\n            if sum_val in empirical_dict:\n                empirical_dict[sum_val] = count / len(current_sums)\n        \n        # Plot empirical distribution\n        ax.bar([str(s) for s in theoretical_sums], \n               [empirical_dict[s] for s in theoretical_sums],\n               color=\"steelblue\", label=\"Empirical\")\n        \n        # Plot theoretical distribution\n        ax.plot([str(s) for s in theoretical_sums], theoretical_probs, \n                'r', marker='_', linestyle='', label=\"Theoretical\")\n\n        ax.set_title(\"Probability Mass Function of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Probability\")\n        ax.legend()\n        plt.xticks(rotation=90)\n\n        return fig\n\n    # Approximate probability of the selected dice total\n    @output\n    @render.text\n    def approxProbability():\n        if not input.selectedTotal():\n            return \"Select a dice total to see probabilities.\"\n\n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n\n        count = sum(1 for x in current_sums if x == selected_total)\n        if len(current_sums) == 0:\n            prob = 0\n        else:\n            prob = count / len(current_sums)\n\n        return f\"Approx. Probability of {selected_total}: {prob:.4f}\"\n\n    # Exact probability of the selected dice total\n    @output\n    @render.text\n    def exactProbability():\n        if not input.selectedTotal():\n            return \"\"\n\n        selected_total = int(input.selectedTotal())\n        dist = exact_distribution()\n\n        # If the selected total is out of range, probability is 0\n        if selected_total &lt; 0 or selected_total &gt;= len(dist):\n            prob = 0\n        else:\n            prob = dist[selected_total]\n\n        return f\"Exact Probability of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)\nHopefully you can demonstrate to yourself that with enough samples, the approximate probability calculation is awfully close to the exact probability. However, there is an exception. The tails (the slim far ends) are not as accurate. Properly calculating probability in these tail sections happens to be trivial for dice where an exact solution is available, but for a real problem, accurate tail probabilities are incredibly difficult.\n\n\n\nExact Probabilities from a Discrete Probability Distribution\nHere we provide an example of the more ‘typical’ use of a parametric discrete probability distribution, to compute the probability of an event. Let’s use the properties of the binomial probability distribution to determine the probability of having less than 999 products pass a quality test when we have a true 0.999 pass rate and we inspect 1,000 products.\nWe’ll solve this problem by computing the probability of having 999 or 1,000 products pass, and then just inverting the probability. Note that \\(P(X = 1000)\\) is simplified below based on the multiplication rule of independent events to equal 0.999^1,000. Also, if you’re unfamiliar with the n over k in parenthesis, look up ‘n choose k’.\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\n\\[\nP(X = 999) + P(X = 1,000) = \\binom{1000}{999} (0.999)^{999} (0.001)^1 + (0.999)^{1,000}\n\\]\nThe result is 0.736, so the chance of having a real pass rate of 0.999 and having less than 999 pass is 1 - 0.736 = 0.264. It’s fairly plausible to have 0.999 pass rate and have a couple of products fail. However, if you had four or more fail, you should have a very hard time convincing management that it was just ‘bad luck’. The more formal way to discuss this is hypothesis testing.\n\n\n\n\n\n\nNote\n\n\n\nMuch of statistics is dedicated to hypothesis testing. There’s no reason for this primer to repeat such a repeated topic. The problem though, is that in the real world, folks get it wrong. I have a simple hypothesis for this, and that’s because non-expert practitioners do not understand their statistical models. And there’s good reasons they don’t, they are often analytical mathematical ‘magic’ with buried assumptions. We take a different approach.\n\n\n\n\nProbability of Multiple Events\nSo far we have only considered \\(P(D|M)\\) where the data is a single event. However, it is more common to have multiple events (i.e. multiple data points), and we’d like to know the probability of a specific dataset vs other datasets we may have sampled. To calculate probabilities we’ll use the multiplication rule of independent events. In practice it may be very difficult to prove perfectly independent events - however be sure there is not a reason for strong correlation, such as almost all time series.\nTo calculate the probability of \\(n\\) independent events \\(E_1, E_2, \\ldots, E_n\\), we simply multiply them together:\n\\[\nP\\left( \\bigcap_{i=1}^{n} E_i \\right) = \\prod_{i=1}^{n} P(E_i)\n\\]\nWhere the left side of the equation refers to the probability of multiple independent events, and the right side describes multiplying the probability of each event in a continuous chain.\n\nNumerical Stability\nWe should also mention for many small probabilities, multiplication risks numerical instability. However there is one very simple and clever workaround to this, which is to add log probabilities instead:\n\\[\nP(A \\cap B \\cap C \\cap \\dots) = P(A) \\cdot P(B) \\cdot P(C) \\cdot \\dots = 10^{(\\log_{10}(P(A)) + \\log_{10}(P(B)) + \\log_{10}(P(C)) + \\dots))}\n\\]\n\n\nExample\nLet’s work through a couple example calculations before extending it to a scenario that approximates more serious problems we’ll tackle later. What is the probability of rolling three dice out of a cup, where the total is 11, and then rolling the dice again, where the total is 7? We use the app above to get the probabilities of each roll, where 11 = 0.1250 and 7 = 0.0694.\n\\[\nP(Roll 1 \\cap Roll 2) = 0.1250 \\cdot 0.0694 = 0.008675 = 10^{(\\log_{10}(0.1250) + \\log_{10}(0.0694))}\n\\]\nIf we stay in log probabilities, as that is generally more convenient:\n\\[\n\\log_{10}(P(Roll 1 \\cap Roll 2)) = \\log_{10}(0.1250) + \\log_{10}(0.0694) = -0.9031 + -1.1586 = -2.0617\n\\]\n\n\n\nCalculating the Relative Probability of Multiple Events\nWith the background from the last section we can now simulate the probability of multiple events. However, often it is more useful to think about the relative probability of a long series of events rather than the absolute probability, since the absolute probability will be tiny. I think an example will help. Again, to have better intuition about the problem, we’ll use dice, although it can obviously be extended to other discrete probability distributions.\nWe’ll extend the recent example of rolling three dice out of a cup. We’ll let most of our simulations use fair dice, and we’ll show the log probability of each exact series of events as we perform rolls out of the cup. Note that for three dice, the values can range from 3 to 18, and values near the middle, such as 10, will be more common than those at the end, such as 3 or 18. The log probability of more rare events will be more negative. If we have a series of very unlikely events, the values will become negative more quickly.\nWe’re also going to include one series of rolls with a cup of three weighted dice, which will be the dotted blue line. The values and probabilities will be 1=0.10, 2=0.10, 3=0.15, 4=0.15, 5=0.2, 6=0.3. If we expect the outcomes of fair dice, we’ll see that the weighted dice will usually trend towards being a fairly improbable series of rolls. We can refer to the percentile of their log-probability to see how unusual they are.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Utility Functions ---\n\ndef roll_three_dice_fair():\n    \"\"\"Simulate rolling three fair dice and return their sum.\"\"\"\n    return np.sum(np.random.randint(1, 7, size=3))\n\ndef roll_three_dice_weighted():\n    \"\"\"Simulate rolling three weighted dice and return their sum.\"\"\"\n    weights = [0.10, 0.10, 0.15, 0.15, 0.2, 0.30]\n    # Define possible outcomes\n    outcomes = [1, 2, 3, 4, 5, 6]\n    # Roll three weighted dice\n    return np.sum(np.random.choice(outcomes, size=3, p=weights))\n\ndef get_probability_of_sum(dice_sum):\n    \"\"\"Calculate probability of getting a specific sum with three fair dice.\"\"\"\n    possibilities = 0\n    total_outcomes = 216  # 6^3 possible outcomes\n    \n    for i in range(1, 7):\n        for j in range(1, 7):\n            for k in range(1, 7):\n                if i + j + k == dice_sum:\n                    possibilities += 1\n                        \n    return possibilities / total_outcomes\n\ndef get_probability_of_sum_weighted(dice_sum):\n    # We want to pretend it has the same probability and see how much of an outlier weighted dice are\n    return get_probability_of_sum(dice_sum)\n\ndef simulate_rolls_fair(num_rolls=10):\n    \"\"\"Simulate a series of rolls with fair dice and calculate cumulative log probability.\"\"\"\n    rolls = [roll_three_dice_fair() for _ in range(num_rolls)]\n    probabilities = [get_probability_of_sum(roll) for roll in rolls]\n    \n    # To handle log(0), replace zero probabilities with a very small number\n    probabilities = [p if p &gt; 0 else 1e-10 for p in probabilities]\n    log_probs = np.log10(probabilities)\n    cumulative_log_probs = np.cumsum(log_probs)\n    \n    return rolls, cumulative_log_probs\n\ndef simulate_rolls_weighted(num_rolls=10):\n    \"\"\"Simulate a series of rolls with weighted dice and calculate cumulative log probability.\"\"\"\n    rolls = [roll_three_dice_weighted() for _ in range(num_rolls)]\n    probabilities = [get_probability_of_sum_weighted(roll) for roll in rolls]\n    \n    # To handle log(0), replace zero probabilities with a very small number\n    probabilities = [p if p &gt; 0 else 1e-10 for p in probabilities]\n    log_probs = np.log10(probabilities)\n    cumulative_log_probs = np.cumsum(log_probs)\n    \n    return rolls, cumulative_log_probs\n\n# -------------------------- UI Definition ---------------------------\napp_ui = ui.page_fluid(\n    ui.h2(\"Three-Dice Roll Simulations with Cumulative Log Probability\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_select(\n                \"selectedSim\", \n                \"Select Simulation to Highlight\", \n                choices=[],   # initially empty, will be updated dynamically\n                multiple=False,\n            ),\n            ui.output_text(\"selectedDetails\"),\n            ui.hr(),\n            # Removed the Number of Simulations slider\n            # Set Number of Simulations to 100 for fair and 5 for weighted\n            # Retain Number of Rolls slider\n            ui.input_slider(\"numRolls\", \"Number of Rolls per Simulation\", min=5, max=100, value=10, step=1),\n            ui.input_action_button(\"runSim\", \"Run Simulations\")\n        ),\n        ui.output_plot(\"probPlot\", height=\"500px\"),\n    )\n)\n\n# -------------------------- Server Definition -------------------------\ndef server(input, output, session):\n    # Reactive value to store simulations as a list of tuples: (label, cum_log_probs)\n    simulations = reactive.Value([])\n    \n    # Reactive: Perform simulations when 'Run Simulations' button is clicked\n    @reactive.Effect\n    def _run_simulations():\n        input.runSim()  # Depend on the runSim button\n        num_rolls = input.numRolls()\n        new_simulations = []\n        \n        # Run 99 fair simulations (99 + 1 unfair = 100)\n        num_sim_fair = 99\n        for i in range(num_sim_fair):\n            _, cum_probs = simulate_rolls_fair(num_rolls)\n            label = f\"Simulation {i+1}\"\n            new_simulations.append((label, cum_probs))\n        \n        # Run 1 weighted simulations (was 5)\n        num_sim_weighted = 1\n        for i in range(num_sim_weighted):\n            _, cum_probs = simulate_rolls_weighted(num_rolls)\n            label = f\"Weighted Simulation {i+1}\"\n            new_simulations.append((label, cum_probs))\n        \n        simulations.set(new_simulations)\n        \n        # Update the select input choices\n        sim_choices = [sim[0] for sim in new_simulations]\n        ui.update_select(\n            \"selectedSim\",\n            choices=sim_choices,\n            selected=sim_choices[0] if sim_choices else \"\"\n        )\n    \n    # Initialize simulations on app start\n    @reactive.Effect\n    def _initialize():\n        input.runSim()  # Trigger initial simulation run\n    \n    # Plot the simulations, highlighting the selected one\n    @output\n    @render.plot\n    def probPlot():\n        sims = simulations()\n        num_sim = len(sims)\n        if num_sim == 0:\n            fig, ax = plt.subplots()\n            ax.text(0.5, 0.5, \"No simulations to display.\\nClick 'Run Simulations' to start.\", \n                    horizontalalignment='center', verticalalignment='center', fontsize=12)\n            ax.axis('off')\n            return fig\n        \n        num_rolls = len(sims[0][1])\n        x = np.arange(1, num_rolls + 1)\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot all fair simulations in light gray\n        for label, probs in sims:\n            if label.startswith(\"Simulation \"):\n                ax.plot(x, probs, color='#D3D3D3', alpha=0.5)\n        \n        # Plot all weighted simulations in blue\n        for label, probs in sims:\n            if label.startswith(\"Weighted Simulation \"):\n                ax.plot(x, probs, color='blue', alpha=0.7, linestyle='--')\n        \n        # Highlight the selected simulation\n        selected = input.selectedSim()\n        if selected:\n            try:\n                # Find the simulation by label\n                selected_sim = next((sim for sim in sims if sim[0] == selected), None)\n                if selected_sim:\n                    label, selected_probs = selected_sim\n                    color = 'red' if label.startswith(\"Simulation \") else 'green'\n                    linestyle = '-' if label.startswith(\"Simulation \") else '--'\n                    ax.plot(x, selected_probs, color=color, linewidth=2.5, linestyle=linestyle, label=label)\n            except (IndexError, ValueError):\n                pass\n        \n        ax.set_xlabel('Number of Rolls')\n        ax.set_ylabel('Cumulative Log10 Probability')\n        ax.set_title('Cumulative Log Probability of Multiple Three-Dice Roll Simulations')\n        ax.grid(True)\n        if selected:\n            ax.legend()\n        \n        plt.tight_layout()\n        return fig\n    \n    # Display details of the selected simulation, including percentile\n    @output\n    @render.text\n    def selectedDetails():\n        sims = simulations()\n        selected = input.selectedSim()\n        if not selected:\n            return \"No simulation selected.\"\n        try:\n            # Find the simulation by label\n            selected_sim = next((sim for sim in sims if sim[0] == selected), None)\n            if not selected_sim:\n                return \"Selected simulation not found.\"\n            label, cum_probs = selected_sim\n            # Get the final cumulative log probability\n            final_log_prob = cum_probs[-1]\n            # Collect all final cumulative log probabilities\n            all_final_log_probs = [sim[1][-1] for sim in sims]\n            # Compute percentile\n            percentile = (np.sum(np.array(all_final_log_probs) &lt;= final_log_prob) / len(all_final_log_probs)) * 100\n            return (\n                f\"{label}\\n\\n\"\n                f\"Final Cumulative Log10 Probability: {final_log_prob:.4f}\\n\"\n                f\"Percentile: {percentile:.2f}th\"\n            )\n        except (IndexError, ValueError):\n            return \"Invalid selection.\"\n\napp = App(app_ui, server)\nYou may notice that it’s easier to spot the weighted dice with a longer series of rolls. In just a couple of rolls they often don’t seem that unusual, but most of the time over 100 roles they have a markedly different slope than the other roles. When we calculate the percentile of the weighted dice, we are approximating the p-value of a hypothesis test. The p-value essentially says, if you assume your model is correct, here is the probability of witnessing data as, or more, extreme than what you witnessed. If the p-value becomes small enough, you may suspect it’s not bad luck, but that the data is being generated by a different model.\n\n\n\n\n\n\nNote\n\n\n\nTraditional hypothesis testing starts with a null model which we can think of as the status quo. If the data has less than a (traditionally) 0.05 probability of being generated by that null model, we assume that something other than the null model generated the data.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#appendix-on-probability-distributions",
    "href": "2.1-discrete_probability_distributions.html#appendix-on-probability-distributions",
    "title": "Discrete Probability Distributions",
    "section": "Appendix on Probability Distributions",
    "text": "Appendix on Probability Distributions\nFor the interested reader, we include an appendix to try to cement more general concepts of probability distributions.\nJohn K. Kruschke said it well:\n\nA probability distribution is simply a list of all possible outcomes and their corresponding probabilities.\n\nWe think probability distributions in statistics is confusing because of the extreme emphasis on the named parametric probability distributions. Here we build up intuition for probability distributions in the general sense, with some reference to the named parametric probability distributions when appropriate.\n\nJoint Discrete Probability Distributions\nThe first thing we normally note in a data generating process is what we consider to be the outcome, or in common mathematical parlance, the \\(y\\) variable. For example, if we are asking whether a car breaks down over the course of a year, we want P(Breakdown).\n\n\n\nBreakdown\nProbability\n\n\n\n\nYes\n0.30\n\n\nNo\n0.70\n\n\n\nThis is an empirical discrete probability distribution as it is based on observations (or in this case made up observations). We could also track whether a car, over the course of a year, has experienced extreme heat, which we could call P(Extreme_Heat).\n\n\n\nExtreme_Heat\nProbability\n\n\n\n\nYes\n0.40\n\n\nNo\n0.60\n\n\n\nNow, we could be interested in the combination of the two things, which would be the combination of all possible P(Breakdown) and P(Extreme_Heat) outcomes, which we’d call P(Breakdown, Extreme_Heat). Here’s a possible version of that joint probability distribution:\n\n\n\nBreakdown\nExtreme_Heat\nProbability\n\n\n\n\nYes\nYes\n0.10\n\n\nYes\nNo\n0.20\n\n\nNo\nYes\n0.30\n\n\nNo\nNo\n0.40\n\n\n\nNow it’s worth pointing out that while many of us may be interested in the breakdown outcome, one could also be interested in the extreme heat outcome - what we consider the \\(y\\) can be a matter of perspective. Now it’s obvious that more variables could be part of this joint probability distribution, although their usefulness ultimately depends on how correlated they are to the outcome of interest. Lets assume we want to predict breakdowns, so we add another variable that is probably useful. Now we have P(Brand, Breakdown, Extreme_Heat) in a joint probability distribution:\n\n\n\nBrand\nBreakdown\nExtreme Heat\nProbability\n\n\n\n\nBrandA\nYes\nYes\n0.05\n\n\nBrandA\nYes\nNo\n0.10\n\n\nBrandA\nNo\nYes\n0.15\n\n\nBrandA\nNo\nNo\n0.20\n\n\nBrandB\nYes\nYes\n0.10\n\n\nBrandB\nYes\nNo\n0.10\n\n\nBrandB\nNo\nYes\n0.10\n\n\nBrandB\nNo\nNo\n0.20\n\n\n\nWe could imagine all sorts of variables that may impact Breakdown. Please also note, that regardless of how many variables we have, the probability column always sums to 1. Also note that we chose only two possibilities for each column (e.g. BrandA, BrandB), but there could have been many more.\n\n\nConditional Discrete Probability Distributions\nA conditional probability distribution is one in which we are given the state of one or more variables. For example, let’s examine P(Breakdown, Extreme_Heat | Brand = BrandB), which in words is the joint probability distribution of Breakdown and Extreme_Heat given that Brand = BrandB.\n\n\n\nBrand\nBreakdown\nExtreme Heat\nProbability\n\n\n\n\nBrandB\nYes\nYes\n0.20\n\n\nBrandB\nYes\nNo\n0.20\n\n\nBrandB\nNo\nYes\n0.20\n\n\nBrandB\nNo\nNo\n0.40\n\n\n\nBecause BrandB is a constant, we could just write it this way:\n\n\n\nBreakdown\nExtreme Heat\nProbability\n\n\n\n\nYes\nYes\n0.20\n\n\nYes\nNo\n0.20\n\n\nNo\nYes\n0.20\n\n\nNo\nNo\n0.40\n\n\n\nNote how this is not the same probability distribution as P(Breakdown, Extreme_Heat).\nWhen we work with a parametric probability distribution, like the binomial, we give the value of the distributions parameters so they can be plugged into the mathematical model of the data generating process - which is what distinguishes it from an empirical distribution. Here is a discrete probability distribution for a Binomial process with n=3 and p=0.2, i.e. P( k | n=3, p=0.2).\n\n\n\nSuccesses (k)\nn\nk\nProbability\n\n\n\n\n0\n3\n0.2\n0.512\n\n\n1\n3\n0.2\n0.384\n\n\n2\n3\n0.2\n0.096\n\n\n3\n3\n0.2\n0.008\n\n\n\nBecause n and k are constant, we may prefer a table without them, although they are still there, implicitly.\n\n\n\nSuccesses (k)\nProbability\n\n\n\n\n0\n0.512\n\n\n1\n0.384\n\n\n2\n0.096\n\n\n3\n0.008\n\n\n\nWe can’t examine the joint probability distribution of the Binomial distribution without giving a value for n and p because the number of possibilities for n and p are infinite, and subsequently the joint probability distribution is infinite. Note we can effectively do the same thing with the ‘Breakdown’ joint probability distribution if we just changed extreme_heat to temperature, and tried to observe breakdowns for every temperature. The named probability distributions are special in that we have a complete mathematical description of the process, so we can simply calculate the results of any combination, while in the empirical distribution of Breakdown, we would have to observe the results.\nIt is also possible to have a probability distribution from one of the parametric probability distributions in which we have a set of values for the parameters, instead of one value. For example, we can have a Binomial distribution using the set notation \\(\\in\\) which states that \\(n\\) must be either 3 or 4 and \\(p\\) must be either 0.2 or 0.3.\n\\[\nP(k, \\quad n \\in \\{3, 4\\}, \\quad p \\in \\{0.2, 0.3\\})\n\\]\nWe should also explicity define how each combination in the set contributes to the total probability, but for this simple example we will assume they contribute equally. This is like the ‘Breakdown’ table which was limited to the following set of data:\n\\[\nP(Breakdown, \\quad ExtremeHeat \\in \\{Yes, No\\}, \\quad Brand \\in \\{BrandA, BrandB\\})\n\\]\nAt this point hopefully the empirical and parametric probability distributions ‘feel’ the same, except we know the mathematical models behind the parametric probability distributions, so we do not have to guess their distributions through observation.\n\n\nMarginal Discrete Probability Distributions\nA marginal probability distribution is essentially a way to work from the end of the section Joint Discrete Probability Distributions backwards to the beginning - which is a way to eliminate variables from the joint probability distribution and make them simpler. Let’s say we want to eliminate the Extreme_Heat variable:\n\n\n\nBreakdown\nExtreme_Heat\nProbability\n\n\n\n\nYes\nYes\n0.10\n\n\nYes\nNo\n0.20\n\n\nNo\nYes\n0.30\n\n\nNo\nNo\n0.40\n\n\n\nWe simply add up all the possible values of Extreme_Heat within a given Breakdown, e.g. for Breakdown = Yes, 0.10 + 0.20, and for Breakdown = No, 0.30+ 0.40. Then we arrive at the original table:\n\n\n\nBreakdown\nProbability\n\n\n\n\nYes\n0.30\n\n\nNo\n0.70\n\n\n\nThe name ‘marginal’ comes from the fact that, in the times of paper tables, a statistician could do this addition and write the result in the margin.\n\n\nSummary App\nAfter the discussion on probability distributions, we think, as usual, that it is best to learn by applying the concepts in a very hands-on way. Use the app below to see the behavior of a dataset under different joint and marginal conditions.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Simulated Breakdown Data\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"numEvents\",\n                \"Number of Simulated Outcomes\",\n                min=1,\n                max=100000,\n                value=10000,\n                step=1000,\n            ),\n            ui.input_checkbox_group(\n                \"covariates\",\n                \"Select Covariates to Group By\",\n                choices=[\"Mileage\", \"Brand\"],\n                inline=True,\n                selected=[\"Mileage\", \"Brand\"],\n            ),\n            ui.input_select(\n                \"filter_covariate\",\n                \"Conditional Filter: Choose Covariate\",\n                choices=[\"None\", \"Mileage\", \"Brand\"],\n                selected=\"None\",\n            ),\n            # Dynamic UI element for filter value input:\n            ui.output_ui(\"filter_value_ui\"),\n        ),\n        ui.output_table(\"results_table\")\n    ),\n)\n\ndef server(input, output, session):\n    # Set the seed for reproducibility.\n    np.random.seed(42)\n    \n    @reactive.Calc\n    def simulated_data():\n        n = input.numEvents()\n        miles_driven = np.random.lognormal(mean=np.log(10000), sigma=0.5, size=n)\n        brands = np.random.choice(\n            [\"Brand_A\", \"Brand_B\", \"Brand_C\"],\n            size=n,\n            p=[0.4, 0.4, 0.2],\n        )\n        beta_miles = 0.0000000001\n        brand_effects = {\"Brand_A\": 0.001, \"Brand_B\": 0.0005, \"Brand_C\": -0.0002}\n        lambda_vals = np.exp(beta_miles * miles_driven + np.vectorize(brand_effects.get)(brands))\n        breakdowns = np.random.poisson(lambda_vals)\n        data = pd.DataFrame({\n            \"Miles_Driven\": miles_driven,\n            \"Brand\": brands,\n            \"Breakdowns\": breakdowns,\n        })\n        data[\"Miles_Binned\"] = (data[\"Miles_Driven\"] // 10000) * 10000\n        return data\n\n    @reactive.Calc\n    def grouped_table():\n        data = simulated_data()\n        covariates = input.covariates()\n        group_cols = []\n        if \"Mileage\" in covariates:\n            group_cols.append(\"Miles_Binned\")\n        if \"Brand\" in covariates:\n            group_cols.append(\"Brand\")\n        group_cols.append(\"Breakdowns\")\n        \n        grouped = data.groupby(group_cols).size().reset_index(name=\"Frequency\")\n        \n        filter_cov = input.filter_covariate()\n        if filter_cov != \"None\":\n            if filter_cov == \"Mileage\":\n                filter_val = input.filter_value()\n                if filter_val is not None:\n                    grouped = grouped[grouped[\"Miles_Binned\"] == float(filter_val)]\n            elif filter_cov == \"Brand\":\n                filter_val = input.filter_value()\n                if filter_val is not None:\n                    grouped = grouped[grouped[\"Brand\"] == filter_val]\n        \n        total_freq = grouped[\"Frequency\"].sum()\n        if total_freq &gt; 0:\n            grouped[\"Probability\"] = grouped[\"Frequency\"] / total_freq\n        else:\n            grouped[\"Probability\"] = 0\n        return grouped\n\n    @output\n    @render.table\n    def results_table():\n        return grouped_table()\n\n    @output\n    @render.ui\n    def filter_value_ui():\n        filter_cov = input.filter_covariate()\n        if filter_cov == \"Mileage\":\n            data = simulated_data()\n            unique_bins = sorted(data[\"Miles_Binned\"].unique())\n            return ui.input_select(\n                \"filter_value\",\n                \"Select Mileage Bin\",\n                choices=[str(int(x)) for x in unique_bins],\n            )\n        elif filter_cov == \"Brand\":\n            return ui.input_select(\n                \"filter_value\",\n                \"Select Brand\",\n                choices=[\"Brand_A\", \"Brand_B\", \"Brand_C\"],\n            )\n        else:\n            return ui.div()\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#footnotes",
    "href": "2.1-discrete_probability_distributions.html#footnotes",
    "title": "Discrete Probability Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne of the common problems with the brevity of mathematical notation is its ambiguity. The statement P(D|M), the probability of the data given a model, can have what statisticians would call a Frequentist or a Bayesian interpretation. This primer assumes the Frequentist interpretation (as implied when we said we use ‘static’ models to generate data) until we get to more advanced topics, in which we will make the switch explicit. The Frequentist interpretation assumes point estimates of the parameters within our model, i.e. a single model that always generates data with the same parameters. We should note that this ‘static’ model can still generate data in a non-deterministic, i.e. probabilistic, way.\nThe other interpretation is the Bayesian one. In the Bayesian interpretation a model is not defined with point estimates of its parameters. The parameters themselves are given probability distributions reflecting the fact that they, in general, cannot be known with certainty. Thus when writing P(D|M) in a Bayesian framework it implies a model with a range of parameter values. There is also additional areas of ambiguity in this notation in the Bayesian approach due to the way P(M|D) is calculated and the way in which it ultimately relies on P(D|M)… We leave much of this for much more advanced textbooks, but will try to balance a reduction in ambiguity for the more advanced reader with simplicity for the more general reader.\nSuffice it to say unless otherwise indicated, P(D|M) is to be interpreted in the most literal way, the probability of data given a model (note the emphasis on a, as in singular).↩︎",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html",
    "href": "2.4-machine_learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Due to popular demand, we move on from traditional statistical models to those considered machine learning. We touch on the differences and similarities between the two categories before choosing to continue with a machine learning model that allows for statistical (i.e. probabilistic) interpretation.\nThe chapter is focused on Bayesian Neural Networks, and although they are a relatively advanced topic, you may be surprised to learn the major building blocks have already been covered. In our version, each neuron is just logistic regression (which is just generalized linear regression) built into a network. Once we build this network we will allow it to generate data like in previous chapters. Because this type of machine learning is probabilistic, we can calculate P(D|M).",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#preview",
    "href": "2.4-machine_learning.html#preview",
    "title": "Machine Learning",
    "section": "",
    "text": "Due to popular demand, we move on from traditional statistical models to those considered machine learning. We touch on the differences and similarities between the two categories before choosing to continue with a machine learning model that allows for statistical (i.e. probabilistic) interpretation.\nThe chapter is focused on Bayesian Neural Networks, and although they are a relatively advanced topic, you may be surprised to learn the major building blocks have already been covered. In our version, each neuron is just logistic regression (which is just generalized linear regression) built into a network. Once we build this network we will allow it to generate data like in previous chapters. Because this type of machine learning is probabilistic, we can calculate P(D|M).",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#statistical-models-vs.-machine-learning",
    "href": "2.4-machine_learning.html#statistical-models-vs.-machine-learning",
    "title": "Machine Learning",
    "section": "Statistical Models vs. Machine Learning",
    "text": "Statistical Models vs. Machine Learning\nI assume most engineers, a priori, consider machine learning models as somehow ‘better’ than statistical models. In reality there is a lot of overlap between them, and the situation should dictate the model selection. However, since we assume the reader will inevitably be interested in machine learning, we think it’s best to include it, at the very least so we can understand the similarities and differences when compared to statistical models.\nDue to the amount of blur/overlap, this may not be a universally agreed distinction, but we define an important difference this way:\n\nStatistical models are built around probability theory\nMachine learning models are built around an objective/loss function\n\nThe confusing thing is that a lot of methods known as being machine learning have objective/loss functions that are purely probabilistic - but the key is they do not have to be.\nThere are other common differences that are also blurry - like that machine learning is focused on larger datasets and often have more flexible (i.e. more parameters) methods. This tends to also mean overfitting is a larger concern in machine learning. You would also typically not define a causal model as a foundation for a machine learning model, as they tend to use predefined algorithms.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#loss-function",
    "href": "2.4-machine_learning.html#loss-function",
    "title": "Machine Learning",
    "section": "Loss Function",
    "text": "Loss Function\nA loss function, denoted as \\(L\\), is typically computed as the difference between the true target values \\(y\\) and the predicted values \\(\\hat{y}\\) The generic form of a loss function can be expressed as:\n\\[\nL(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\nVariables:\n\n\\(y_i\\): The true value for the (i)-th data point.\n\\(\\hat{y}_i\\): The predicted value for the (i)-th data point.\n\\(\\ell(y_i, \\hat{y}_i)\\): The individual error for the (i)-th data point, defined by a specific loss function (e.g., squared error, cross-entropy).\n\\(N\\): The total number of data points.\n\n\n\nExplanation:\n\nThe function \\(\\ell(y_i, \\hat{y}_i)\\) depends on the task (regression or classification).\n\nExample for regression: \\(\\ell(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\\) (Mean Squared Error).\nExample for classification: \\(\\ell(y_i, \\hat{y}_i) = -[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\\) (Binary Cross-Entropy).\n\nThe summation aggregates the error across all data points.\nDividing by \\(N\\) ensures the loss represents the average error.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#likelihood-function",
    "href": "2.4-machine_learning.html#likelihood-function",
    "title": "Machine Learning",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe loss function is in contrast to using likelihood functions, which are based in probability theory. We use likelihood to find the best statistical model, specifically by finding the Maximum Likelihood Estimate (MLE). We will explore these more in the second half of the primer, \\(\\mathscr{L}(M|D)\\).",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#machine-learning-models",
    "href": "2.4-machine_learning.html#machine-learning-models",
    "title": "Machine Learning",
    "section": "Machine Learning Models",
    "text": "Machine Learning Models\nThe following are some of the most common machine learning models. Supervised learning means that there is a result or ‘y’ variable to learn from. Unsupervised learning does not have the result or ‘y’ variable, which is a little confusing - instead of trying to predict they are generally attempting some kind of categorization/clustering or simplification.\nYou may note that the first two on the list below, linear regression and logistic regression, would also firmly sit on most lists of common statistical models.\n\nSupervised Learning\n\nLinear Regression: Predicts continuous values.\nLogistic Regression: For binary or multi-class classification.\nDecision Trees: Simple, interpretable models for classification and regression.\nRandom Forest: Ensemble of decision trees for better accuracy.\nGradient Boosting (e.g., XGBoost, LightGBM): Powerful ensemble method for structured data.\nSupport Vector Machines (SVM): For classification and regression.\nNeural Networks: Flexible models, basis of deep learning.\n\n\n\nUnsupervised Learning\n\nk-Means Clustering: Groups similar data points.\nPrincipal Component Analysis (PCA): Reduces dimensionality while preserving variance.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#bayesian-neural-net",
    "href": "2.4-machine_learning.html#bayesian-neural-net",
    "title": "Machine Learning",
    "section": "Bayesian Neural Net",
    "text": "Bayesian Neural Net\nThere are many machine learning models we could focus on, however, we’ll choose Bayesian Neural Nets for a few reasons:\n\nThey make probabilistic predictions just like our statistical models\nWe have already learned most of the basic building blocks\n[Standard] Neural nets are the basis of modern AI\n\n\n‘Standard’ Neural Net\nBefore we layer in the Bayesian piece, we start with ‘standard’ neural nets. Although there are many variations, we can think of a neural net as layers of artificial neurons that simply perform logistic regression on the previous layer of neurons to determine their activation state.\n\nStructure\nTo explain that further it helps immensely to have a diagram that points out the layers, the neurons, and what information is being fed into each neuron.\n\n\n\n\n\n%%{init: {'themeVariables': {'spacing': 5}}}%%\ngraph LR\n  %% Define classes with desired styles\n  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;\n  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;\n  classDef hiddenLayer2 fill:#d8c2f9,stroke:#6a00d6;\n  classDef outputLayer fill:#c2f9c9,stroke:#00d640;\n\n  subgraph Input Layer\n    N1([\"N1\"])\n    class N1 inputLayer;\n  end\n\n  subgraph Hidden Layer 1\n    N2([\"N2\"])\n    N3([\"N3\"])\n    N4([\"N4\"])\n    N5([\"N5\"])\n    class N2,N3,N4,N5 hiddenLayer1;\n  end\n\n  subgraph Hidden Layer 2\n    N6([\"N6\"])\n    N7([\"N7\"])\n    N8([\"N8\"])\n    N9([\"N9\"])\n    class N6,N7,N8,N9 hiddenLayer2;\n  end\n\n  subgraph Output Layer\n    N10([\"N10\"])\n    class N10 outputLayer;\n  end\n\n  N1 --&gt;|C1| N2\n  N1 --&gt;|C2| N3\n  N1 --&gt;|C3| N4\n  N1 --&gt;|C4| N5\n  \n  N2 --&gt;|C5| N6\n  N2 --&gt;|C6| N7\n  N2 --&gt;|C7| N8\n  N2 --&gt;|C8| N9\n  \n  N3 --&gt;|C9| N6\n  N3 --&gt;|C10| N7\n  N3 --&gt;|C11| N8\n  N3 --&gt;|C12| N9\n  \n  N4 --&gt;|C13| N6\n  N4 --&gt;|C14| N7\n  N4 --&gt;|C15| N8\n  N4 --&gt;|C16| N9\n  \n  N5 --&gt;|C17| N6\n  N5 --&gt;|C18| N7\n  N5 --&gt;|C19| N8\n  N5 --&gt;|C20| N9\n  \n  N6 --&gt;|C21| N10\n  N7 --&gt;|C22| N10\n  N8 --&gt;|C23| N10\n  N9 --&gt;|C24| N10\n\n\n\n\n\n\n\n\nAs shown in the diagram above, there are the following pieces with the following purpose:\n\nThe Input Layer, which takes in data. The number of neurons match the number of variables (or dimensions) of the data we want to input. We are going to input one variable (the size/carats of a diamond) so we only need one input neuron.\nThat input goes to Hidden Layer 1. Each neuron will take the input and be activated according to the input value and a weight/coefficient associated with the neuron.\nAt Hidden Layer 2 things get interesting. Each of these neurons take the values from the first layer of neurons and performs another logistic regression to determine their activation. From the diagram you can see there are lots of connections here. N6 will take the activation value from N2 and apply it’s own weight/coefficient, same with N3, N4, and N5, and use logistic regression to get it’s own activation. That same process repeats with N7, N8, and N9, each with their own weights/coefficients.\nThe Output Layer which is a final regression using the activation of N6, N7, N8, and N9 to get the output value of the neural net.\n\n\n\nWeights/Coefficients\nWhile the structure of a neural net influences the types of problems it is most appropriate for, the knowledge of the neural net is held in the weights. Determining the weights of a neural network is generally accomplished by training on data via backpropogation, for which numerous resources are available elsewhere. Here, we will jump past that the details of that step so that we can proceed with a reasonably trained neural net.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#stan-neural-network-sim",
    "href": "2.4-machine_learning.html#stan-neural-network-sim",
    "title": "Machine Learning",
    "section": "Stan Neural Network Sim",
    "text": "Stan Neural Network Sim\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\n# --- Stan Parameter Summaries ---\nweights_in_means   = np.array([-3.66,  -17.54,  10.90,  16.47])\nweights_in_sds     = np.array([36.38,   13.83,  15.57,  15.74])\n\nbias_in_means      = np.array([0.39, 23.51, -12.30, -16.51])\nbias_in_sds        = np.array([58.16, 23.08, 23.47, 19.09])\n\nweights_hidden_means = np.array([\n    [-6.52,  15.34, -18.64, -18.16],\n    [ 5.09,  -6.29, -23.97,  -2.87],\n    [ 1.61,   0.52,   0.60, -36.48],\n    [ 1.48, -18.81,   7.46,  24.19]\n])\nweights_hidden_sds = np.array([\n    [ 8.49, 21.82, 15.54, 20.49],\n    [16.63, 21.01, 29.03, 27.77],\n    [35.43, 19.57, 17.53, 28.02],\n    [26.89, 27.62, 26.75, 27.26]\n])\n\nbias_hidden_means  = np.array([-0.60, 10.05, 17.50,  6.19])\nbias_hidden_sds    = np.array([9.18, 13.59, 28.24,  5.59])\n\nweights_out_means  = np.array([3661.48, 1975.56, 4818.68, 3557.60])\nweights_out_sds    = np.array([388.84, 2510.70, 1523.12, 492.29])\n\nbias_out_mean      = 688.15\nbias_out_sd        = 1084.63\n\nsigma_mean         = 160.61\nsigma_sd           = 1.08\n\n# --- Neural Network Forward Pass ---\ndef neural_net_forward(x, weights_in, bias_in, weights_hidden, bias_hidden, weights_out, bias_out):\n    z1 = np.dot(x, weights_in) + bias_in      # first hidden layer linear combination\n    a1 = 1 / (1 + np.exp(-z1))                # sigmoid activation\n    z2 = np.dot(a1, weights_hidden) + bias_hidden\n    a2 = 1 / (1 + np.exp(-z2))                # second hidden layer activation\n    mu = np.dot(a2, weights_out) + bias_out   # output layer (linear)\n    return mu\n\n# --- Simulation Function ---\ndef simulate_random_predictions(nsim):\n    \"\"\"\n    For each simulation:\n      - Randomly sample a carat value between 0.1 and 2,\n      - Sample the neural network parameters from Normal distributions,\n      - Compute the predicted diamond price (with noise added).\n    Returns arrays of simulated carat values and predicted prices.\n    \"\"\"\n    sim_carat = []\n    sim_price = []\n    for _ in range(nsim):\n        # Randomly choose a carat value in [0.1, 2]\n        carat_val = np.random.uniform(0.1, 2.0)\n        x_val = np.array([[carat_val]])\n        \n        # Sample NN parameters\n        wi = np.random.normal(weights_in_means, weights_in_sds).reshape(1, 4)\n        bi = np.random.normal(bias_in_means, bias_in_sds).reshape(1, 4)\n        wh = np.random.normal(weights_hidden_means, weights_hidden_sds)\n        bh = np.random.normal(bias_hidden_means, bias_hidden_sds)\n        wo = np.random.normal(weights_out_means, weights_out_sds)\n        bo = np.random.normal(bias_out_mean, bias_out_sd)\n        sigma_sample = np.abs(np.random.normal(sigma_mean, sigma_sd))\n        \n        # Compute network prediction and add noise\n        mu = neural_net_forward(x_val, wi, bi, wh, bh, wo, bo)\n        price_pred = mu + np.random.normal(0, sigma_sample, size=mu.shape)\n        \n        sim_carat.append(carat_val)\n        # Use price_pred[0] since price_pred is 1-dimensional\n        sim_price.append(price_pred[0])\n    return np.array(sim_carat), np.array(sim_price)\n\n# --- Load Diamond Data ---\ndata_str = \"\"\"ID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\ndf = pd.read_csv(StringIO(data_str))\n# Restrict to diamonds with carat values between 0.1 and 2\ndf_filtered = df[(df[\"carat\"] &gt;= 0.1) & (df[\"carat\"] &lt;= 2)]\n\n# --- Shiny UI ---\napp_ui = ui.page_fluid(\n    ui.h2(\"Random Diamond Price Simulation\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"nsim\", \"Number of Simulations\", min=1, max=100, value=20, step=1),\n            ui.input_checkbox(\"show_actual\", \"Show Actual Diamond Prices\", value=True)\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Predicted Prices\",\n                ui.output_plot(\"predictionPlot\", height=\"400px\")\n            )\n        )\n    )\n)\n\n# --- Shiny Server ---\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def simulation_data():\n        nsim = input.nsim()\n        sim_carat, sim_price = simulate_random_predictions(nsim)\n        return {\"sim_carat\": sim_carat, \"sim_price\": sim_price}\n    \n    @output\n    @render.plot\n    def predictionPlot():\n        data_sim = simulation_data()\n        sim_carat = data_sim[\"sim_carat\"]\n        sim_price = data_sim[\"sim_price\"]\n        \n        fig, ax = plt.subplots(figsize=(8,6))\n        ax.scatter(sim_carat, sim_price, color=\"red\", label=\"Simulated Price\", alpha=0.7)\n        \n        if input.show_actual():\n            ax.scatter(df_filtered[\"carat\"], df_filtered[\"price\"], color=\"blue\", label=\"Actual Price\", alpha=0.5)\n        \n        ax.set_xlabel(\"Carat\")\n        ax.set_ylabel(\"Price\")\n        ax.set_title(\"Simulated Diamond Prices vs. Carat Size\")\n        ax.legend()\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#stan-sim-v2",
    "href": "2.4-machine_learning.html#stan-sim-v2",
    "title": "Machine Learning",
    "section": "Stan Sim v2",
    "text": "Stan Sim v2\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# --- Helper Functions ---\n\ndef inv_logit(z):\n    \"\"\"Sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef generate_parameters():\n    \"\"\"\n    Draw a single set of neural network parameters using the Stan summary's means and sds.\n    \"\"\"\n    params = {}\n    # Input -&gt; first hidden layer (1 x 4)\n    params[\"weights_in\"] = np.array([\n         np.random.normal(-0.84,  0.01),\n         np.random.normal(2.71, 0.01),\n         np.random.normal(-0.82, 0.01),\n         np.random.normal(-9.16, 0.01)\n    ])\n    params[\"bias_in\"] = np.array([\n         np.random.normal(1.03, 0.01),\n         np.random.normal(-3.78, 0.01),\n         np.random.normal(0.92, 0.01),\n         np.random.normal(13.41, 0.014)\n    ])\n    \n    # First hidden layer -&gt; second hidden layer (4 x 4)\n    params[\"weights_hidden\"] = np.array([\n        [np.random.normal(3.16, 0.01), np.random.normal(-4.52, 0.01),\n         np.random.normal(-0.40, 0.01), np.random.normal(0.07, 0.01)],\n        [np.random.normal(-8.63, 0.01), np.random.normal(7.35, 0.01),\n         np.random.normal(2.56, 0.01), np.random.normal(-1.52, 0.01)],\n        [np.random.normal(0.05, 0.01), np.random.normal(-1.25, 0.01),\n         np.random.normal(-0.73, 0.01), np.random.normal(-2.39, 0.01)],\n        [np.random.normal(-7.14, 0.01), np.random.normal(-1.35, 0.01),\n         np.random.normal(-7.77, 0.01), np.random.normal(4.00, 0.01)]\n    ])\n    params[\"bias_hidden\"] = np.array([\n         np.random.normal(3.71, 0.01),\n         np.random.normal(-3.22, 0.01),\n         np.random.normal(2.19, 0.01),\n         np.random.normal(-1.57, 0.01)\n    ])\n    \n    # Second hidden layer -&gt; output layer (vector of length 4)\n    params[\"weights_out\"] = np.array([\n         np.random.normal(3856.76, 495.69),\n         np.random.normal(3715.54, 513.76),\n         np.random.normal(3879.81, 467.75),\n         np.random.normal(3916.50, 487.24)\n    ])\n    params[\"bias_out\"] = np.random.normal(528.46, 120.71)\n    \n    # Noise standard deviation (sigma)\n    params[\"sigma\"] = np.random.normal(470.55, 5.22)\n    return params\n\ndef simulate_network(x, params):\n    \"\"\"\n    Given input array x (carat values) and a set of parameters,\n    compute the network output and add noise (sigma scales with x).\n    \"\"\"\n    # First hidden layer: apply sigmoid( x * weights_in + bias_in )\n    h1 = inv_logit( x[:, None] * params[\"weights_in\"][None, :] + params[\"bias_in\"][None, :] )\n    # Second hidden layer: apply sigmoid( h1 dot weights_hidden + bias_hidden )\n    h2 = inv_logit( np.dot(h1, params[\"weights_hidden\"]) + params[\"bias_hidden\"] )\n    # Output: linear combination (no activation)\n    mu = np.dot(h2, params[\"weights_out\"]) + params[\"bias_out\"]\n    # Add noise with standard deviation proportional to carat\n    # noise = np.random.normal(0, params[\"sigma\"] * x)\n    # return mu + noise\n    return mu\n\n# --- Define the UI ---\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Neural Network Data Generator\"),\n    ui.p(\"Generate simulated diamond prices from a Bayesian neural network.\"),\n    ui.input_slider(\"n_points\", \"Total number of points:\",\n                    min=100, max=1000, value=100, step=100),\n    ui.input_action_button(\"add\", \"Add to Plot\"),\n    ui.input_action_button(\"reset\", \"Reset\"),\n    ui.output_plot(\"plot\")\n)\n\n# --- Define the Server ---\n\ndef server(input, output, session):\n    # Reactive value to store accumulated data across \"Add to Plot\" clicks.\n    accumulated_data = reactive.Value(pd.DataFrame(columns=[\"carat\", \"price\", \"block\"]))\n    # A counter to number each block uniquely.\n    block_counter = reactive.Value(1)\n    \n    @reactive.Effect\n    @reactive.event(input.add)\n    def add_points():\n        n_points = input.n_points()\n        blocks = n_points // 100  # Each block is 100 points.\n        current_df = accumulated_data.get()\n        counter = block_counter.get()\n        new_blocks = []\n        for _ in range(blocks):\n            # Generate 100 carat values between 0.5 and 2.5.\n            x = np.linspace(0.5, 2.5, 100)\n            params = generate_parameters()  # Draw new parameters for each block.\n            y = simulate_network(x, params)\n            block_label = f\"Block {counter}\"\n            counter += 1\n            df = pd.DataFrame({\"carat\": x, \"price\": y, \"block\": block_label})\n            new_blocks.append(df)\n        # Append the new blocks to the accumulated data.\n        new_data = pd.concat(new_blocks, ignore_index=True)\n        accumulated_data.set(pd.concat([current_df, new_data], ignore_index=True))\n        block_counter.set(counter)\n    \n    @reactive.Effect\n    @reactive.event(input.reset)\n    def reset_data():\n        # Clear the accumulated data and reset the block counter.\n        accumulated_data.set(pd.DataFrame(columns=[\"carat\", \"price\", \"block\"]))\n        block_counter.set(1)\n    \n    @output\n    @render.plot\n    def plot():\n        df = accumulated_data.get()\n        fig, ax = plt.subplots(figsize=(6,4))\n        if not df.empty:\n            # Plot each block's points as scatter (no connecting lines)\n            for block, group in df.groupby(\"block\"):\n                ax.scatter(group[\"carat\"], group[\"price\"], label=block)\n        ax.set_xlabel(\"Carat\")\n        ax.set_ylabel(\"Price\")\n        ax.set_title(\"Simulated Data from Bayesian Neural Network\")\n        if not df.empty:\n            ax.legend()\n        return fig\n\n# --- Create the App ---\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#point-estimate-neural-network",
    "href": "2.4-machine_learning.html#point-estimate-neural-network",
    "title": "Machine Learning",
    "section": "Point-Estimate Neural Network",
    "text": "Point-Estimate Neural Network\n\nTraining a Simple Neural Network\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\n# Data (an excerpt from the diamond dataset)\ndata_str = \"\"\"ID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\n# --- Activation Functions ---\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_deriv(x):\n    return (x &gt; 0).astype(float)\n\ndef logistic(x):\n    return 1 / (1 + np.exp(-x))\n\ndef logistic_deriv(x):\n    sig = logistic(x)\n    return sig * (1 - sig)\n\n# --- Neural Network Class ---\n# Now accepts an 'activation' parameter (\"relu\" or \"logistic\") and hidden layer size.\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, activation=\"relu\"):\n        self.activation = activation  # \"relu\" or \"logistic\"\n        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)\n        self.b1 = np.zeros((1, hidden_size1))\n        \n        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)\n        self.b2 = np.zeros((1, hidden_size2))\n        \n        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)\n        self.b3 = np.zeros((1, output_size))\n    \n    def forward(self, X):\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        if self.activation == \"relu\":\n            self.A1 = relu(self.Z1)\n        else:\n            self.A1 = logistic(self.Z1)\n        \n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        if self.activation == \"relu\":\n            self.A2 = relu(self.Z2)\n        else:\n            self.A2 = logistic(self.Z2)\n        \n        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n        return self.Z3\n    \n    def compute_loss(self, y_pred, y_true):\n        return np.mean((y_pred - y_true)**2)\n    \n    def backward(self, X, y_true, y_pred, learning_rate):\n        m = y_true.shape[0]\n        dZ3 = (2.0 / m) * (y_pred - y_true)\n        dW3 = np.dot(self.A2.T, dZ3)\n        db3 = np.sum(dZ3, axis=0, keepdims=True)\n        \n        dA2 = np.dot(dZ3, self.W3.T)\n        if self.activation == \"relu\":\n            dZ2 = dA2 * relu_deriv(self.Z2)\n        else:\n            dZ2 = dA2 * logistic_deriv(self.Z2)\n        dW2 = np.dot(self.A1.T, dZ2)\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dA1 = np.dot(dZ2, self.W2.T)\n        if self.activation == \"relu\":\n            dZ1 = dA1 * relu_deriv(self.Z1)\n        else:\n            dZ1 = dA1 * logistic_deriv(self.Z1)\n        dW1 = np.dot(X.T, dZ1)\n        db1 = np.sum(dZ1, axis=0, keepdims=True)\n        \n        # Update parameters.\n        self.W3 -= learning_rate * dW3\n        self.b3 -= learning_rate * db3\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def train(self, X, y, epochs, learning_rate):\n        losses = []\n        for epoch in range(epochs):\n            y_pred = self.forward(X)\n            loss = self.compute_loss(y_pred, y)\n            losses.append(loss)\n            self.backward(X, y, y_pred, learning_rate)\n        return losses\n\n# --- Shiny UI ---\napp_ui = ui.page_fluid(\n    ui.h2(\"Neural Network Training\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"epochs\", \"Number of Training Epochs\", min=100, max=5000, value=1000, step=100),\n            ui.input_select(\"activation\", \"Activation Function\", choices=[\"ReLu\", \"Logistic\"], selected=\"Logistic\"),\n            ui.input_slider(\"hidden_neurons\", \"Number of Neurons in Hidden Layers\", min=1, max=20, value=4, step=1)\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Training Loss\",\n                ui.output_plot(\"lossPlot\", height=\"400px\")\n            ),\n            ui.nav_panel(\"Predictions\",\n                ui.output_plot(\"predictionPlot\", height=\"400px\")\n            )\n        )\n    )\n)\n\n# --- Shiny Server ---\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def train_model():\n        # Read the diamond dataset.\n        df = pd.read_csv(StringIO(data_str))\n        X = df[\"carat\"].values.reshape(-1, 1)\n        y = df[\"price\"].values.reshape(-1, 1)\n        \n        # Standardize the data.\n        X_mean, X_std = X.mean(), X.std()\n        X_norm = (X - X_mean) / X_std\n        \n        y_mean, y_std = y.mean(), y.std()\n        y_norm = (y - y_mean) / y_std\n        \n        # Choose the activation function.\n        activation = input.activation().lower()\n        \n        # Get the number of neurons for the hidden layers.\n        hidden_neurons = input.hidden_neurons()\n        \n        # Create and train the neural network.\n        nn = NeuralNetwork(input_size=1, hidden_size1=hidden_neurons, hidden_size2=hidden_neurons, \n                           output_size=1, activation=activation)\n        epochs = input.epochs()\n        learning_rate = 0.01\n        losses = nn.train(X_norm, y_norm, epochs, learning_rate)\n        \n        # Generate predictions.\n        y_pred_norm = nn.forward(X_norm)\n        y_pred = y_pred_norm * y_std + y_mean\n        \n        return {\"losses\": losses, \"X\": X, \"y\": y, \"y_pred\": y_pred}\n    \n    @output\n    @render.plot\n    def lossPlot():\n        result = train_model()\n        losses = result[\"losses\"]\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(losses, color=\"blue\")\n        ax.set_title(\"Training Loss over Epochs\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"MSE Loss\")\n        return fig\n    \n    @output\n    @render.plot\n    def predictionPlot():\n        result = train_model()\n        X = result[\"X\"]\n        y = result[\"y\"]\n        y_pred = result[\"y_pred\"]\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.scatter(X, y, color=\"blue\", label=\"Actual Price\")\n        ax.scatter(X, y_pred, color=\"red\", label=\"Predicted Price\")\n        ax.set_title(\"Actual vs. Predicted Diamond Price\")\n        ax.set_xlabel(\"Carat\")\n        ax.set_ylabel(\"Price\")\n        ax.legend()\n        return fig\n\napp = App(app_ui, server)\n\nIn training a ‘standard’ neural network, we hope to obtain a point estimate of the parameter values that will have the most accuracy in predicting data.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#bayesian-neural-network-simplified-implementation",
    "href": "2.4-machine_learning.html#bayesian-neural-network-simplified-implementation",
    "title": "Machine Learning",
    "section": "Bayesian Neural Network (Simplified Implementation)",
    "text": "Bayesian Neural Network (Simplified Implementation)\nThe difference between a ‘standard’ neural network and a Bayesian one is that the Bayesian neural network will have uncertainty associated with the parameter values. (It could also have meaningful prior values for the parameters, but this is generally not relevant when training a new model). Here we will assume this uncertainty follows a normal/gaussian distribution. This uncertainty is key to making this method statistical/probabilistic - otherwise an input will always produce the same output. If the structure of the neural net is appropriate for the model and it has been well trained, a Bayesian neural network should mimic the variation/uncertainty in the outputs.\n\nMore control on noise\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\n\n# --- Diamond Dataset ---\ndata_str = \"\"\"ID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n\n\n# --- Activation Function (ReLU) and Its Derivative ---\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_deriv(x):\n    return (x &gt; 0).astype(float)\n\n# --- Neural Network Class with SGLD and Customizable Noise & LR Decay ---\n# Fixed architecture: 1-4-4-1 using ReLU.\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)\n        self.b1 = np.zeros((1, hidden_size1))\n        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)\n        self.b2 = np.zeros((1, hidden_size2))\n        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)\n        self.b3 = np.zeros((1, output_size))\n    \n    def forward(self, X):\n        self.Z1 = np.dot(X, self.W1) + self.b1\n        self.A1 = relu(self.Z1)\n        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n        self.A2 = relu(self.Z2)\n        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n        return self.Z3\n    \n    def compute_loss(self, y_pred, y_true):\n        return np.mean((y_pred - y_true)**2)\n    \n    def backward(self, X, y_true, y_pred, lr, noise_multiplier, noise_exponent):\n        m = y_true.shape[0]\n        dZ3 = (2.0 / m) * (y_pred - y_true)\n        dW3 = np.dot(self.A2.T, dZ3)\n        db3 = np.sum(dZ3, axis=0, keepdims=True)\n        \n        dA2 = np.dot(dZ3, self.W3.T)\n        dZ2 = dA2 * relu_deriv(self.Z2)\n        dW2 = np.dot(self.A1.T, dZ2)\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dA1 = np.dot(dZ2, self.W2.T)\n        dZ1 = dA1 * relu_deriv(self.Z1)\n        dW1 = np.dot(X.T, dZ1)\n        db1 = np.sum(dZ1, axis=0, keepdims=True)\n        \n        # Customizable SGLD update:\n        # Compute noise standard deviation based on the user-specified exponent and multiplier.\n        noise_std = noise_multiplier * (lr ** noise_exponent)\n        self.W3 = self.W3 - (lr/2)*dW3 + np.random.normal(0, noise_std, self.W3.shape)\n        self.b3 = self.b3 - (lr/2)*db3 + np.random.normal(0, noise_std, self.b3.shape)\n        self.W2 = self.W2 - (lr/2)*dW2 + np.random.normal(0, noise_std, self.W2.shape)\n        self.b2 = self.b2 - (lr/2)*db2 + np.random.normal(0, noise_std, self.b2.shape)\n        self.W1 = self.W1 - (lr/2)*dW1 + np.random.normal(0, noise_std, self.W1.shape)\n        self.b1 = self.b1 - (lr/2)*db1 + np.random.normal(0, noise_std, self.b1.shape)\n    \n    def train(self, X, y, epochs, learning_rate, burn_in, sample_interval, noise_multiplier, noise_exponent, lr_decay_factor):\n        burn_in_losses = []\n        sampling_losses = []\n        samples = []\n        \n        lr = learning_rate\n        for epoch in range(epochs):\n            y_pred = self.forward(X)\n            loss = self.compute_loss(y_pred, y)\n            if epoch &lt; burn_in:\n                burn_in_losses.append(loss)\n            else:\n                sampling_losses.append(loss)\n            self.backward(X, y, y_pred, lr, noise_multiplier, noise_exponent)\n            # At the end of burn-in, decay the learning rate.\n            if epoch == burn_in:\n                lr = learning_rate * lr_decay_factor\n            # After burn-in, record weight samples at intervals.\n            if epoch &gt;= burn_in and (epoch - burn_in) % sample_interval == 0:\n                sample = {\n                    \"W1\": self.W1.copy(),\n                    \"b1\": self.b1.copy(),\n                    \"W2\": self.W2.copy(),\n                    \"b2\": self.b2.copy(),\n                    \"W3\": self.W3.copy(),\n                    \"b3\": self.b3.copy()\n                }\n                samples.append(sample)\n        return burn_in_losses, sampling_losses, samples\n\n# --- Helper: Forward Pass with a Given Weight Sample ---\ndef forward_with_sample(X, sample):\n    Z1 = np.dot(X, sample[\"W1\"]) + sample[\"b1\"]\n    A1 = relu(Z1)\n    Z2 = np.dot(A1, sample[\"W2\"]) + sample[\"b2\"]\n    A2 = relu(Z2)\n    Z3 = np.dot(A2, sample[\"W3\"]) + sample[\"b3\"]\n    return Z3\n\n# --- Shiny UI ---\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Neural Network with SGLD - Custom Noise and LR Decay\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"epochs\", \"Training Epochs\", min=100, max=3000, value=1000, step=100),\n            ui.input_slider(\"learning_rate\", \"Learning Rate\", min=0.0001, max=0.01, value=0.01, step=0.0001),\n            ui.input_slider(\"burn_in\", \"Burn-in Epochs\", min=0, max=1000, value=500, step=50),\n            ui.input_slider(\"sample_interval\", \"Sample Interval\", min=1, max=50, value=10, step=1),\n            ui.input_slider(\"noise_exponent\", \"Noise Exponent\", min=0.1, max=1.0, value=0.5, step=0.05),\n            ui.input_slider(\"noise_multiplier\", \"Noise Multiplier\", min=0.1, max=10.0, value=1.0, step=0.1),\n            ui.input_slider(\"lr_decay\", \"Learning Rate Decay Factor\", min=0.1, max=1.0, value=0.1, step=0.05)\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Burn-in Loss\",\n                ui.output_plot(\"burnInLossPlot\", height=\"400px\")\n            ),\n            ui.nav_panel(\"Sampling Loss\",\n                ui.output_plot(\"samplingLossPlot\", height=\"400px\")\n            ),\n            ui.nav_panel(\"Predictions\",\n                ui.output_plot(\"predictionPlot\", height=\"400px\")\n            )\n        )\n    )\n)\n\n# --- Shiny Server ---\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def train_model():\n        # Read and preprocess data.\n        df = pd.read_csv(StringIO(data_str))\n        X = df[\"carat\"].values.reshape(-1, 1)\n        y = df[\"price\"].values.reshape(-1, 1)\n        X_mean, X_std = X.mean(), X.std()\n        X_norm = (X - X_mean) / X_std\n        y_mean, y_std = y.mean(), y.std()\n        y_norm = (y - y_mean) / y_std\n        \n        # Get hyperparameters from the UI.\n        epochs = input.epochs()\n        learning_rate = input.learning_rate()\n        burn_in = input.burn_in()\n        sample_interval = input.sample_interval()\n        noise_exponent = input.noise_exponent()\n        noise_multiplier = input.noise_multiplier()\n        lr_decay = input.lr_decay()\n        \n        # Create and train the network using SGLD.\n        nn = NeuralNetwork(input_size=1, hidden_size1=4, hidden_size2=4, output_size=1)\n        burn_in_losses, sampling_losses, samples = nn.train(\n            X_norm, y_norm, epochs, learning_rate, burn_in, sample_interval,\n            noise_multiplier, noise_exponent, lr_decay\n        )\n        \n        # Compute predictions from the collected samples.\n        preds = []\n        for sample in samples:\n            pred_norm = forward_with_sample(X_norm, sample)\n            preds.append(pred_norm)\n        preds = np.array(preds)\n        pred_mean_norm = preds.mean(axis=0)\n        pred_std_norm = preds.std(axis=0)\n        pred_mean = pred_mean_norm * y_std + y_mean\n        pred_std = pred_std_norm * y_std\n        \n        return {\"burn_in_losses\": burn_in_losses,\n                \"sampling_losses\": sampling_losses,\n                \"X\": X, \"y\": y,\n                \"pred_mean\": pred_mean, \"pred_std\": pred_std}\n    \n    @output\n    @render.plot\n    def burnInLossPlot():\n        result = train_model()\n        losses = result[\"burn_in_losses\"]\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(losses, color=\"green\")\n        ax.set_title(\"Burn-in Phase Loss\")\n        ax.set_xlabel(\"Epoch (burn-in)\")\n        ax.set_ylabel(\"MSE Loss\")\n        return fig\n    \n    @output\n    @render.plot\n    def samplingLossPlot():\n        result = train_model()\n        losses = result[\"sampling_losses\"]\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(losses, color=\"purple\")\n        ax.set_title(\"Sampling Phase Loss (Post Burn-in)\")\n        ax.set_xlabel(\"Epoch (sampling)\")\n        ax.set_ylabel(\"MSE Loss\")\n        return fig\n    \n    @output\n    @render.plot\n    def predictionPlot():\n        result = train_model()\n        X = result[\"X\"].flatten()\n        y = result[\"y\"].flatten()\n        pred_mean = result[\"pred_mean\"].flatten()\n        pred_std = result[\"pred_std\"].flatten()\n        order = np.argsort(X, axis=0)\n        X_sorted = X[order]\n        pred_mean_sorted = pred_mean[order]\n        pred_std_sorted = pred_std[order]\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.scatter(X, y, color=\"blue\", label=\"Actual Price\")\n        ax.plot(X_sorted, pred_mean_sorted, color=\"red\", label=\"Predicted Mean\")\n        ax.fill_between(X_sorted,\n                        pred_mean_sorted - 2*pred_std_sorted,\n                        pred_mean_sorted + 2*pred_std_sorted,\n                        color=\"red\", alpha=0.3, label=\"Uncertainty (±2σ)\")\n        ax.set_title(\"Actual vs. Predicted Diamond Price (SGLD)\")\n        ax.set_xlabel(\"Carat\")\n        ax.set_ylabel(\"Price\")\n        ax.legend()\n        return fig\n\napp = App(app_ui, server)\n\n\nTraining a Bayesian neural network occurs in effectively two steps:\n\nThe ‘burn-in’ phase, this is similar in some sense to the point estimate of the parameters in a ‘standard’ neural network. Here we want to ensure we have found the parameter space that is somewhat close to the optimal parameter values. This is contrast to being in some random combination of parameters that have practically zero chance of being correct.\nThe inference phase should explore the parameter values from their near-optimum to where they start to become totally unlikely. If done well we can be confident we know not just the most likely value of the parameter, but how uncertain we are in that parameter value.\n\nWe can expect that parameters with impactful values to the results and small uncertainty are very important to the model. The opposite is true as well, if a parameter has a large range of uncertainty, and is not impactful, which typically means the estimated range of the parameter includes zero, then we can conclude the parameter is not improtant to the model.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#discussion",
    "href": "2.4-machine_learning.html#discussion",
    "title": "Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nThis second section is only a rough approximation of Bayesian updating. A more formal approach involves a well-defined posterior, which typically requires advanced methods. However, this snippet shows how the concept of a prior on weights can be approximated by combining a prior-based penalty with noise around the final solution to visualize uncertainty.\nKey takeaways and caveats: - Implementing a neural network from scratch with NumPy requires manual forward/backprop calculations. - True Bayesian neural networks often rely on MCMC or variational inference. - Weight decay alone is not fully Bayesian, but it is consistent with a Gaussian prior if used carefully. - Adding noise around learned weights is only a heuristic approach to capturing model uncertainty, not a rigorous posterior sampling method.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#conclusion",
    "href": "2.4-machine_learning.html#conclusion",
    "title": "Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nWe have demonstrated how to implement (1) a basic neural network using only NumPy and manual gradient-based learning and (2) a very simplified approximation of Bayesian neural networks by imposing a Gaussian prior on the weights. While this example is primarily for educational purposes and lacks the efficiency, stability, and full correctness of modern Bayesian frameworks, it should help illustrate the main ideas in a more “bare-metal” approach.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "2.4-machine_learning.html#with-diamond-dataset",
    "href": "2.4-machine_learning.html#with-diamond-dataset",
    "title": "Machine Learning",
    "section": "With Diamond Dataset",
    "text": "With Diamond Dataset",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Machine Learning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Engineers in a Hurry (an Illustrated Primer)",
    "section": "",
    "text": "Please use the Github repo for the book to make corrections or contributions: https://github.com/Bearsetc/SEH\nThere are plenty of texts written by academic statisticians that take a deep mathematical perspective on statistics. Having been introduced to statistics that way, I fear many, if not most, will miss the key concepts that form the very structure of statistics.\nThis ‘primer’ is an attempt to create the right intuitions without diving too deeply into the math. An engineer already has many of the technical skills needed to do useful statistical analysis, and the hope is that the right foundation of statistical concepts allows them to view problems when needed from the right statistical and probabilistic perspective."
  },
  {
    "objectID": "3.1-without_priors.html",
    "href": "3.1-without_priors.html",
    "title": "Without Priors",
    "section": "",
    "text": "We finally flip to \\(\\mathcal{L}(M|D)\\) the likelihood of a model given the data. In practice, we typically find \\(\\mathcal{L}(M|D)\\) so that we can find the best parameter values for our chosen model type, although it could also be used to select between model types. We will describe what best is soon.\nLike before, we begin with a simple dice model as we consider it to be an intuitive subject. We then work forward in generally the same order as the first half of the primer. We work with probability distributions and how to find the most likely parameters given the observed data. We pause to examine how likely we are to calculate one set of parameters when the true process has another set of parameters. As usual we use computation to avoid misunderstood analytical solutions.\nThroughout the chapter assume no prior knowledge of what the best model parameters may be - although in an app where you can guess and check you are probably not guessing randomly, which is already a hint at what a prior would be. In the next chapter we’ll formalize how to find optimal models when we have prior information about the parameter values. In more statistical terms, this chapter utilizes a Frequentist perspective and the next a Bayesian perspective.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#preview",
    "href": "3.1-without_priors.html#preview",
    "title": "Without Priors",
    "section": "",
    "text": "We finally flip to \\(\\mathcal{L}(M|D)\\) the likelihood of a model given the data. In practice, we typically find \\(\\mathcal{L}(M|D)\\) so that we can find the best parameter values for our chosen model type, although it could also be used to select between model types. We will describe what best is soon.\nLike before, we begin with a simple dice model as we consider it to be an intuitive subject. We then work forward in generally the same order as the first half of the primer. We work with probability distributions and how to find the most likely parameters given the observed data. We pause to examine how likely we are to calculate one set of parameters when the true process has another set of parameters. As usual we use computation to avoid misunderstood analytical solutions.\nThroughout the chapter assume no prior knowledge of what the best model parameters may be - although in an app where you can guess and check you are probably not guessing randomly, which is already a hint at what a prior would be. In the next chapter we’ll formalize how to find optimal models when we have prior information about the parameter values. In more statistical terms, this chapter utilizes a Frequentist perspective and the next a Bayesian perspective.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#guessing-the-best-model-parameter",
    "href": "3.1-without_priors.html#guessing-the-best-model-parameter",
    "title": "Without Priors",
    "section": "Guessing the Best Model Parameter",
    "text": "Guessing the Best Model Parameter\nIn part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we use the data to find the most likely model of the data generating process, which requires use to calculate \\(P(D|M)\\). Intuitively, this is just a model that generates or predicts data that is similar to our observed data. Subsequently we start with an intuitive excercise before diving further into the technical details.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#likelihood",
    "href": "3.1-without_priors.html#likelihood",
    "title": "Without Priors",
    "section": "Likelihood",
    "text": "Likelihood\nIn the previous dice example, we were just eyeballing the right model parameters to maximize the probability of observing our data under the model. We’d like a more consistent and mathematical way to achieve the same intent, which is called a likelihood function. In generic form, it looks like this:\n\\[\n\\mathcal{L}(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{N} P(y_i \\mid \\theta)\n\\]\n\n\\(\\mathcal{L}(\\theta \\mid \\mathbf{y})\\): The likelihood of the model parameter[s] (\\(\\theta\\)) given the observed data (\\(\\mathbf{y}\\)).\n\\(P(y_i \\mid \\theta)\\): The probability of a data point (\\(y_i\\)) given the model parameter[s] (\\(\\theta\\)). This is the individual probability density for continuous functions or probability for discrete functions.\n\\(\\prod_{i=1}^{N}\\): This is the product operation, which multiplies the individual probabilities \\(P(y_i \\mid \\theta)\\) across all \\(N\\) data points in the dataset. This assumes that the data points are independent. In practice this is typically replaced by the addition of log probabilities, as previously described.\n\nIn practice, calculating \\(P(y_i \\mid \\theta)\\) generally requires making an informed assumption about which named parametric probability distribution best represents your data. We’ve tried to make the point that reality does not consist of only the named probability distributions, however, we use them because they allow for efficient probability calculations, and we can generally choose a named probability distribution that fits the problem as well as practically needed.\nIn the dice app above, the likelihood of the number of dice parameter being a value of 1, 2, or 3 based on the data shown is approximately zero (\\(\\mathcal{L}(\\theta \\in {1,2,3} \\mid \\mathbf{y}) \\approx 0\\)). In the next section we’ll make the calculations more precisely.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#maximum-likelihood-estimation-with-discrete-data",
    "href": "3.1-without_priors.html#maximum-likelihood-estimation-with-discrete-data",
    "title": "Without Priors",
    "section": "Maximum Likelihood Estimation with Discrete Data",
    "text": "Maximum Likelihood Estimation with Discrete Data\nIt is common to consider the best model to be the one that maximizes the likelihood of the observed data. This is called Maximum Likelihood Estimation (MLE). The difficulty in finding the maximum likelihood estimate varies substantially depending on the problem. For simpler problems there is often a derived analytical solution. We will touch on these briefly, but we hold to one of our general themes, which is that we prefer computational solutions that can handle real-world problems.\nWe introduced the binomial distribution in our chapter on discrete probability distributions. There we worked a problem in which we could find the exact probability of the data given a model, P(D|M), where the model was the binomial distribution with a given value of the p parameter. However, we now want to work in the other direction, \\(\\mathcal{L}(M|D)\\), in which we find the value of the p parameter with the maximum likelihood.\nAlthough the app below does not automatically give you the maximum likelihood estimate, it will hopefully make it easy for you to approximate the value. Pick several p values you think are reasonable and for each select ‘Add to plot’.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\nfrom scipy.stats import binom\n\napp_ui = ui.page_fluid(\n    ui.h3(\"In four batches of 1,000 products, we have 992, 995, 989, and 998 pass, what is the most likely pass probability?\"),\n    \n    # Input row\n    ui.row(\n        ui.column(\n            4,\n            ui.input_numeric(\n                \"pInput\", \"Enter probability (p):\",\n                value=0.993, min=0, max=1, step=0.001\n            ),\n        ),\n        ui.column(\n            4,\n            ui.br(),\n            ui.br(),\n            ui.input_action_button(\"addBtn\", \"Add to Plot\"),\n            ui.input_action_button(\"resetBtn\", \"Reset\", class_=\"btn-danger ms-2\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"MLE Estimate of p:\"),\n            ui.output_text(\"mleOutput\"),\n        ),\n    ),\n    \n    ui.br(),\n    \n    # Plots\n    ui.row(\n        ui.column(\n            6,\n            ui.output_plot(\"batchPlot\"),\n        ),\n        ui.column(\n            6,\n            ui.output_plot(\"probPlot\"),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    # Data for the four batches\n    batch_data = [(992, 1000), (995, 1000), (989, 1000), (998, 1000)]  # (successes, trials)\n    \n    # Store the user-selected probabilities and their log-likelihoods\n    stored_probs = reactive.Value([])\n    stored_lls = reactive.Value([])\n    \n    # Calculate MLE estimate\n    total_success = sum(x[0] for x in batch_data)\n    total_trials = sum(x[1] for x in batch_data)\n    mle_p = total_success / total_trials\n    \n    @output\n    @render.text\n    def mleOutput():\n        return f\"p = {mle_p:.6f}\"\n    \n    # Reset function\n    @reactive.Effect\n    @reactive.event(input.resetBtn)\n    def _():\n        stored_probs.set([])\n        stored_lls.set([])\n    \n    # Add new probability when button is clicked\n    @reactive.Effect\n    @reactive.event(input.addBtn)\n    def _():\n        p = input.pInput()\n        if p not in stored_probs.get():\n            probs = list(stored_probs.get())\n            probs.append(p)\n            stored_probs.set(probs)\n            \n            # Calculate cumulative log likelihood for this p\n            total_ll = 0\n            batch_lls = []\n            for success, trials in batch_data:\n                ll = binom.logpmf(success, trials, p)\n                total_ll += ll\n                batch_lls.append(total_ll)\n            \n            lls = list(stored_lls.get())\n            lls.append((p, batch_lls))\n            stored_lls.set(lls)\n    \n    @output\n    @render.plot\n    def batchPlot():\n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n        for p, batch_lls in stored_lls.get():\n            ax.plot(range(1, 5), batch_lls, 'o-', label=f'p={p:.3f}')\n        \n        ax.set_xlabel('Batch Number')\n        ax.set_ylabel('Cumulative Log Probability')\n        ax.set_title('Cumulative Log Probability by Batch')\n        ax.grid(True)\n        \n        # Set x-axis ticks to integers\n        ax.set_xticks(range(1, 5))\n        \n        if stored_lls.get():\n            ax.legend()\n        \n        return fig\n   \n    @output\n    @render.plot\n    def probPlot():\n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n        if stored_lls.get():\n            # Sort the data by probability before plotting\n            sorted_data = sorted(stored_lls.get(), key=lambda x: x[0])\n            probs = [x[0] for x in sorted_data]\n            final_lls = [x[1][-1] for x in sorted_data]\n            \n            ax.plot(probs, final_lls, 'o-')\n            ax.axvline(mle_p, color='red', linestyle='--', \n                      label=f'MLE (p={mle_p:.3f})')\n        \n        ax.set_xlabel('Probability (p)')\n        ax.set_ylabel('Total Log Probability')\n        ax.set_title('Total Log Probability vs. p')\n        ax.grid(True)\n        if stored_lls.get():\n            ax.legend()\n        \n        return fig\n\napp = App(app_ui, server)\nLet’s breakdown how the likelihood was calculated for one of the datapoints. First, we needed to determine a probability distribution that represents our data. Because we have a fixed number of trials and probability of success for each trial, the Binomial probability distribution was a good choice.\n\nReminder of Binomial Probabilities\nNext, let’s remind ourselves how to calculate the probability of an observation that follows the Binomial probability distribution:\n\\[\nP(X = k | n, p) = \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nwhere:\n\n\\(k\\) is the observed number of successes in \\(n\\) trials.\n\\(p\\) is the probability of success in each trial.\n\nOur first data point is 992 successes, which equals k, out of 1,000 trials, which equals n. The best value of p is what we are unsure about, and our current strategy is to strategically try some values to try to understand the problem. Let’s assume we tried p = 0.995 as a reasonable guess. The calculation is:\n\\[\nP(X = 992 | n = 1000, p = 0.995) =\n\\binom{1000}{992} (0.995)^{992} (0.005)^8 = 0.0652\n\\]\nThe result is that the 992 data point has a probability of 0.0652, if we assume n=1000 and p=0.995. That value is about -2.73 using the natural log or -1.19 using log base 10. In the app above, if you select p=0.995 and select ‘Add to Plot’, the value of the first point, which corresponds to k=992, has a value of about -2.73.\n\n\nDifference between \\(\\mathcal{L}(M|D)\\) and P(D|M)\nNow you should be asking yourself - what is the difference between what we are doing here and what we did in the first half when we calculated P(D|M)? And the answer is that up until this step - nothing. So where does the difference come in?\nThe difference is that we use the probability of the data to determine the likelihood of the model in comparison to other possible models. We then optimize the parameters until we find the best model, i.e the one with the Maximum Likelihood Estimate. So in summary, the calculation of the probabilities is the same - the difference is we use an optimization strategy to find the best \\(\\mathcal{L}(M|D)\\).\n\n\nA Subtly Good Strategy\nPlaying around with the app you should be able to see that the chart on the right, the ‘Total Log Probability vs p.’ has an obvious peak. What we want are parameter values that are the most probable, i.e. the ones with the largest total log probability. Here we can fairly easily pick random values, and maybe use some hints from the slope of the graph, to find a good estimate of p. That strategy to find the most probable model is actually not a bad one - a state of the art method is effectively a very sophisticated version of that same strategy.\n\n\nAnalytical Solution\nFor the Binomial distribution there’s actually a simple analytical solution, and while it may seem simple in hindsight, the proper derivation is maybe more complex than you’d expect… The basic idea is that the derivative of the ‘Total Log Probability vs p.’ chart will equal zero at the maximum probability. After a brief drum roll we can reveal that the most likely estimate of p is ……. just the average success rate. More precisely, the successes divided by the total n, e.g. (992 + 995 + 989 + 998)/(1,000 + 1,000 + 1,000 + 1,000).\nAlthough we mostly avoid them, analytical solutions can be very useful. For one they are typically very fast to compute. And secondly, they can be a nice verification that approximate/numerical methods converge to reasonable solutions, for at least some set of circumstances.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#computational-strategies-for-finding-the-maximum-likelihood-estimate",
    "href": "3.1-without_priors.html#computational-strategies-for-finding-the-maximum-likelihood-estimate",
    "title": "Without Priors",
    "section": "Computational Strategies for finding the Maximum Likelihood Estimate",
    "text": "Computational Strategies for finding the Maximum Likelihood Estimate\nAnalytical solutions are not common for real-world problems, so we consider some computational strategies for finding the Maximum Likelihood Estimate.\n\nGrid Search\nGrid search simply divides up the parameter space (i.e. the possible parameter values) into a discrete number of values to check. For example, if we are trying to estimate \\(p\\) in a simple model based on the binomial distribution, it can range from 0 to 1, so we could try the values 0.01, 0.02 … 0.98, and 0.99. Of the 99 likelihood estimates we calculate, we simply pick the p value associated with the largest likelihood.\nThe grid search estimate for the binomial example would be denoted as \\(\\hat{p}_{\\text{grid}}\\), which is the value of \\(p\\) from the grid \\(G\\) that maximizes the likelihood (or log-likelihood):\n\\[\n\\hat{p}_{\\text{grid}} = \\arg\\max_{p_i \\in G} l(p_i \\mid n, k)\n\\]\nThis notation means: “\\(\\hat{p}_{\\text{grid}}\\) is the value of \\(p_i\\), chosen from the set \\(G\\), that maximizes the function \\(l(p_i \\mid n, k)\\).”\nGrid search is attractive due to its simplicity. However, it doesn’t scale well if you need to estimate multiple parameters (high dimensionality) because the plausible number of combinations increases exponentially (also known the curse of dimensionality). For example, if may believe we can reasonably estimate a parameter by picking a minimum and a maximum value, dividing the space between into 100 values to check, and finding the parameter value with the maximum likelihood. With two parameters, this is quite feasible, 100^2, or 10,000. Three parameters is still quite feasible, at 100^3 or 1,000,000 combinations, with modern computers - but we can see where this is going. Any large number of parameters will be intractable.\n\n\nGradient Ascent (or Descent on the Negative Log-Likelihood)\nIn our manual estimation of the Maximum Likelihood with discrete data, we created a chart that had an obvious peak at the Maximum Likelihood Estimate. If we want to find that peak we simply need to ‘walk’ up the hill. If the slope decreases near the top it may be a hint we should slow down to not overshoot the peak. This is the idea of using gradients/derivatives to make our calculation more efficient.\nWe aim to minimize \\(min_{\\theta} f(\\theta)\\) by computing the gradient of \\(f(\\theta)\\) with respect to \\(\\theta\\):\n\\[\n\\nabla f(\\theta) = \\left[ \\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\dots, \\frac{\\partial f}{\\partial \\theta_n} \\right]\n\\]\nThis gradient points in the direction of the steepest ascent. We update our previous estimate by taking a step with a step size with learning rate \\(\\eta\\):\n\\[\n\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla f(\\theta^{(t)})\n\\]\nwhere \\(t\\) is the iteration index.\nThe use of gradients/derivatives forms a core piece of most modern optimization methods. The methods that use these gradients/derivatives are so effective, practitioners will go to great lengths to ensure that the gradients/derivatives can be calculated for their problem. This fact has given birth to many ‘autograd’ libraries that efficiently find derivatives in very complex problem domains. Modern neural networks are possible due to efficient matrix multiplication and gradient/derivative estimation.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#maximum-likelihood-estimate-with-continuous-data",
    "href": "3.1-without_priors.html#maximum-likelihood-estimate-with-continuous-data",
    "title": "Without Priors",
    "section": "Maximum Likelihood Estimate with Continuous Data",
    "text": "Maximum Likelihood Estimate with Continuous Data\nWe move onto continuous data and continuous probability distributions, but with the same goal of finding the model parameters that maximize the likelihood of the observed data. The methods remain the same. The only difference we should keep in mind is that when we calculate \\(P(y_i \\mid \\theta)\\), with a continuous probability distribution, it is a probability density, and not a true probability. However, since probability densities can still be used to compare relative probability, and we are only trying to find the model parameter values that are the most likely relative to all other possibilities, this nuance has little practical impact.\n\nNormal Distribution with MLE Estimate\nThe following is a very similar app to what we saw earlier that just calculated with likelihood of the data. However, this time we also have ability to find the most likely model to explain the data, i.e the Maximimum Likelihood Estimate MLE …\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plots\n    ui.row(\n        ui.column(6, ui.output_plot(\"normalPlot\", height=\"400px\")),\n        ui.column(6, ui.output_plot(\"cumLogProbPlot\", height=\"400px\"))\n    ),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\n    # Plot the cumulative log probability\n    @output\n    @render.plot\n    def cumLogProbPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        # Sort data points for plotting\n        sorted_indices = np.argsort(y)\n        sorted_y = y[sorted_indices]\n        \n        # Calculate log probabilities\n        log_probs = -0.5 * np.log(2 * math.pi * var) - 0.5 * ((sorted_y - mu) / sigma)**2\n        cum_log_probs = np.cumsum(log_probs)\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(sorted_y, cum_log_probs, 'b-', marker='o')\n        ax.set_title(\"Cumulative Log Probability\")\n        ax.set_xlabel(\"Data Points\")\n        ax.set_ylabel(\"Cumulative Log Probability\")\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\n\n\nLog Normal\n\n\nLinear Regression",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#summary",
    "href": "3.1-without_priors.html#summary",
    "title": "Without Priors",
    "section": "Summary",
    "text": "Summary\nTo find the most likely model Finding the most likely model, i.e. using Maximum Likelihood Estimation, to explain",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#segway",
    "href": "3.1-without_priors.html#segway",
    "title": "Without Priors",
    "section": "Segway",
    "text": "Segway\nThe following is a nice segway to the machine learning section, but should be at the very end of the chapter.\nMaximum Likelihood Estimation is not the only method to find optimal model parameters, although it is the standard for statistical models.\n\n\n\n\n\n\nLoss Functions\n\n\n\nLikelihood is a specific form of a loss function. Likelihood is rooted in probability, but a loss function does not need to be. Loss functions in machine learning are what likelihood functions are in statistics. In generic form they look like this:\n\\[\n\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\n\\(\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\\): This represents the overall loss function, which measures the difference between the true values \\(\\mathbf{y}\\) and the predicted values \\(\\hat{\\mathbf{y}}\\). It aggregates the individual losses across all data points.\n\\(\\ell(y_i, \\hat{y}_i)\\): This is the individual loss for a single data point \\(i\\). It can take different forms depending on the type of problem.\n\\(\\frac{1}{N} \\sum_{i=1}^{N}\\): This is the averaging operation, which sums up the individual losses \\(\\ell(y_i, \\hat{y}_i)\\) across all \\(N\\) data points in the dataset and divides by \\(N\\) to compute the average loss. This helps ensure that the loss function is independent of the dataset size.\n\n\n\nWe introduce loss functions here but will apply them next chapter??",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#methods",
    "href": "3.1-without_priors.html#methods",
    "title": "Without Priors",
    "section": "Methods",
    "text": "Methods\nBefore we dive too deep into the details, we are going to take a moment to reflect on what we’re attempting to do and common techniques to do it…\nWhen we were just generating data, it was obvious what variables went into the model, and what data came out of the model. However, we should do a better job of defining that now. Part of the issue here is that many names are used for many applications. We will call inputs predictor/feature variables, and we will call the outputs the outcome/target variables…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "href": "3.1-without_priors.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "title": "Without Priors",
    "section": "Note in 3.2 we will go into the likelihood calculation itself",
    "text": "Note in 3.2 we will go into the likelihood calculation itself",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#traditional-linear-regression",
    "href": "3.1-without_priors.html#traditional-linear-regression",
    "title": "Without Priors",
    "section": "Traditional Linear Regression",
    "text": "Traditional Linear Regression\nSingle-variable linear regression aims to model the relationship between a single independent variable ( x ) and a dependent variable ( y ) using a linear equation:\n[ y = mx + b ]\nwhere: - ( m ) (slope) represents the rate of change of ( y ) with respect to ( x ), - ( b ) (intercept) is the value of ( y ) when ( x = 0 ).",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#least-squares-method",
    "href": "3.1-without_priors.html#least-squares-method",
    "title": "Without Priors",
    "section": "Least Squares Method",
    "text": "Least Squares Method\nThe goal is to minimize the sum of squared residuals:\n[ J(m, b) = _{i=1}^{n} (y_i - (mx_i + b))^2 ]\nwhere ( (x_i, y_i) ) are the given data points.\n\nSolving for ( m ) and ( b )\nTo find ( m ) and ( b ) analytically, we take the partial derivatives of ( J(m, b) ) and set them to zero:\n\nStep 1: Compute the slope ( m )\n[ m = ]\nwhere ( {x} ) and ( {y} ) are the means of ( x ) and ( y ):\n[ {x} = x_i, {y} = y_i ]\n\n\nStep 2: Compute the intercept ( b )\n[ b = {y} - m {x} ]\n\n\n\nFinal Model\nThus, the best-fitting line is:\n[ y = mx + b ]\nwhere ( m ) and ( b ) are computed using the formulas above.\nThis method provides an exact solution without requiring iterative optimization, making it efficient for small datasets.\n\n\nNotes\nTODO: We should maintain approximately the same rate of callouts?",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html",
    "href": "2.2-continuous-probability-distributions.html",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "We continue with \\(P(D|M)\\), the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nLike we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#preview",
    "href": "2.2-continuous-probability-distributions.html#preview",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "We continue with \\(P(D|M)\\), the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nLike we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#models-of-continuous-data-generating-processes",
    "href": "2.2-continuous-probability-distributions.html#models-of-continuous-data-generating-processes",
    "title": "Continuous Probability Distributions",
    "section": "Models of Continuous Data Generating Processes",
    "text": "Models of Continuous Data Generating Processes\nLike we said before - when we change the data generating process, the distribution of outcomes changes. Here we’ll examine data generating processes that create continuous data.\n\nThe Random Walk Rocket\nLet’s assume we are shooting a rocket into the sky and letting it land. We have designed a simple guidance system that will correct the rocket to fly vertically after deviating from vertical flight. However, before a correction, the rocket will have wandered slightly from its original launch point. For simplicity in modeling, we’ll assume it only wanders left and right. We also assume that the deviations in flight are totally random - i.e. there is not a tendency to always move in one of the two directions. We want to answer the question, how far away is our rocket likely to land?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Random Rocket Simulator\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"stepSize\", \"Deviation Size\", \n                           min=0.01, max=10, value=1, step=0.01),\n            ui.input_slider(\"numSteps\", \"Number of Deviations\", \n                           min=10, max=1000, value=100, step=10),\n            ui.input_slider(\"numTrials\", \"Number of Trials\", \n                           min=10, max=10000, value=100, step=10),\n            ui.input_slider(\"numBins\", \"Number of Histogram Bins\", \n                           min=5, max=100, value=30, step=5),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Histogram\",\n                ui.output_plot(\"distPlot\", height=\"400px\"),\n            ),\n            ui.nav_panel(\"Percentile Plot\",\n                ui.output_plot(\"percentilePlot\", height=\"400px\"),\n            ),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def calculate_distances():\n        distances = []\n        for _ in range(input.numTrials()):\n            steps = np.random.uniform(0, 1, input.numSteps()) * input.stepSize()\n            final_position = np.sum(steps)\n            distances.append(final_position)\n        return distances\n\n    @output\n    @render.plot\n    def distPlot():\n        distances = calculate_distances()\n        fig, ax = plt.subplots()\n        \n        ax.hist(distances, bins=input.numBins(), color=\"steelblue\", edgecolor=\"black\")\n        ax.set_title(\"Distribution of Distances from Launch\")\n        ax.set_xlabel(\"Distance from Start\")\n        ax.set_ylabel(\"Frequency\")\n\n        return fig\n\n    @output\n    @render.plot\n    def percentilePlot():\n        distances = calculate_distances()\n        fig, ax = plt.subplots()\n        \n        # Calculate percentiles\n        sorted_distances = np.sort(distances)\n        percentiles = np.linspace(0, 100, len(distances))\n        \n        ax.plot(sorted_distances, percentiles, color=\"steelblue\")\n        ax.set_title(\"Percentile Plot of Distances\")\n        ax.set_xlabel(\"Distance from Start\")\n        ax.set_ylabel(\"Percentile\")\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\nEven though this is similar to some other outputs we’ve seen, we need to stress a few key points:\n\nThe output values are no longer discrete/integers as we saw previously.\nThe app now lets choose the size of the bin since there is no ‘right’ answer to the interval used.\nThere is now a percentile plot option for the type of graph generated.\nThe percentile plot shows, in a cumulative fashion, what percent of distances are closer to the start.\n\n\n\n\n\n\n\nNote\n\n\n\nThe percentile plot is a foreshadowing of another way to compute continuous probability distributions, called the cumulative distribution function. This approach can remove some confusing properties of continuous probability distributions, however, it tends to make the shape of the distribution harder to interpret as many distributions tend to look similar.\n\n\n\n\nContinuous Probability Distributions as Models of Data Generating Processes\nThe real world is full of data generating processes, each of which has a probability distribution. We do not expect, however, that many would match a named parametric probability distribution from statistics. That doesn’t mean the named parametric probability distributions aren’t useful - they are extremely useful approximations due to their ability to model/approximate common processes and calculate \\(P(D|M)\\) easily.\n\nThe Normal Distribution\nYou may have spotted a trend in the Random Rocket example, as well as a lot of our earlier examples. Whenever we increased the number of samples, the histograms started to look an awful lot like the well-known normal/gaussian distribution. There’s actually a theorem for that, called the the Central Limit Theorem. You can easily research the details, but we’ll summarize it by saying that any data generating process that is additive in nature tends to produce normal/gaussian distributions. And almost everything we’ve seen so far has utilized additive processes.\nTo restate slightly from our perspective of models of data generating processes, the Normal distribution takes additive processes to the limit, in which they generate the perfect ‘Bell Curve’. Normal distributions are a good approximation for modeling the variability/variation in many things. An example is modeling height - but beware it is the logical extreme - real heights cannot be perfectly Normal, because they cannot have negative values.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import norm\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"mean\", \n                \"Mean (μ)\", \n                min=-100.0, \n                max=100.0, \n                value=0.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"stddev\", \n                \"Standard Deviation (σ)\", \n                min=0.1, \n                max=50.0, \n                value=1.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Number of Trials\",\n                min=100,\n                max=10000,\n                value=1000,\n                step=100\n            ),\n            ui.input_slider(\n                \"num_bins\",\n                \"Number of Bins\",\n                min=10,\n                max=100,\n                value=30,\n                step=5\n            ),\n        ),\n        ui.output_plot(\"normPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def normal_samples():\n        mu = input.mean()\n        sigma = input.stddev()\n        size = input.num_trials()\n        return np.random.normal(mu, sigma, size)\n\n    @output\n    @render.plot\n    def normPlot():\n        samples = normal_samples()\n        \n        # Use the number of bins from the slider\n        num_bins = input.num_bins()\n        \n        # Compute histogram (both count and density)\n        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)\n        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)\n        bin_width = bin_edges[1] - bin_edges[0]\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Create the second y-axis sharing the same x-axis\n        ax2 = ax1.twinx()\n        \n        # Plot histogram with counts on left y-axis\n        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color=\"steelblue\", \n                      alpha=0.6, edgecolor=\"black\", align='center', \n                      label=f'Histogram (n={input.num_trials():,})')\n        \n        # Calculate theoretical normal distribution\n        x = np.linspace(min(samples), max(samples), 100)\n        pdf = norm.pdf(x, input.mean(), input.stddev())\n        \n        # Plot theoretical curve on right y-axis\n        line = ax2.plot(x, pdf, 'r-', lw=2, label='Normal PDF')[0]\n        \n        # Set labels and title\n        ax1.set_xlabel(\"Value\", fontsize=14)\n        ax1.set_ylabel(\"Count\", fontsize=12, color='steelblue')\n        ax2.set_ylabel(\"Density\", fontsize=12, color='red')\n        plt.title(\"Normal Distribution: Histogram and PDF\", fontsize=16)\n        \n        # Color the tick labels to match the respective plots\n        ax1.tick_params(axis='y', labelcolor='steelblue')\n        ax2.tick_params(axis='y', labelcolor='red')\n        \n        # Ensure both axes start at 0\n        ax1.set_ylim(bottom=0)\n        ax2.set_ylim(bottom=0)\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers], rotation=90)\n        \n        # Add legends for both axes\n        lines = [bars, line]\n        labels = [b.get_label() for b in lines]\n        ax1.legend(lines, labels, loc='upper left')\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\nWe’ve also included the exact values of the probability density function (PDF) to suggest that the underlying distribution is continuous.\n\n\nThe Lognormal Distribution\nData generating processes do not need to be additive though, some processes tend to multiply. These kinds of processes will create a notably different distribution, called the log-normal distribution. It has two important differences from the normal distribution:\n\nIt contains only positive values.\nIt has a very long ‘tail’ on the right hand side. Another way to describe this is skewness.\n\nIt’s worth noting that there are many real world problems where values can only be positive. This simple fact also implies that many real distributions skew towards larger values.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import lognorm\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Log-Normal Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"mu\", \n                \"Log-mean (μ)\", \n                min=-2.0, \n                max=2.0, \n                value=0.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"sigma\", \n                \"Log-standard deviation (σ)\", \n                min=0.1, \n                max=2.0, \n                value=0.5, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Number of Trials\",\n                min=100,\n                max=10000,\n                value=1000,\n                step=100\n            ),\n            ui.input_slider(\n                \"num_bins\",\n                \"Number of Bins\",\n                min=10,\n                max=100,\n                value=30,\n                step=5\n            ),\n        ),\n        ui.output_plot(\"lognormPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def lognormal_samples():\n        mu = input.mu()\n        sigma = input.sigma()\n        size = input.num_trials()\n        return np.random.lognormal(mu, sigma, size)\n\n    @output\n    @render.plot\n    def lognormPlot():\n        samples = lognormal_samples()\n        \n        # Use the number of bins from the slider\n        num_bins = input.num_bins()\n        \n        # Compute histogram (both count and density)\n        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)\n        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)\n        bin_width = bin_edges[1] - bin_edges[0]\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Create the second y-axis sharing the same x-axis\n        ax2 = ax1.twinx()\n        \n        # Plot histogram with counts on left y-axis\n        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color=\"steelblue\", \n                      alpha=0.6, edgecolor=\"black\", align='center', \n                      label=f'Histogram (n={input.num_trials():,})')\n        \n        # Calculate theoretical log-normal distribution\n        x = np.linspace(min(samples), max(samples), 1000)\n        pdf = lognorm.pdf(x, input.sigma(), scale=np.exp(input.mu()))\n        \n        # Plot theoretical curve on right y-axis\n        line = ax2.plot(x, pdf, 'r-', lw=2, label='Log-Normal PDF')[0]\n        \n        # Set labels and title\n        ax1.set_xlabel(\"Value\", fontsize=14)\n        ax1.set_ylabel(\"Count\", fontsize=12, color='steelblue')\n        ax2.set_ylabel(\"Density\", fontsize=12, color='red')\n        plt.title(\"Log-Normal Distribution: Histogram and PDF\", fontsize=16)\n        \n        # Color the tick labels to match the respective plots\n        ax1.tick_params(axis='y', labelcolor='steelblue')\n        ax2.tick_params(axis='y', labelcolor='red')\n        \n        # Ensure both axes start at 0\n        ax1.set_ylim(bottom=0)\n        ax2.set_ylim(bottom=0)\n        \n        # Set x-axis limits to focus on the main part of the distribution\n        upper_limit = np.percentile(samples, 99)  # Show up to 99th percentile\n        ax1.set_xlim(0, upper_limit)\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers], rotation=90)\n        \n        # Add legends for both axes\n        lines = [bars, line]\n        labels = [b.get_label() for b in lines]\n        ax1.legend(lines, labels, loc='upper right')\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\nYou may have guessed that if you have a set of values from the lognormal distribution, if you take the the log of their values and replot them, you’ll end up plotting values in a normal distribution.\n\n\nSummary\nAgain we keep this section brief as there are plenty of easily accessible references for continuous probability distributions. Hopefully the point was made though - that each continuous probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcome of the process.\n\n\n\n\n\n\nNote\n\n\n\nWe’ve touched on this briefly, but we should explicitly acknowledge that the named probability distributions can be used in different ways:\n\nGenerate data from a known distribution.\nCalculate the probability of data that has come from a known distribution.\nCommuniciate uncertainty in the value of a parameter.\n\nWe haven’t seen the third bullet yet, it will eventually come up in the second half of the primer. But it is fundamentally different than the other two in that it represents knowledge, and with additional knowledge the uncertainty can become zero. This is not the same as the inherent variation in statistical processes, for example, the variation in peoples height will not become zero with more knowledge.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#probability-of-data",
    "href": "2.2-continuous-probability-distributions.html#probability-of-data",
    "title": "Continuous Probability Distributions",
    "section": "Probability of Data",
    "text": "Probability of Data\nWe return to the main theme of the first half of the primer, the probability of data given a model, \\(P(D|M)\\).\n\nProbability Density Function\nWe’ve been hinting that the probability density function would require some explanation, and we’ve finally come to the right place to tackle it. We’ve created histograms of our continuous data, in which we take multiple exact values and lump them together in a bin of the histogram. It’s possible to use our relative frequency technique to estimate the probability of the bin (just divide the bin count by the total count of all bins). However, how would estimate the probability of a single point within the bin? The problem is that, as we’ve noted earlier, the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nTo solve this, the probability density function does not give a true probability, it gives a value such that the following properties are true:\n\nThe area under the curve sums to 1, i.e. all possible values have a total probability of 1.\nThe values give the relative probability of that point vs other points.\n\n\n\nSingle Data Point Example\nAs already mentioned, referring to the probability of an exact value on the real number line is meaningless. Instead, we refer to either the relative probability of a value, or the probability of getting a value as extreme or more extreme than the value that we observed. With a single data point we use an example of the ‘as extreme or more extreme’ approach.\nIn the app below, we find where the data point lies on the chart and then find the area under all larger values. The area is equal to the probability of sampling points larger than the observation. This is utilizing bullet one from the properties of the probability density function given above. Note that we can get this value more directly if we work from the cumulative distribution, as shown in the other plot tab.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Probability Calculator\"),\n    \n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n            ui.input_numeric(\n                \"xInput\", \"Value (x):\",\n                value=105, min=0, max=200\n            ),\n            ui.br(),\n            ui.h4(\"Probability (P(X ≥ x)):\"),\n            ui.output_text(\"probOutput\"),\n            width=300\n        ),\n        \n        # Main panel with tabset\n        ui.navset_tab(\n            ui.nav_panel(\"PDF Plot\",\n                ui.output_plot(\"normalPlot\", height=\"400px\"),\n            ),\n            ui.nav_panel(\"CDF Plot\",\n                ui.output_plot(\"cdfPlot\", height=\"400px\"),\n            ),\n        ),\n    )\n)\n\ndef server(input, output, session):\n    # Calculate probability\n    @reactive.Calc\n    def calculate_probability():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n        return 1 - stats.norm.cdf(x, mu, sigma)\n\n    # Show the probability\n    @output\n    @render.text\n    def probOutput():\n        prob = calculate_probability()\n        return f\"{prob:.4f}\"\n\n    # Plot the normal PDF with shaded area\n    @output\n    @render.plot\n    def normalPlot():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n\n        x_min = mu - 4 * sigma\n        x_max = mu + 4 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')\n        \n        x_shade = x_vals[x_vals &gt;= x]\n        y_shade = stats.norm.pdf(x_shade, mu, sigma)\n        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, \n                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')\n\n        ax.axvline(x, color='red', linestyle='--', alpha=0.5)\n\n        ax.set_title(f\"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        return fig\n\n    # Plot the CDF\n    @output\n    @render.plot\n    def cdfPlot():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n\n        # Generate random data\n        data = np.random.normal(mu, sigma, 1000)\n        \n        # Calculate empirical CDF\n        sorted_data = np.sort(data)\n        empirical_cdf = np.arange(1, len(data) + 1) / len(data)\n        \n        # Calculate theoretical CDF\n        x_vals = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)\n        theoretical_cdf = stats.norm.cdf(x_vals, mu, sigma)\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot empirical and theoretical CDFs\n        ax.plot(sorted_data, empirical_cdf, 'b-', label='Empirical CDF', alpha=0.7)\n        ax.plot(x_vals, theoretical_cdf, 'r--', label='Theoretical CDF')\n        \n        # Add vertical line at x\n        ax.axvline(x, color='red', linestyle='--', alpha=0.5)\n        \n        ax.set_title(f\"Cumulative Distribution Function (μ={mu:.1f}, σ²={var:.1f})\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Cumulative Probability\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        return fig\n\napp = App(app_ui, server)\nWhen we refer to probabilities of “as extreme or more extreme” values, we are effectively computing tail probabilities using integrals. For example, for a right-tailed probability beyond a point \\(c\\), we evaluate:\n\\[\nP(X \\geq c) = \\int_c^\\infty f(x) \\, dx\n\\]\nThose who prefer to avoid calculus (and what engineer doesn’t?), can simply utilize the cumulative (CDF) plot.\n\n\nMultiple Data Points Example\n\nRelative Probability\nFor multiple data points, we are interested in the relative probability of one series of events vs some other series of events. This is utilizing bullet two from the properties of the probability density function given above. In the example below we will multiply the probability densities similar to how we multiplied actual probabilities in the previous chapter on discrete probability distributions - this should given you reason to pause - the reason we can do this is that we will only use the results for relative comparison, and not as an absolute probability.\nAgain, we assume that in the series of events each event is independent. And again, this typically cannot be proven to be strictly true - however avoid the cases where it is obviously not true, such as time series. Also, we use only log probabilities and addition (in contrast with the more obvious multiplication of non-log probabilities) as they are more convenient.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Relative Probability of Normally Distributed Data\"),\n\n    # Row 1: Sliders and New Sample button\n    ui.row(\n        ui.column(\n            5,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            5,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            2,\n            ui.br(),\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: All data and probabilities\n    ui.row(\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            5,\n            ui.h4(\"Log Relative Probabilities:\"),\n            ui.output_text_verbatim(\"pointLogProbs\"),\n        ),\n        ui.column(\n            3,\n            ui.h4(\"Log Sum:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 5 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=5)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=5)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # Calculate log probability for each point\n    @reactive.Calc\n    def point_log_probs():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        if var &lt;= 0:\n            return [float(\"nan\")] * len(y)\n        \n        log_probs = []\n        for yi in y:\n            term1 = -0.5 * math.log(2 * math.pi * var)\n            term2 = -0.5 * ((yi - mu)**2) / var\n            log_probs.append(term1 + term2)\n        return log_probs\n\n    # Display individual log probabilities\n    @output\n    @render.text\n    def pointLogProbs():\n        probs = point_log_probs()\n        return \", \".join(f\"{p:.2f}\" for p in probs)\n\n    # Reactive expression for total log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        return sum(point_log_probs())\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nAs you adjust the parameters of the probability distribution, the relative probability of seeing the data changes. When the probability distribution is far away from the points, we see the sum of the probabilities becomes smaller (less likely). When the probability distribution is near the points, the sum gets larger, showing it is comparitively more likely.\n\n\n\n\n\n\nWarning\n\n\n\nI’ve reserved the use of likelihood until the second half of this primer during which we’ll find the best model based on the data from the data generating process. However, in other texts you will also find descriptions of the relative probability of continuous distributions called the relative likelihood.\n\n\n\n\nApproximate P-Value\nIn the following app, there will be 100 samples of a series of 10 events. Each of those will have been generated from a Normal distribution with a mean of 100 and a variance of 10. You can then create your own series of 10 events, sampled from a Normal distribution with a mean and variance of your choosing. The relative probability of your series of 10 events will be calculated as if they had come from a distribution with a mean of 100 and a variance of 10. The percentile shown will indicate how unusual your series of events appears to be.\nLike the similar app for discrete distributions, this is an approximation of your data sets p-value. If your dataset is in a reasonable percentile, you have little evidence to assume it was not created by the null model with mean of 100 and variance of 10. If it is at an extreme percentile, you may reasonably suspect it was generated by a different data generating process.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Approximate P-Value Calculator\"),\n    ui.p(\"Generate your own series of 10 events from a Normal distribution with your chosen mean and variance. The probability will be calculated assuming the events came from a Normal(μ=100, σ²=10) distribution.\"),\n\n    # Row 1: Sliders and New Sample button\n    ui.row(\n        ui.column(\n            5,\n            ui.input_slider(\n                \"muInput\", \"Your Mean (μ):\",\n                min=80, max=120, value=95, step=0.1\n            ),\n        ),\n        ui.column(\n            5,\n            ui.input_slider(\n                \"varInput\", \"Your Variance (σ²):\",\n                min=1, max=100, value=20, step=1\n            ),\n        ),\n        ui.column(\n            2,\n            ui.br(),\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLES\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Data and percentile\n    ui.row(\n        ui.column(\n            9,\n            ui.h4(\"Your Data:\"),\n            ui.output_text_verbatim(\"dataText1\"),\n        ),\n        ui.column(\n            3,\n            ui.h4(\"Percentile:\"),\n            ui.output_text(\"percentileOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"cumulativePlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize user's data and reference datasets\n    data_vals1 = reactive.Value(None)\n    reference_data = reactive.Value(None)\n\n    # Generate new samples when parameters change or button is pressed\n    @reactive.Effect\n    @reactive.event(input.muInput, input.varInput, input.newSampleBtn)\n    def _():\n        data_vals1.set(\n            np.random.normal(loc=input.muInput(), scale=np.sqrt(input.varInput()), size=10)\n        )\n        reference_data.set(\n            [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]\n        )\n\n    # Initial data generation\n    @reactive.Effect\n    def _():\n        if data_vals1() is None:\n            data_vals1.set(\n                np.random.normal(loc=95, scale=np.sqrt(20), size=10)\n            )\n        if reference_data() is None:\n            reference_data.set(\n                [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]\n            )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText1():\n        y = data_vals1()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # Calculate cumulative log probabilities\n    def calc_cum_log_probs(data, mu=100, var=10):  # Default parameters set to true distribution\n        cum_probs = []\n        running_sum = 0\n        \n        for yi in data:\n            term1 = -0.5 * math.log(2 * math.pi * var)\n            term2 = -0.5 * ((yi - mu)**2) / var\n            running_sum += (term1 + term2)\n            cum_probs.append(running_sum)\n            \n        return cum_probs\n\n    # Calculate and show the percentile\n    @output\n    @render.text\n    def percentileOutput():\n        user_final_prob = calc_cum_log_probs(data_vals1())[-1]\n        ref_final_probs = [calc_cum_log_probs(ref_data)[-1] \n                          for ref_data in reference_data()]\n        percentile = sum(1 for x in ref_final_probs if x &lt; user_final_prob) / len(ref_final_probs) * 100\n        return f\"{percentile:.1f}%\"\n\n    # Plot the cumulative log probabilities\n    @output\n    @render.plot\n    def cumulativePlot():\n        # Calculate probabilities for user's data\n        user_probs = calc_cum_log_probs(data_vals1())\n        \n        # Calculate probabilities for reference data\n        ref_probs_list = [calc_cum_log_probs(data) \n                         for data in reference_data()]\n\n        events = range(1, 11)  # 10 events\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot reference lines in gray\n        for ref_probs in ref_probs_list:\n            ax.plot(events, ref_probs, 'gray', alpha=0.1, linewidth=1)\n\n        # Plot user's line in red\n        ax.plot(events, user_probs, 'r-', label='Your samples', \n                linewidth=2, alpha=1.0)\n\n        # Add a dummy plot for the reference distribution legend\n        ax.plot([], [], 'gray', alpha=0.5, label='100 samples from N(100,10)')\n\n        ax.set_title(\"Cumulative Log Probability vs. Number of Events\")\n        ax.set_xlabel(\"Number of Events\")\n        ax.set_ylabel(\"Cumulative Log Probability\")\n        ax.grid(True, alpha=0.3)\n        ax.legend()\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#a-mini-appendix-for-a-more-formal-definition-of-probability-distributions",
    "href": "2.2-continuous-probability-distributions.html#a-mini-appendix-for-a-more-formal-definition-of-probability-distributions",
    "title": "Continuous Probability Distributions",
    "section": "A Mini Appendix for a more Formal Definition of Probability Distributions",
    "text": "A Mini Appendix for a more Formal Definition of Probability Distributions\nWe have, and will, use intuitive language like ‘data generating process’ that creates a ‘relative distribution of outcomes’. While these phrases are ‘real’ in the statistical lexicon, for the sake of giving you an interface to more formal statistical language, we give a more ‘standard’ development of the probability distribution.\n\nSample Space\nThe sample space, often denoted as \\(\\Omega\\), is the complete set of all possible outcomes of a data generating process. Think of it as the “limits” of what the process can produce. For example:\n\nFor a standard six-sided die, the sample space is \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\), since those are the only outcomes that can occur.\nFor a process that only produces positive numbers, the sample space might be something like \\(\\Omega = \\{x \\in \\mathbb{R} \\mid x &gt; 0\\}\\), which excludes any negative outcomes.\n\n\n\nRandom Variable\nA random variable is a function that assigns a numerical value to each outcome in the sample space. We usually denote a random variable by a capital letter (e.g., \\(X\\)). It “translates” or maps the abstract outcomes into numerical data that we can analyze.\nFor example, consider a six-sided die:\n\nOne random variable might simply be the value on the face: \\(X(\\omega) = \\omega, \\quad \\text{for } \\omega \\in \\{1, 2, 3, 4, 5, 6\\}.\\)\nAlternatively, you may be interested in the number of rolls needed until a six appears. In this case, you would define another random variable, say \\(Y\\), which maps the sequence of rolls to a count (e.g., \\(Y = 3\\) if it takes three rolls to get a six).\n\nEven though these random variables come from the same underlying process (rolling a die), they capture different information about the process.\n\n\nProbability Distribution\nThe probability distribution describes how the probabilities are assigned to the values that a random variable can take. It answers the question: “How likely is it that the random variable equals a certain value?” This is typically expressed as \\(P(X = x)\\).\nFor a fair six-sided die (where \\(X\\) represents the outcome of a single roll), the probability distribution is given by: \\[\nP(X = x) = \\frac{1}{6} \\quad \\text{for } x \\in \\{1, 2, 3, 4, 5, 6\\}.\n\\]\nThis distribution tells you that each face (or outcome) is equally likely. In general, the probability distribution provides a complete picture of how the process behaves by assigning probabilities to every possible outcome mapped by the random variable.\nIn summary, think of the sample space as the set of all possible outcomes, the random variable as the rule that translates these outcomes into meaningful numbers, and the probability distribution as the description of how likely each of those numbers is to occur.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#welcome",
    "href": "1-intro.html#welcome",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#models-and-data",
    "href": "1-intro.html#models-and-data",
    "title": "Introduction",
    "section": "Models and Data",
    "text": "Models and Data\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#reading-order",
    "href": "1-intro.html#reading-order",
    "title": "Introduction",
    "section": "Reading Order",
    "text": "Reading Order\nThe primer can obviously be read by just plowing down the table of contents on the left side of the page. This should be the most straightforward but possibly not the most interesting way to proceed. An alternative I recommend you consider is jumping through the first chapter of each major section, then the second chapter of each major section, and so on. This gets you into the material quicker…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#last-notes",
    "href": "1-intro.html#last-notes",
    "title": "Introduction",
    "section": "Last Notes",
    "text": "Last Notes\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#glossary",
    "href": "1-intro.html#glossary",
    "title": "Introduction",
    "section": "Glossary",
    "text": "Glossary\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  }
]