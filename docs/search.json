[
  {
    "objectID": "3.4-Naive_Monte_Carlo.html",
    "href": "3.4-Naive_Monte_Carlo.html",
    "title": "Naive Monte Carlo",
    "section": "",
    "text": "We use the term naive to distinguish this method from more advanced Monte Carlo methods. The advantage here will be simplicity, but it will not be a good solution with many parameters (due to the spareseness of high dimensional space), or where we do not have good priors.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re wondering why generate data for examples instead of using ‘real’ data sets, it’s because it’s the only way to have certainty in the data generating process. This allows us to see how well we fit the model. Obviously the goal is real data - but unfortunately you’ll probably never know exactly what the data generating process was, and that makes understanding the accuracy of ‘new’ methods very difficult.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.4-Naive_Monte_Carlo.html#intro",
    "href": "3.4-Naive_Monte_Carlo.html#intro",
    "title": "Naive Monte Carlo",
    "section": "",
    "text": "We use the term naive to distinguish this method from more advanced Monte Carlo methods. The advantage here will be simplicity, but it will not be a good solution with many parameters (due to the spareseness of high dimensional space), or where we do not have good priors.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re wondering why generate data for examples instead of using ‘real’ data sets, it’s because it’s the only way to have certainty in the data generating process. This allows us to see how well we fit the model. Obviously the goal is real data - but unfortunately you’ll probably never know exactly what the data generating process was, and that makes understanding the accuracy of ‘new’ methods very difficult.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.4-Naive_Monte_Carlo.html#bayesian-methods",
    "href": "3.4-Naive_Monte_Carlo.html#bayesian-methods",
    "title": "Naive Monte Carlo",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\nFor determining the \\(P(M|D)\\), we will use Bayesian methods. There’s a whole historical debate you could research, but when it comes to most scientific and engineering subjects, you wouldn’t be studying the subject if you didn’t have some prior information, even if you can only describe it as intuition. This prior information is valuable, and it should be included in the model. Priors are also valueable in more bespoke models due to their ability to limit solutions to those known to be plausible…\nAllen Downey said it well when he stated in Think Bayes that if don’t have much data you should use Bayesian methods, and when you have lots of data, it dominates the priors so you can still use Bayesian methods… (research the real quote).",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.4-Naive_Monte_Carlo.html#dataset",
    "href": "3.4-Naive_Monte_Carlo.html#dataset",
    "title": "Naive Monte Carlo",
    "section": "Dataset",
    "text": "Dataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# =========================\n# Step 1: Generate Data\n# =========================\n\n# True parameters\nmu_true = 5.5  # average height in feet\nsigma_true = 0.3  # standard deviation in feet\n\n# Generate dataset of 100 persons\ndataset_size = 100\nheights = np.random.normal(loc=mu_true, scale=sigma_true, size=dataset_size)\n\n# Plot the generated heights\nplt.figure(figsize=(10, 6))\nsns.histplot(heights, bins=15, kde=True, color='skyblue')\nplt.title('Histogram of Generated Heights (100 Persons)')\nplt.xlabel('Height (feet)')\nplt.ylabel('Frequency')\nplt.show()",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.4-Naive_Monte_Carlo.html#priors",
    "href": "3.4-Naive_Monte_Carlo.html#priors",
    "title": "Naive Monte Carlo",
    "section": "Priors",
    "text": "Priors\n\n# =========================\n# Step 2: Define Priors (Corrected)\n# =========================\n\n# Define prior ranges\nmu_prior_min = 4.0  # feet\nmu_prior_max = 7.0  # feet\nsigma_prior_min = 0.0\nsigma_prior_max = 3 * sigma_true  # 0.9 feet\n\n# Plot the prior distributions\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Prior for mu\nmu_values = np.linspace(mu_prior_min, mu_prior_max, 1000)\nmu_prior = np.ones_like(mu_values) / (mu_prior_max - mu_prior_min)\nax[0].plot(mu_values, mu_prior, color='blue')\nax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\nax[0].set_xlabel(r'$\\mu$ (feet)')\nax[0].set_ylabel('Probability Density')\n\n# Prior for sigma\nsigma_values = np.linspace(sigma_prior_min, sigma_prior_max, 1000)\nsigma_prior_dist = np.ones_like(sigma_values) / (sigma_prior_max - sigma_prior_min)\nax[1].plot(sigma_values, sigma_prior_dist, color='green')\nax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\nax[1].set_xlabel(r'$\\sigma$ (feet)')\nax[1].set_ylabel('Probability Density')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 3: Sample from Priors\n# =========================\n\n# Number of samples\nnum_prior_samples = 10000\n\n# Sample mu and sigma from their priors\nmu_samples = np.random.uniform(mu_prior_min, mu_prior_max, num_prior_samples)\nsigma_samples = np.random.uniform(sigma_prior_min, sigma_prior_max, num_prior_samples)\n\n# To avoid sigma=0, set a minimum sigma\nsigma_samples[sigma_samples == 0] = 1e-6\n\n# Generate heights based on sampled mu and sigma\nheights_prior = np.random.normal(loc=mu_samples, scale=sigma_samples)\n\n# Plot the prior-generated heights\nplt.figure(figsize=(10, 6))\nsns.histplot(heights_prior, bins=50, kde=True, color='orange')\nplt.title('Histogram of Heights Generated from Priors (10,000 Persons)')\nplt.xlabel('Height (feet)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 4: Naive Monte Carlo (Corrected)\n# =========================\n\nimport pandas as pd\n\n# Precompute constants\nn = len(heights)\ndata = heights\n\n# Vectorized computation of log-likelihoods\n# There's a hell of a lot happening in this one line. For each of the 10,000 random samples\n# from the prior, it is using each as the loc and scale of a norm dist to find the \n# likelihood of getting the height data. Then summing the 100 to get a 10,000 lenth array\nlog_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n\n# To prevent numerical underflow, we'll work with log-likelihoods\n# Normalize log-likelihoods by subtracting the max\nmax_log_likelihood = np.max(log_likelihoods)\nnormalized_log_likelihood = log_likelihoods - max_log_likelihood\nlikelihoods = np.exp(normalized_log_likelihood)\n\n# Compute posterior probabilities (unnormalized)\n# Due to flat priors posterior is just equivalent to the likelihoods\nposterior = likelihoods\n\n# Normalize the posterior (so it sums to 1)\nposterior /= np.sum(posterior)\n\n# Verify that 'posterior' is one-dimensional\nprint(f\"Shape of posterior: {posterior.shape}\")\n\n# Create a DataFrame for easier handling\ndf = pd.DataFrame({\n    'mu': mu_samples,\n    'sigma': sigma_samples,\n    'posterior': posterior\n})\n\n# Plot the posterior distributions for mu and sigma using Seaborn\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot posterior for mu\nsns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\nax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\nax[0].set_title(r'Posterior Distribution for $\\mu$')\nax[0].set_xlabel(r'$\\mu$ (feet)')\nax[0].set_ylabel('Posterior Probability')\nax[0].legend()\n\n# Plot posterior for sigma\nsns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\nax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\nax[1].set_title(r'Posterior Distribution for $\\sigma$')\nax[1].set_xlabel(r'$\\sigma$ (feet)')\nax[1].set_ylabel('Posterior Probability')\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nShape of posterior: (10000,)\n\n\n\n\n\n\n\n\n\n\n# =========================\n# Step 5: Visualize Parameter Likelihood (Log-Scaled Color)\n# =========================\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a small epsilon to avoid log(0)\nepsilon = 1e-10\n\n# Compute the logarithm of posterior probabilities\nlog_posterior = np.log(posterior + epsilon)\n\n# Create the scatter plot\nplt.figure(figsize=(10, 8))\n\n# Scatter plot using log_posterior for color scaling\nscatter = plt.scatter(\n    mu_samples,\n    sigma_samples,\n    c=log_posterior,          # Use log-transformed posterior\n    cmap='viridis',\n    alpha=0.5,\n    s=10\n)\n\n# Add a colorbar with appropriate labeling\ncbar = plt.colorbar(scatter)\ncbar.set_label('Log Posterior Probability')\n\n# Add reference lines for true parameter values using raw strings\nplt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\nplt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n\n# Set plot titles and labels using raw strings\nplt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\nplt.xlabel(r'$\\mu$ (feet)')\nplt.ylabel(r'$\\sigma$ (feet)')\n\n# Add legend\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nShiny Live!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Inference of Heights\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.h4(\"True Parameters\"),\n            ui.input_numeric(\n                \"mu_true\", \n                \"True Mean Height (μ_true) [feet]\", \n                value=5.5, \n                min=4.0, \n                max=7.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_true\", \n                \"True Std Dev (σ_true) [feet]\", \n                value=0.3, \n                min=0.1, \n                max=1.0, \n                step=0.05\n            ),\n            ui.hr(),\n            ui.h4(\"Dataset Generation\"),\n            ui.input_numeric(\n                \"dataset_size\", \n                \"Number of Persons\", \n                value=100, \n                min=10, \n                max=1000, \n                step=10\n            ),\n            ui.hr(),\n            ui.h4(\"Prior Ranges\"),\n            ui.input_numeric(\n                \"mu_prior_min\", \n                \"μ Prior Minimum [feet]\", \n                value=4.0, \n                min=0.0, \n                max=6.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"mu_prior_max\", \n                \"μ Prior Maximum [feet]\", \n                value=7.0, \n                min=5.0, \n                max=10.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_prior_min\", \n                \"σ Prior Minimum [feet]\", \n                value=0.0, \n                min=0.0, \n                max=1.0, \n                step=0.05\n            ),\n            ui.input_numeric(\n                \"sigma_prior_max\", \n                \"σ Prior Maximum [feet]\", \n                value=0.9, \n                min=0.5, \n                max=3.0, \n                step=0.1\n            ),\n            ui.hr(),\n            ui.h4(\"Sampling\"),\n            ui.input_numeric(\n                \"num_prior_samples\", \n                \"Number of Prior Samples\", \n                value=10000, \n                min=1000, \n                max=100000, \n                step=1000\n            ),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Generated Data\", ui.output_plot(\"generated_heights_plot\")),\n            ui.nav_panel(\"Prior Distributions\", ui.output_plot(\"prior_distributions_plot\")),\n            ui.nav_panel(\"Prior-Generated Heights\", ui.output_plot(\"prior_generated_heights_plot\")),\n            ui.nav_panel(\"Posterior Distributions\", ui.output_plot(\"posterior_distributions_plot\")),\n            ui.nav_panel(\"Parameter Scatter Plot\", ui.output_plot(\"parameter_scatter_plot\")),\n        )\n\n    )\n)\n\ndef server(input, output, session):\n    # Reactive expression to generate dataset\n    @reactive.Calc\n    def heights():\n        np.random.seed(42)  # For reproducibility\n        mu = input.mu_true()\n        sigma = input.sigma_true()\n        size = input.dataset_size()\n        return np.random.normal(loc=mu, scale=sigma, size=size)\n    \n    # Reactive expressions for prior ranges\n    @reactive.Calc\n    def mu_prior_range():\n        return (input.mu_prior_min(), input.mu_prior_max())\n    \n    @reactive.Calc\n    def sigma_prior_range():\n        return (input.sigma_prior_min(), input.sigma_prior_max())\n    \n    # Reactive expression to sample from priors\n    @reactive.Calc\n    def prior_samples():\n        num_samples = input.num_prior_samples()\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        mu_samples = np.random.uniform(mu_min, mu_max, num_samples)\n        sigma_samples = np.random.uniform(sigma_min, sigma_max, num_samples)\n        # Avoid sigma=0\n        sigma_samples[sigma_samples == 0] = 1e-6\n        return mu_samples, sigma_samples\n    \n    # Reactive expression to generate heights from priors\n    @reactive.Calc\n    def heights_prior_samples():\n        mu_samples, sigma_samples = prior_samples()\n        return np.random.normal(loc=mu_samples, scale=sigma_samples)\n    \n    # Reactive expression to compute posterior\n    @reactive.Calc\n    def posterior():\n        data = heights()\n        mu_samples, sigma_samples = prior_samples()\n        n = len(data)\n        \n        # Compute log-likelihoods\n        log_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n        \n        # Normalize log-likelihoods to prevent underflow\n        max_log_likelihood = np.max(log_likelihoods)\n        normalized_log_likelihood = log_likelihoods - max_log_likelihood\n        likelihoods = np.exp(normalized_log_likelihood)\n        \n        # Posterior probabilities (unnormalized)\n        posterior_probs = likelihoods\n        \n        # Normalize posterior\n        posterior_probs /= np.sum(posterior_probs)\n        \n        return posterior_probs\n    \n    # Reactive expression to create DataFrame for posterior\n    @reactive.Calc\n    def posterior_df():\n        mu_samples, sigma_samples = prior_samples()\n        posterior_probs = posterior()\n        return pd.DataFrame({\n            'mu': mu_samples,\n            'sigma': sigma_samples,\n            'posterior': posterior_probs\n        })\n    \n    # Plot 1: Generated Heights Histogram\n    @output\n    @render.plot\n    def generated_heights_plot():\n        data = heights()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(data, bins=15, kde=True, color='skyblue')\n        plt.title(f'Histogram of Generated Heights ({input.dataset_size()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 2: Prior Distributions for mu and sigma\n    @output\n    @render.plot\n    def prior_distributions_plot():\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Prior for mu\n        mu_values = np.linspace(mu_min, mu_max, 1000)\n        mu_prior = np.ones_like(mu_values) / (mu_max - mu_min)\n        ax[0].plot(mu_values, mu_prior, color='blue')\n        ax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Probability Density')\n        \n        # Prior for sigma\n        sigma_values = np.linspace(sigma_min, sigma_max, 1000)\n        sigma_prior_dist = np.ones_like(sigma_values) / (sigma_max - sigma_min)\n        ax[1].plot(sigma_values, sigma_prior_dist, color='green')\n        ax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Probability Density')\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 3: Heights Generated from Priors\n    @output\n    @render.plot\n    def prior_generated_heights_plot():\n        heights_prior = heights_prior_samples()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(heights_prior, bins=50, kde=True, color='orange')\n        plt.title(f'Histogram of Heights Generated from Priors ({input.num_prior_samples()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 4: Posterior Distributions for mu and sigma\n    @output\n    @render.plot\n    def posterior_distributions_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Posterior for mu\n        sns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\n        ax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        ax[0].set_title(r'Posterior Distribution for $\\mu$')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Posterior Probability')\n        ax[0].legend()\n        \n        # Posterior for sigma\n        sns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\n        ax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\n        ax[1].set_title(r'Posterior Distribution for $\\sigma$')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Posterior Probability')\n        ax[1].legend()\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 5: Scatter Plot of mu vs sigma with Log Posterior Probability\n    @output\n    @render.plot\n    def parameter_scatter_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        epsilon = 1e-10  # To avoid log(0)\n        log_posterior = np.log(df['posterior'] + epsilon)\n        \n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            df['mu'],\n            df['sigma'],\n            c=log_posterior,\n            cmap='viridis',\n            alpha=0.5,\n            s=10\n        )\n        cbar = plt.colorbar(scatter)\n        cbar.set_label('Log Posterior Probability')\n        \n        plt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        plt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n        \n        plt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\n        plt.xlabel(r'$\\mu$ (feet)')\n        plt.ylabel(r'$\\sigma$ (feet)')\n        plt.legend()\n        plt.tight_layout()\n        return plt.gcf()\n\napp = App(app_ui, server)\nv2\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Bayesian Inference of Heights\"),\n    ui.row(\n        ui.column(3,\n            ui.h4(\"True Parameters\"),\n            ui.input_numeric(\n                \"mu_true\", \n                \"True Mean Height [feet]\", \n                value=5.5, \n                min=4.0, \n                max=7.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"sigma_true\", \n                \"True Std Dev (σ_true) [feet]\", \n                value=0.3, \n                min=0.1, \n                max=1.0, \n                step=0.05\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Dataset Generation\"),\n            ui.input_numeric(\n                \"dataset_size\", \n                \"Number of Persons\", \n                value=100, \n                min=10, \n                max=1000, \n                step=10\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Prior Ranges\"),\n            ui.input_numeric(\n                \"mu_prior_min\", \n                \"μ Prior Minimum [feet]\", \n                value=4.0, \n                min=0.0, \n                max=6.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"mu_prior_max\", \n                \"μ Prior Maximum [feet]\", \n                value=7.0, \n                min=5.0, \n                max=10.0, \n                step=0.1\n            ),\n        ),\n        ui.column(3,\n            ui.h4(\"Prior Ranges & Sampling\"),\n            ui.input_numeric(\n                \"sigma_prior_min\", \n                \"σ Prior Minimum [feet]\", \n                value=0.0, \n                min=0.0, \n                max=1.0, \n                step=0.05\n            ),\n            ui.input_numeric(\n                \"sigma_prior_max\", \n                \"σ Prior Maximum [feet]\", \n                value=0.9, \n                min=0.5, \n                max=3.0, \n                step=0.1\n            ),\n            ui.input_numeric(\n                \"num_prior_samples\", \n                \"Number of Prior Samples\", \n                value=10000, \n                min=1000, \n                max=100000, \n                step=1000\n            ),\n        )\n    ),\n    ui.navset_tab(\n        ui.nav_panel(\"True Height Data\", ui.output_plot(\"generated_heights_plot\")),\n        ui.nav_panel(\"Prior\", ui.output_plot(\"prior_distributions_plot\")),\n        ui.nav_panel(\"Prior-Generated Heights\", ui.output_plot(\"prior_generated_heights_plot\")),\n        ui.nav_panel(\"Posterior\", ui.output_plot(\"posterior_distributions_plot\")),\n        ui.nav_panel(\"Parameter Plot\", ui.output_plot(\"parameter_scatter_plot\")),\n    )\n)\n\n\ndef server(input, output, session):\n    # Reactive expression to generate dataset\n    @reactive.Calc\n    def heights():\n        np.random.seed(42)  # For reproducibility\n        mu = input.mu_true()\n        sigma = input.sigma_true()\n        size = input.dataset_size()\n        return np.random.normal(loc=mu, scale=sigma, size=size)\n    \n    # Reactive expressions for prior ranges\n    @reactive.Calc\n    def mu_prior_range():\n        return (input.mu_prior_min(), input.mu_prior_max())\n    \n    @reactive.Calc\n    def sigma_prior_range():\n        return (input.sigma_prior_min(), input.sigma_prior_max())\n    \n    # Reactive expression to sample from priors\n    @reactive.Calc\n    def prior_samples():\n        num_samples = input.num_prior_samples()\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        mu_samples = np.random.uniform(mu_min, mu_max, num_samples)\n        sigma_samples = np.random.uniform(sigma_min, sigma_max, num_samples)\n        # Avoid sigma=0\n        sigma_samples[sigma_samples == 0] = 1e-6\n        return mu_samples, sigma_samples\n    \n    # Reactive expression to generate heights from priors\n    @reactive.Calc\n    def heights_prior_samples():\n        mu_samples, sigma_samples = prior_samples()\n        return np.random.normal(loc=mu_samples, scale=sigma_samples)\n    \n    # Reactive expression to compute posterior\n    @reactive.Calc\n    def posterior():\n        data = heights()\n        mu_samples, sigma_samples = prior_samples()\n        n = len(data)\n        \n        # Compute log-likelihoods\n        log_likelihoods = norm.logpdf(data[:, np.newaxis], loc=mu_samples, scale=sigma_samples).sum(axis=0)\n        \n        # Normalize log-likelihoods to prevent underflow\n        max_log_likelihood = np.max(log_likelihoods)\n        normalized_log_likelihood = log_likelihoods - max_log_likelihood\n        likelihoods = np.exp(normalized_log_likelihood)\n        \n        # Posterior probabilities (unnormalized)\n        posterior_probs = likelihoods\n        \n        # Normalize posterior\n        posterior_probs /= np.sum(posterior_probs)\n        \n        return posterior_probs\n    \n    # Reactive expression to create DataFrame for posterior\n    @reactive.Calc\n    def posterior_df():\n        mu_samples, sigma_samples = prior_samples()\n        posterior_probs = posterior()\n        return pd.DataFrame({\n            'mu': mu_samples,\n            'sigma': sigma_samples,\n            'posterior': posterior_probs\n        })\n    \n    # Plot 1: Generated Heights Histogram\n    @output\n    @render.plot\n    def generated_heights_plot():\n        data = heights()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(data, bins=15, kde=True, color='skyblue')\n        plt.title(f'Histogram of Generated Heights ({input.dataset_size()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 2: Prior Distributions for mu and sigma\n    @output\n    @render.plot\n    def prior_distributions_plot():\n        mu_min, mu_max = mu_prior_range()\n        sigma_min, sigma_max = sigma_prior_range()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Prior for mu\n        mu_values = np.linspace(mu_min, mu_max, 1000)\n        mu_prior = np.ones_like(mu_values) / (mu_max - mu_min)\n        ax[0].plot(mu_values, mu_prior, color='blue')\n        ax[0].set_title(r'Prior Distribution for $\\mu$ (Mean Height)')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Probability Density')\n        \n        # Prior for sigma\n        sigma_values = np.linspace(sigma_min, sigma_max, 1000)\n        sigma_prior_dist = np.ones_like(sigma_values) / (sigma_max - sigma_min)\n        ax[1].plot(sigma_values, sigma_prior_dist, color='green')\n        ax[1].set_title(r'Prior Distribution for $\\sigma$ (Std Dev)')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Probability Density')\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 3: Heights Generated from Priors\n    @output\n    @render.plot\n    def prior_generated_heights_plot():\n        heights_prior = heights_prior_samples()\n        plt.figure(figsize=(8, 5))\n        sns.histplot(heights_prior, bins=50, kde=True, color='orange')\n        plt.title(f'Histogram of Heights Generated from Priors ({input.num_prior_samples()} Persons)')\n        plt.xlabel('Height (feet)')\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 4: Posterior Distributions for mu and sigma\n    @output\n    @render.plot\n    def posterior_distributions_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Posterior for mu\n        sns.histplot(data=df, x='mu', weights='posterior', bins=50, kde=True, color='purple', ax=ax[0])\n        ax[0].axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        ax[0].set_title(r'Posterior Distribution for $\\mu$')\n        ax[0].set_xlabel(r'$\\mu$ (feet)')\n        ax[0].set_ylabel('Posterior Probability')\n        ax[0].legend()\n        \n        # Posterior for sigma\n        sns.histplot(data=df, x='sigma', weights='posterior', bins=50, kde=True, color='brown', ax=ax[1])\n        ax[1].axvline(sigma_true, color='red', linestyle='--', label=r'True $\\sigma$')\n        ax[1].set_title(r'Posterior Distribution for $\\sigma$')\n        ax[1].set_xlabel(r'$\\sigma$ (feet)')\n        ax[1].set_ylabel('Posterior Probability')\n        ax[1].legend()\n        \n        plt.tight_layout()\n        return plt.gcf()\n    \n    # Plot 5: Scatter Plot of mu vs sigma with Log Posterior Probability\n    @output\n    @render.plot\n    def parameter_scatter_plot():\n        df = posterior_df()\n        mu_true = input.mu_true()\n        sigma_true = input.sigma_true()\n        \n        epsilon = 1e-10  # To avoid log(0)\n        log_posterior = np.log(df['posterior'] + epsilon)\n        \n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            df['mu'],\n            df['sigma'],\n            c=log_posterior,\n            cmap='viridis',\n            alpha=0.5,\n            s=10\n        )\n        cbar = plt.colorbar(scatter)\n        cbar.set_label('Log Posterior Probability')\n        \n        plt.axvline(mu_true, color='red', linestyle='--', label=r'True $\\mu$')\n        plt.axhline(sigma_true, color='blue', linestyle='--', label=r'True $\\sigma$')\n        \n        plt.title(r'Posterior Probability of $\\mu$ and $\\sigma$ (Log Scale)')\n        plt.xlabel(r'$\\mu$ (feet)')\n        plt.ylabel(r'$\\sigma$ (feet)')\n        plt.legend()\n        plt.tight_layout()\n        return plt.gcf()\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.4-Naive_Monte_Carlo.html#less-naive-monte-carlo",
    "href": "3.4-Naive_Monte_Carlo.html#less-naive-monte-carlo",
    "title": "Naive Monte Carlo",
    "section": "Less Naive Monte Carlo",
    "text": "Less Naive Monte Carlo\nIf you paid attention to the Posterior Probability of \\(\\mu\\) and \\(\\sigma\\) plot, you probabiliy noticed that a lot of the points/space on the chart was consumed by areas of low probability. This was true even though we had few parameters (low dimensions) and good priors. The trick of the more advanced Monte Carlo techniques is how to find the areas of high likelihood and sample them efficiently. These include Hamiltonian Monte Carlo (HMC) and the No-U-Turn Sampler (NUTS). We’ll use em, and thankfully the details are easily researched if you want to know more…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Naive Monte Carlo"
    ]
  },
  {
    "objectID": "3.1-without_priors.html",
    "href": "3.1-without_priors.html",
    "title": "Without Priors",
    "section": "",
    "text": "We finally flip to \\(P(D|M)\\) the probability of a model given the data.\nLike before, we begin with a simple dice model as we consider it to be an intuitive subject. We then consider a continuous example before moving on to generalized linear models. We will save bespoke models for the next chapter that includes prior knowledge for estimating models.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#preview",
    "href": "3.1-without_priors.html#preview",
    "title": "Without Priors",
    "section": "",
    "text": "We finally flip to \\(P(D|M)\\) the probability of a model given the data.\nLike before, we begin with a simple dice model as we consider it to be an intuitive subject. We then consider a continuous example before moving on to generalized linear models. We will save bespoke models for the next chapter that includes prior knowledge for estimating models.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#data-generation-reversed",
    "href": "3.1-without_priors.html#data-generation-reversed",
    "title": "Without Priors",
    "section": "Data Generation Reversed",
    "text": "Data Generation Reversed\nIn part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we want to take the data and find the most likely model of the data generating process. It’s reasonable to think of this as the ‘reverse’ of our previous approach in part one.\nThe simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Precompute the \"permanent\" histogram for 7 dice, 10,000 rolls ---\nFIXED_NUM_DICE = 7\nFIXED_NUM_ROLLS = 10000\n\nfixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]\nfixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"num_dice\",\n                \"Number of Dice (1–10)\",\n                min=1,\n                max=10,\n                value=2,\n                step=1\n            ),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def user_sums():\n        # Always roll the user-selected dice 10,000 times\n        N_ROLLS = 10000\n        n_dice = input.num_dice()\n        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]\n        return rolls\n\n    @output\n    @render.plot\n    def dicePlot():\n        # Get the user’s histogram\n        sums = user_sums()\n        user_unique_vals, user_counts = np.unique(sums, return_counts=True)\n        \n        # Determine the union of x-values (totals) so both histograms can share the same axis\n        all_x = np.arange(\n            min(fixed_unique_vals[0], user_unique_vals[0]),\n            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1\n        )\n        \n        # Convert the unique/value arrays to dictionaries for easy indexing\n        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))\n        user_map = dict(zip(user_unique_vals, user_counts))\n        \n        # Pull out frequency or 0 if total not present in the distribution\n        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]\n        user_freqs  = [user_map.get(x, 0) for x in all_x]\n        \n        # Plot\n        fig, ax = plt.subplots()\n        \n        # Bar chart for the fixed 7-dice histogram\n        ax.bar(all_x, fixed_freqs, color=\"lightblue\", alpha=0.6, label=\"Fixed Dice\")\n        \n        # Overlay user histogram as points\n        ax.scatter(all_x, user_freqs, color=\"red\", marker=\"o\", label=\"User Selected Dice\")\n        \n        ax.set_title(\"Update the Input Parameter to Match Observations\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        ax.legend()\n        \n        # Make x-axis tick at every possible total\n        plt.xticks(all_x, rotation=90)\n        \n        return fig\n\napp = App(app_ui, server)\nI’m guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren’t certain about the model generating it, we can work in ‘reverse’ to find a likely model of the real world data generating process.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#likelihood",
    "href": "3.1-without_priors.html#likelihood",
    "title": "Without Priors",
    "section": "Likelihood",
    "text": "Likelihood\nIn the previous dice example, we were just eyeballing the right model parameters to maximize the probability of observing our data under the model. We’d like a more consistent and mathematical way to achieve the same intent, which is called a likelihood function. In generic form, it looks like this:\n\\[\n\\mathcal{L}(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{N} P(y_i \\mid \\theta)\n\\]\n\n\\(\\mathcal{L}(\\theta \\mid \\mathbf{y})\\): This represents the overall likelihood function, which measures the likelihood of the model parameters \\(\\theta\\) given the observed data \\(\\mathbf{y}\\). It aggregates the individual probability densities for continuous functions and probabilities for discrete functions across all data points, reflecting how well the parameters \\(\\theta\\) explain the entire dataset.\n\\(P(y_i \\mid \\theta)\\): This is the individual probability density for continuous functions or probability for discrete functions for a single data point \\(i\\). It represents the probability of observing the specific outcome \\(y_i\\) under the model parameterized by \\(\\theta\\). The form of this probability depends on the assumed probability distribution of the data (e.g., binomial for dice rolls, Gaussian for continuous data).\n\\(\\prod_{i=1}^{N}\\): This is the product operation, which multiplies the individual probabilities \\(P(y_i \\mid \\theta)\\) across all \\(N\\) data points in the dataset. This multiplication reflects the assumption that the data points are independent and identically distributed (i.i.d.), meaning the probability of the entire dataset is the product of the probabilities of each individual data point.\n\nThe specific form of the likelihood function is important and can change meaningfully depending on the problem… (MAYBE SAY LATER) If you have fit a regression line to data you probably used a loss function without knowing it, and it was almost certainly base don least squares, which happens to be appropriate when the errors in your data are normally distributed…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#continuous-data",
    "href": "3.1-without_priors.html#continuous-data",
    "title": "Without Priors",
    "section": "Continuous Data",
    "text": "Continuous Data\n\nNormal Distribution with MLE Estimate\nThe following is a very similar app to what we saw earlier that just calculated with likelihood of the data. However, this time we also have ability to find the most likely model to explain the data, i.e the Maximimum Likelihood Estimate MLE …\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\n\n\n\n\n\n\nLoss Functions\n\n\n\nLikelihood is a specific form of a loss function. Likelihood is rooted in probability, but a loss function does not need to be. Loss functions in machine learning are what likelihood functions are in statistics. In generic form they look like this:\n\\[\n\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\n\\(\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\\): This represents the overall loss function, which measures the difference between the true values \\(\\mathbf{y}\\) and the predicted values \\(\\hat{\\mathbf{y}}\\). It aggregates the individual losses across all data points.\n\\(\\ell(y_i, \\hat{y}_i)\\): This is the individual loss for a single data point \\(i\\). It can take different forms depending on the type of problem.\n\\(\\frac{1}{N} \\sum_{i=1}^{N}\\): This is the averaging operation, which sums up the individual losses \\(\\ell(y_i, \\hat{y}_i)\\) across all \\(N\\) data points in the dataset and divides by \\(N\\) to compute the average loss. This helps ensure that the loss function is independent of the dataset size.\n\n\n\nWe introduce loss functions here but will apply them next chapter??",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#methods",
    "href": "3.1-without_priors.html#methods",
    "title": "Without Priors",
    "section": "Methods",
    "text": "Methods\nBefore we dive too deep into the details, we are going to take a moment to reflect on what we’re attempting to do and common techniques to do it…\nWhen we were just generating data, it was obvious what variables went into the model, and what data came out of the model. However, we should do a better job of defining that now. Part of the issue here is that many names are used for many applications. We will call inputs predictor/feature variables, and we will call the outputs the outcome/target variables…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "3.1-without_priors.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "href": "3.1-without_priors.html#note-in-3.2-we-will-go-into-the-likelihood-calculation-itself",
    "title": "Without Priors",
    "section": "Note in 3.2 we will go into the likelihood calculation itself",
    "text": "Note in 3.2 we will go into the likelihood calculation itself",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Without Priors"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html",
    "href": "2.1-discrete_probability_distributions.html",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "In the first half of the primer we focus on \\(P(D|M)\\), the probability of the data given a model of a data generating process. This first chapter considers only models with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce more discrete probability distributions as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. For any model we can use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. Where an analytical solution of \\(P(D|M)\\) is available, we will also compare exact probabilities to those estimated from relative frequency.\nWe then shift to the probability of multiple events based on the laws of probability for independent events. We show how we can use computation to calculate the relative probability of a specific series of events by comparing it to a multitude of randomly generated series of events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls. This is our more intuitive approach to hypothesis testing.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#preview",
    "href": "2.1-discrete_probability_distributions.html#preview",
    "title": "Discrete Probability Distributions",
    "section": "",
    "text": "In the first half of the primer we focus on \\(P(D|M)\\), the probability of the data given a model of a data generating process. This first chapter considers only models with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce more discrete probability distributions as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.\nOnce we have have some discussion of models under our belt, we will change our focus to the probability of data. For any model we can use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. Where an analytical solution of \\(P(D|M)\\) is available, we will also compare exact probabilities to those estimated from relative frequency.\nWe then shift to the probability of multiple events based on the laws of probability for independent events. We show how we can use computation to calculate the relative probability of a specific series of events by comparing it to a multitude of randomly generated series of events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls. This is our more intuitive approach to hypothesis testing.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#models-of-discrete-data-generating-processes",
    "href": "2.1-discrete_probability_distributions.html#models-of-discrete-data-generating-processes",
    "title": "Discrete Probability Distributions",
    "section": "Models of Discrete Data Generating Processes",
    "text": "Models of Discrete Data Generating Processes\n\nA Dice Total Model\nWe’d prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we’ll use rolling dice as our first example of a data generating process. It is also convenient that the mathematical model we’ll use is a good approximation of the real data generating process, so long as you’re OK with ignoring all the physical bouncing of the dice and are content with just the result after a roll.\nA dice model is built into the app displayed beneath this paragraph. It will simulate rolling the number of dice you specify, as if you threw them out of a cup all at once, and total the value on those dice from the cup, which is considered one roll. Additionally, it will repeat rolling that cup of dice the number of times you specify, and summarize the results on a histogram. Play around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling App\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\napp = App(app_ui, server)\nHopefully you’ve noted how a larger number of rolls seems to give us smoother and more consistent results. We will revisit this point more precisely in a later section.\n\n\nDiscrete Probability Distributions as Models of Data Generating Processes\nIn the last section, we used dice rolling as our data generating process, however there are other discrete processes we may be interested in, such as testing 1,000 products that have a 0.999 probability of success each. As you can imagine, when you change the data generating process, the relative frequency (distribution) of the outcomes change. There are a number of discrete processes that are important to statisticians, and they have been formalized into mathematical models called discrete probability distributions.\n\n\n\n\n\n\nNote\n\n\n\nIf you are familiar with discrete probability distributions, you may have expected them to be introduced as a way to find the exact probability of an event, instead of using them to generate data. We will eventually use this feature, but our goal is to more generally introduce models of data generating processes and their associated probability distributions. Only a small number have properties that allow for nice analytical solutions to the probabilities of their data, and we’d prefer not to constrain ourselves to thinking in only an analytical (as opposed to computational) framework.\n\n\nInitially we want to think about discrete probability distributions in the same way we thought about our dice model, that it is a model that will generate random events from a data generating process of interest. Below are a couple examples to illustrate the point.\n\nBinomial Distribution\nThe binomial distribution is a model that represents the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (commonly referred to as “success” and “failure”). The probability of success can range between 0 and 1. In the app below you can recreate something like the product testing scenario recently mentioned. Feel free to play around with the settings/parameters to get a feel for how the probability distribution behaves.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Binomial Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"numTrials\", \n                \"Number of Trials (n)\", \n                min=1, \n                max=10000, \n                value=100, \n                step=1\n            ),\n            ui.input_slider(\n                \"probSuccess\", \n                \"Probability of Success (p)\", \n                min=0.001, \n                max=0.999, \n                value=0.5, \n                step=0.001\n            ),\n            # Removed the \"Number of Simulations\" slider\n            # Fixed number of simulations to 10,000\n        ),\n        ui.output_plot(\"binomPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    # Fixed number of simulations\n    FIXED_NUM_SIMULATIONS = 10000\n\n    # Reactive expression to generate binomial samples\n    @reactive.Calc\n    def binomial_samples():\n        n = input.numTrials()\n        p = input.probSuccess()\n        size = FIXED_NUM_SIMULATIONS\n        return np.random.binomial(n, p, size)\n    \n    # Function to determine optimal integer bin width\n    def determine_bin_width(data, max_bins=30):\n        data_min = data.min()\n        data_max = data.max()\n        data_range = data_max - data_min + 1  # +1 to include both endpoints\n        # Start with bin_width = 1 and increase until the number of bins &lt;= max_bins\n        for bin_width in range(1, data_range + 1):\n            num_bins = math.ceil(data_range / bin_width)\n            if num_bins &lt;= max_bins:\n                return bin_width\n        return 1  # Fallback to bin_width=1 if all else fails\n    \n    # Render the binomial distribution plot with limited bins\n    @output\n    @render.plot\n    def binomPlot():\n        samples = binomial_samples()\n        bin_width = determine_bin_width(samples, max_bins=30)\n        \n        # Define bin edges based on bin_width\n        data_min = samples.min()\n        data_max = samples.max()\n        bins = np.arange(data_min, data_max + bin_width, bin_width)\n        \n        # Compute histogram\n        counts, bin_edges = np.histogram(samples, bins=bins)\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        ax.bar(bin_centers, counts, width=bin_width*0.9, color=\"steelblue\", edgecolor=\"black\", align='center')\n        \n        ax.set_title(\"Frequency of Successes in Binomial Simulations\", fontsize=16)\n        ax.set_xlabel(\"Number of Successes\", fontsize=14)\n        ax.set_ylabel(\"Frequency\", fontsize=14)\n        #ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n        \n        # Set x-axis ticks to be at bin centers\n        ax.set_xticks(bin_centers)\n        \n        # To avoid overcrowding, set a maximum number of x-ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax.set_xticks(bin_centers[::step])\n            ax.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)\n        else:\n            ax.set_xticklabels([int(x) for x in bin_centers], rotation=90)\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\n\n\nPoisson Distribution\nThe Poisson distribution is a model that describes the number of events occurring in a fixed interval of time or space, assuming that the events occur independently and at a constant average rate. For example, it might represent the number of phone calls received by a call center in an hour or the number of cars passing through a toll booth in a minute. The Poisson Distribution is similar to the Binomial distribution, except there are not a fixed number of trials, so there is not an upper limit to the number of events returned by a sample. However, values much larger than the average rate become incredibly unlikely.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\nimport math\n\n# Define the User Interface\napp_ui = ui.page_fluid(\n    ui.h2(\"Poisson Distribution Simulation\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"rate_param\", \n                \"Rate Parameter (λ)\", \n                min=0.01, \n                max=10.0, \n                value=1.0, \n                step=0.01\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Times to Repeat Sample\",\n                min=100,\n                max=10000,\n                value=10000,\n                step=100\n            )\n        ),\n        ui.output_plot(\"poissonPlot\", height=\"400px\"),\n    )\n)\n\n# Define the Server Logic\ndef server(input, output, session):\n\n    # Reactive expression to get the number of simulations\n    @reactive.Calc\n    def num_simulations():\n        return input.num_trials()\n\n    # Reactive expression to generate Poisson samples\n    @reactive.Calc\n    def poisson_samples():\n        lam = input.rate_param()\n        n_trials = num_simulations()\n        return np.random.poisson(lam, n_trials)\n    \n    # Function to determine optimal integer bin width with a maximum number of bins\n    def determine_bin_width(data, max_bins=30):\n        data_min = data.min()\n        data_max = data.max()\n        data_range = data_max - data_min + 1  # +1 to include both endpoints\n        # Start with bin_width = 1 and increase until the number of bins &lt;= max_bins\n        for bin_width in range(1, data_range + 1):\n            num_bins = math.ceil(data_range / bin_width)\n            if num_bins &lt;= max_bins:\n                return bin_width\n        return 1  # Fallback to bin_width=1 if all else fails\n\n    # Render the Poisson distribution plot\n    @output\n    @render.plot\n    def poissonPlot():\n        samples = poisson_samples()\n        lam = input.rate_param()\n        \n        bin_width = determine_bin_width(samples, max_bins=30)\n        \n        # Define bin edges based on bin_width\n        data_min = samples.min()\n        data_max = samples.max()\n        bins = np.arange(data_min, data_max + bin_width, bin_width)\n        \n        # Compute histogram\n        counts, bin_edges = np.histogram(samples, bins=bins)\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot the histogram\n        ax.bar(bin_centers, counts, width=bin_width*0.9, color=\"steelblue\", edgecolor=\"black\", align='center')\n        \n        ax.set_title(f\"Poisson Distribution Simulation (λ = {lam})\", fontsize=16)\n        ax.set_xlabel(\"Number of Events\", fontsize=14)\n        ax.set_ylabel(\"Frequency\", fontsize=14)\n        #ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n        \n        # Set x-axis ticks to be at bin centers\n        ax.set_xticks(bin_centers)\n        \n        # To avoid overcrowding, set a maximum number of x-ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax.set_xticks(bin_centers[::step])\n            ax.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)\n        else:\n            ax.set_xticklabels([int(x) for x in bin_centers], rotation=90)\n        \n        plt.tight_layout()\n        \n        return fig\n\n# Create the Shiny App\napp = App(app_ui, server)\n\n\nSummary\nWe keep this section brief as there are plenty of easily accessible references for discrete probability distributions. Hopefully the point was made though - that each discrete probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcomes of a process.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "2.1-discrete_probability_distributions.html#probability-of-data",
    "href": "2.1-discrete_probability_distributions.html#probability-of-data",
    "title": "Discrete Probability Distributions",
    "section": "Probability of Data",
    "text": "Probability of Data\nHaving given some background on models, we can now focus on the probability of the data given the model, \\(P(D|M)\\).\n\nDice Totals Probability of Data\n\nRelative Frequency\nWe want to find the probability of a particular dice total (the data) given a dice model. To estimate the probability \\(P(E)\\) of an event \\(E\\), we can use the relative frequency approach. This involves counting the number of occurrences of the event E and dividing it by the total number of trials. For example, if we observed a total of twelve occur in 40 out of 5,000 dice rolls, the probability estimate is:\n\\[\nP(E) \\approx \\frac{\\text{Number of times event } E \\text{ occurs}}{\\text{Total number of trials}} = \\frac{40}{5000} = 0.008\n\\]\n\n\nLaw of Large Numbers\nThe accuracy of this estimate depends on the total number of trials as governed by the Law of Large Numbers. The standard error, which gives a measure of uncertainty in the estimate of a mean value, is proportional to one over the square root of the number of samples:\n\\[\nP_{\\text{error}} \\propto \\frac{1}{\\sqrt{N_{\\text{total}}}}\n\\]\nWhich indicates there are diminishing returns to just making the sample size larger. Now I know you’re smart, and you’re saying to yourself, I can figure out the exact probability of rolling a certain dice total. Of course you can for this example - but you probably can’t for more realistic examples, and we want to learn techniques that work well for real problems. In general, if you are concerned with the quality of an estimate with this approach, just rerun the model and see if the outcome changes meaningfully - if it does, increase the number of times we run the model until the output is stable enough for your application. If that’s still not enough, dig into exact/analytic methods.\n\n\nRevised Dice App\nHere’s another version of the Dice Total App that you saw earlier - except it now has additional functionality to calculate the approximate and exact probability of a certain dice total based on your inputs. It also shows the relative frequency of the dice sums, i.e. a true probability distribution where the sum of the probabilities equals one.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 560\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Utility function to compute exact distribution of sums for n dice ---\ndef dice_sum_distribution(n_dice):\n    \"\"\"\n    Return a list 'dist' where dist[s] = probability of sum s for n_dice dice.\n    Indices go from 0 up to 6*n_dice. Only sums in range [n_dice..6*n_dice]\n    have nonzero probabilities.\n    \"\"\"\n    # ways[s] = number of ways to get sum s\n    ways = [0] * (6*n_dice + 1)\n    ways[0] = 1  # base case\n\n    for _ in range(n_dice):\n        new_ways = [0] * (6*n_dice + 1)\n        for sum_val, count in enumerate(ways):\n            if count &gt; 0:\n                for face in range(1, 7):\n                    new_ways[sum_val + face] += count\n        ways = new_ways\n\n    total_outcomes = 6 ** n_dice\n    dist = [count / total_outcomes for count in ways]\n    return dist\n\n# -------------------------- UI Definition ---------------------------\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling App with Probability Mass Function\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n            ui.input_select(\n                \"selectedTotal\", \n                \"Select Dice Total\", \n                choices=[\"\"],   # initially empty, will be updated dynamically\n                multiple=False,\n            ),\n            ui.output_text(\"approxProbability\"),\n            ui.output_text(\"exactProbability\")\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    )\n)\n\n# -------------------------- Server Definition -------------------------\ndef server(input, output, session):\n    # Reactive: Generate random sums based on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Reactive: Exact distribution of sums for the current number of dice\n    @reactive.Calc\n    def exact_distribution():\n        return dice_sum_distribution(input.numDice())\n\n    # Dynamically update the choices in the 'selectedTotal' select input\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    # Plot the relative frequency of dice totals\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        \n        fig, ax = plt.subplots()\n        \n        # Get theoretical distribution first to set x-axis limits\n        dist = exact_distribution()\n        theoretical_sums = range(input.numDice(), 6*input.numDice()+1)\n        theoretical_probs = [dist[i] for i in theoretical_sums]\n        \n        # Create dictionary to store empirical probabilities for all possible sums\n        empirical_dict = {i: 0 for i in theoretical_sums}\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        for sum_val, count in zip(unique_sums, counts):\n            if sum_val in empirical_dict:\n                empirical_dict[sum_val] = count / len(current_sums)\n        \n        # Plot empirical distribution\n        ax.bar([str(s) for s in theoretical_sums], \n               [empirical_dict[s] for s in theoretical_sums],\n               color=\"steelblue\", label=\"Empirical\")\n        \n        # Plot theoretical distribution\n        ax.plot([str(s) for s in theoretical_sums], theoretical_probs, \n                'r', marker='_', linestyle='', label=\"Theoretical\")\n\n        ax.set_title(\"Probability Mass Function of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Probability\")\n        ax.legend()\n        plt.xticks(rotation=90)\n\n        return fig\n\n    # Approximate probability of the selected dice total\n    @output\n    @render.text\n    def approxProbability():\n        if not input.selectedTotal():\n            return \"Select a dice total to see probabilities.\"\n\n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n\n        count = sum(1 for x in current_sums if x == selected_total)\n        if len(current_sums) == 0:\n            prob = 0\n        else:\n            prob = count / len(current_sums)\n\n        return f\"Approx. Probability of {selected_total}: {prob:.4f}\"\n\n    # Exact probability of the selected dice total\n    @output\n    @render.text\n    def exactProbability():\n        if not input.selectedTotal():\n            return \"\"\n\n        selected_total = int(input.selectedTotal())\n        dist = exact_distribution()\n\n        # If the selected total is out of range, probability is 0\n        if selected_total &lt; 0 or selected_total &gt;= len(dist):\n            prob = 0\n        else:\n            prob = dist[selected_total]\n\n        return f\"Exact Probability of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)\nHopefully you can demonstrate to yourself that with enough samples, the approximate probability calculation is awfully close to the exact probability. However, there is an exception. The tails (the slim far ends) are not as accurate. Properly calculating probability in these tail sections happens to be trivial for dice where an exact solution is available, but for a real problem, accurate tail probabilities are incredibly difficult.\n\n\n\n\n\n\nNote\n\n\n\nA few readers may already know an essay called the ‘The Bitter Lesson’ by Rich Sutton. If you haven’t read it, it is worth a read, and easily found on the internet. I believe it has a strong engineering corollary, in that our education is too obsessed with analytical solutions instead of computational ones. To temper that statement slightly, I can also assure you that solving problems through raw computation has its limits as there’s no way to compute through a bad algorithm. But we’re professionals in a hurry, we choose computation whenever it’s plausible.\n\n\n\n\n\nExact Probabilities from a Named Probability Distributions\nHere we provide an example of the more ‘typical’ use of a named probability distribution, to compute the probability of an event. Let’s use the properties of the binomial probability distribution to determine the probability of having less than 999 products pass a quality test when we have a true 0.999 pass rate and we inspect 1,000 products.\nWe’ll solve this problem by computing the probability of having 999 or 1,000 products pass, and then just inverting the probability. Note that \\(P(X = 1000)\\) is simplified below based on the multiplication rule of independent events to equal 0.999^1,000. Also, if you’re unfamiliar with the n over k in parenthesis, look up ‘n choose k’.\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\n\\[\nP(X = 999) + P(X = 1,000) = \\binom{1000}{999} (0.999)^{999} (0.001)^1 + (0.999)^{1,000}\n\\]\nThe result is 0.736, so the chance of having a real pass rate of 0.999 and having less than 999 pass is 1 - 0.736 = 0.264. It’s fairly plausible to have 0.999 pass rate and have a couple of products fail. However, if you had four or more fail, you should have a very hard time convincing management that it was just ‘bad luck’. The more formal way to discuss this is hypothesis testing.\n\n\n\n\n\n\nNote\n\n\n\nMuch of statistics is dedicated to hypothesis testing. There’s no reason for this primer to repeat such a repeated topic. The problem though, is that in the real world, folks get it wrong. I have a simple hypothesis for this, and that’s because non-expert practitioners do not understand their statistical models. And there’s good reasons they don’t, they are often analytical mathematical ‘magic’ with buried assumptions. We take a different approach.\n\n\n\n\nProbability of Multiple Events\nSo far we have only considered \\(P(D|M)\\) where the data is a single event. However, it is more common to have multiple events (i.e. multiple data points), and we’d like to know the probability of a specific dataset vs other datasets we may have sampled. To calculate probabilities we’ll use the multiplication rule of independent events. In practice it may be very difficult to prove perfectly independent events - however be sure there is not a reason for strong correlation, such as almost all time series.\n\nNumerical Stability\nWe should also mention for many small probabilities, multiplication risks numerical instability. However there is one very simple and clever workaround to this, which is to add log probabilities instead:\n\\[\nP(A \\cap B \\cap C \\cap \\dots) = P(A) \\cdot P(B) \\cdot P(C) \\cdot \\dots = 10^{(\\log_{10}(P(A)) + \\log_{10}(P(B)) + \\log_{10}(P(C)) + \\dots))}\n\\]\n\n\nExample\nLet’s work through a couple example calculations before extending it to a scenario that approximates more serious problems we’ll tackle later. What is the probability of rolling three dice out of a cup, where the total is 11, and then rolling the dice again, where the total is 7? We use the app above to get the probabilities of each roll, where 11 = 0.1250 and 7 = 0.0694.\n\\[\nP(Roll 1 \\cap Roll 2) = 0.1250 \\cdot 0.0694 = 0.008675 = 10^{(\\log_{10}(0.1250) + \\log_{10}(0.0694))}\n\\]\nIf we stay in log probabilities, as that is generally more convenient:\n\\[\n\\log_{10}(P(Roll 1 \\cap Roll 2)) = \\log_{10}(0.1250) + \\log_{10}(0.0694) = -0.9031 + -1.1586 = -2.0617\n\\]\n\n\n\nCalculating the Relative Probability of Multiple Events\nWith the background from the last section we can now simulate the probability of multiple events. However, often it is more useful to think about the relative probability of a long series of events rather than the absolute probability, since the absolute probability will be tiny. I think an example will help. Again, to have better intuition about the problem, we’ll use dice, although it can obviously be extended to other discrete probability distributions.\nWe’ll extend the recent example of rolling three dice out of a cup. We’ll let most of our simulations use fair dice, and we’ll show the log probability of each exact series of events as we perform rolls out of the cup. Note that for three dice, the values can range from 3 to 18, and values near the middle, such as 10, will be more common than those at the end, such as 3 or 18. The log probability of more rare events will be more negative. If we have a series of very unlikely events, the values will become negative more quickly.\nWe’re also going to include one series of rolls with a cup of three weighted dice, which will be the dotted blue line. The values and probabilities will be 1=0.10, 2=0.10, 3=0.15, 4=0.15, 5=0.2, 6=0.3. If we expect the outcomes of fair dice, we’ll see that the weighted dice will usually trend towards being a fairly improbable series of rolls. We can refer to the percentile of their log-probability to see how unusual they are.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Utility Functions ---\n\ndef roll_three_dice_fair():\n    \"\"\"Simulate rolling three fair dice and return their sum.\"\"\"\n    return np.sum(np.random.randint(1, 7, size=3))\n\ndef roll_three_dice_weighted():\n    \"\"\"Simulate rolling three weighted dice and return their sum.\"\"\"\n    weights = [0.10, 0.10, 0.15, 0.15, 0.2, 0.30]\n    # Define possible outcomes\n    outcomes = [1, 2, 3, 4, 5, 6]\n    # Roll three weighted dice\n    return np.sum(np.random.choice(outcomes, size=3, p=weights))\n\ndef get_probability_of_sum(dice_sum):\n    \"\"\"Calculate probability of getting a specific sum with three fair dice.\"\"\"\n    possibilities = 0\n    total_outcomes = 216  # 6^3 possible outcomes\n    \n    for i in range(1, 7):\n        for j in range(1, 7):\n            for k in range(1, 7):\n                if i + j + k == dice_sum:\n                    possibilities += 1\n                        \n    return possibilities / total_outcomes\n\ndef get_probability_of_sum_weighted(dice_sum):\n    # We want to pretend it has the same probability and see how much of an outlier weighted dice are\n    return get_probability_of_sum(dice_sum)\n\ndef simulate_rolls_fair(num_rolls=10):\n    \"\"\"Simulate a series of rolls with fair dice and calculate cumulative log probability.\"\"\"\n    rolls = [roll_three_dice_fair() for _ in range(num_rolls)]\n    probabilities = [get_probability_of_sum(roll) for roll in rolls]\n    \n    # To handle log(0), replace zero probabilities with a very small number\n    probabilities = [p if p &gt; 0 else 1e-10 for p in probabilities]\n    log_probs = np.log10(probabilities)\n    cumulative_log_probs = np.cumsum(log_probs)\n    \n    return rolls, cumulative_log_probs\n\ndef simulate_rolls_weighted(num_rolls=10):\n    \"\"\"Simulate a series of rolls with weighted dice and calculate cumulative log probability.\"\"\"\n    rolls = [roll_three_dice_weighted() for _ in range(num_rolls)]\n    probabilities = [get_probability_of_sum_weighted(roll) for roll in rolls]\n    \n    # To handle log(0), replace zero probabilities with a very small number\n    probabilities = [p if p &gt; 0 else 1e-10 for p in probabilities]\n    log_probs = np.log10(probabilities)\n    cumulative_log_probs = np.cumsum(log_probs)\n    \n    return rolls, cumulative_log_probs\n\n# -------------------------- UI Definition ---------------------------\napp_ui = ui.page_fluid(\n    ui.h2(\"Three-Dice Roll Simulations with Cumulative Log Probability\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_select(\n                \"selectedSim\", \n                \"Select Simulation to Highlight\", \n                choices=[],   # initially empty, will be updated dynamically\n                multiple=False,\n            ),\n            ui.output_text(\"selectedDetails\"),\n            ui.hr(),\n            # Removed the Number of Simulations slider\n            # Set Number of Simulations to 100 for fair and 5 for weighted\n            # Retain Number of Rolls slider\n            ui.input_slider(\"numRolls\", \"Number of Rolls per Simulation\", min=5, max=100, value=10, step=1),\n            ui.input_action_button(\"runSim\", \"Run Simulations\")\n        ),\n        ui.output_plot(\"probPlot\", height=\"500px\"),\n    )\n)\n\n# -------------------------- Server Definition -------------------------\ndef server(input, output, session):\n    # Reactive value to store simulations as a list of tuples: (label, cum_log_probs)\n    simulations = reactive.Value([])\n    \n    # Reactive: Perform simulations when 'Run Simulations' button is clicked\n    @reactive.Effect\n    def _run_simulations():\n        input.runSim()  # Depend on the runSim button\n        num_rolls = input.numRolls()\n        new_simulations = []\n        \n        # Run 99 fair simulations (99 + 1 unfair = 100)\n        num_sim_fair = 99\n        for i in range(num_sim_fair):\n            _, cum_probs = simulate_rolls_fair(num_rolls)\n            label = f\"Simulation {i+1}\"\n            new_simulations.append((label, cum_probs))\n        \n        # Run 1 weighted simulations (was 5)\n        num_sim_weighted = 1\n        for i in range(num_sim_weighted):\n            _, cum_probs = simulate_rolls_weighted(num_rolls)\n            label = f\"Weighted Simulation {i+1}\"\n            new_simulations.append((label, cum_probs))\n        \n        simulations.set(new_simulations)\n        \n        # Update the select input choices\n        sim_choices = [sim[0] for sim in new_simulations]\n        ui.update_select(\n            \"selectedSim\",\n            choices=sim_choices,\n            selected=sim_choices[0] if sim_choices else \"\"\n        )\n    \n    # Initialize simulations on app start\n    @reactive.Effect\n    def _initialize():\n        input.runSim()  # Trigger initial simulation run\n    \n    # Plot the simulations, highlighting the selected one\n    @output\n    @render.plot\n    def probPlot():\n        sims = simulations()\n        num_sim = len(sims)\n        if num_sim == 0:\n            fig, ax = plt.subplots()\n            ax.text(0.5, 0.5, \"No simulations to display.\\nClick 'Run Simulations' to start.\", \n                    horizontalalignment='center', verticalalignment='center', fontsize=12)\n            ax.axis('off')\n            return fig\n        \n        num_rolls = len(sims[0][1])\n        x = np.arange(1, num_rolls + 1)\n        \n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot all fair simulations in light gray\n        for label, probs in sims:\n            if label.startswith(\"Simulation \"):\n                ax.plot(x, probs, color='#D3D3D3', alpha=0.5)\n        \n        # Plot all weighted simulations in blue\n        for label, probs in sims:\n            if label.startswith(\"Weighted Simulation \"):\n                ax.plot(x, probs, color='blue', alpha=0.7, linestyle='--')\n        \n        # Highlight the selected simulation\n        selected = input.selectedSim()\n        if selected:\n            try:\n                # Find the simulation by label\n                selected_sim = next((sim for sim in sims if sim[0] == selected), None)\n                if selected_sim:\n                    label, selected_probs = selected_sim\n                    color = 'red' if label.startswith(\"Simulation \") else 'green'\n                    linestyle = '-' if label.startswith(\"Simulation \") else '--'\n                    ax.plot(x, selected_probs, color=color, linewidth=2.5, linestyle=linestyle, label=label)\n            except (IndexError, ValueError):\n                pass\n        \n        ax.set_xlabel('Number of Rolls')\n        ax.set_ylabel('Cumulative Log10 Probability')\n        ax.set_title('Cumulative Log Probability of Multiple Three-Dice Roll Simulations')\n        ax.grid(True)\n        if selected:\n            ax.legend()\n        \n        plt.tight_layout()\n        return fig\n    \n    # Display details of the selected simulation, including percentile\n    @output\n    @render.text\n    def selectedDetails():\n        sims = simulations()\n        selected = input.selectedSim()\n        if not selected:\n            return \"No simulation selected.\"\n        try:\n            # Find the simulation by label\n            selected_sim = next((sim for sim in sims if sim[0] == selected), None)\n            if not selected_sim:\n                return \"Selected simulation not found.\"\n            label, cum_probs = selected_sim\n            # Get the final cumulative log probability\n            final_log_prob = cum_probs[-1]\n            # Collect all final cumulative log probabilities\n            all_final_log_probs = [sim[1][-1] for sim in sims]\n            # Compute percentile\n            percentile = (np.sum(np.array(all_final_log_probs) &lt;= final_log_prob) / len(all_final_log_probs)) * 100\n            return (\n                f\"{label}\\n\\n\"\n                f\"Final Cumulative Log10 Probability: {final_log_prob:.4f}\\n\"\n                f\"Percentile: {percentile:.2f}th\"\n            )\n        except (IndexError, ValueError):\n            return \"Invalid selection.\"\n\napp = App(app_ui, server)\nYou may notice that it’s easier to spot the weighted dice with a longer series of rolls. In just a couple of rolls they often don’t seem that unusual, but most of the time over 100 roles they have a markedly different slope than the other roles. When we calculate the percentile of the weighted dice, we are approximating the p-value of a hypothesis test. The p-value essentially says, if you assume your model is correct, here is the probability of witnessing data as, or more, extreme than what you witnessed. If the p-value becomes small enough, you may suspect it’s not bad luck, but that the data is being generated by a different model.\n\n\n\n\n\n\nNote\n\n\n\nTraditional hypothesis testing starts with a null model which we can think of as the status quo. If the data has less than a (traditionally) 0.05 probability of being generated by that null model, we assume that something other than the null model generated the data.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Discrete Probability Distributions"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#welcome",
    "href": "1-intro.html#welcome",
    "title": "Introduction",
    "section": "",
    "text": "It’s not uncommon for an engineer to find themselves drawn into the world of statistics. Fortunately the people are wonderful but unfortunately the welcome party is an awfully confusing mishmash of mathematics that all seem important but unrelated. Good luck sorting it all out in less than a few hundred hours of effort. This primer is an opinionated attempt to bypass the ‘confusing welcome’ and quickly give you a trunk of understanding from which you can later efficiently construct the branches and leaves of knowledge.\nThe premise of this primer is that an engineer should think of statistics as a loop involving just two processes:\n\nEstimating the probability of data given a model of a data generating process.\nEstimating the likelihood of a model, given the data.\n\nWe will refer to the real-world process of interest as the data generating process, and we will attempt to summarize it mathematically with a model. When we work in the ‘forward’ direction, where the model is assumed constant, we will find the probability of the data. When we work in the ‘reverse’ direction, where we are trying to find the best model given the data, we will call it the likelihood of the model.\nThese statements are summarized mathematically in Equation 1 and Equation 2, where D represents data and M represents a model.\n\\[\nP(D \\mid M)\n\\tag{1}\\]\n\\[\n\\mathcal{L}(M \\mid D)\n\\tag{2}\\]\nThe equations above can be read as ‘probability of the data given the model’ and ‘likelihood of the model given the data’.\n\n\n\n\n\n\nNote\n\n\n\nNow you may be saying to yourself, why the heck do I want to know the probability of data? A concrete example may be a set of manufacturing machines that need to be rebuilt under certain criteria. If the machine is working perfectly normally you do not want to waste the time and money to rebuild it. But how do you know if it is truly out of spec…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#models-and-data",
    "href": "1-intro.html#models-and-data",
    "title": "Introduction",
    "section": "Models and Data",
    "text": "Models and Data\nIn engineering, a model tends to spark thoughts of physics and interrelated equations. We need to broaden our mindset a bit. Engineering, mathematics, and physics education tend to focus on fundamental equations with no uncertainty… But the real world has plenty of uncertainty, and that’s probably why you’ve been sucked into the statistical void. In this world we need to acknowledge two kinds of uncertainty:\n\nAleatoric Uncertainty (Randomness)\nEpistemic Uncertainty (Lack of Knowledge)\n\nStatistics is fine with both. In fact it will usually bundle up both of them without a second thought and simply try to estimate the outcome without becoming too concerned with the underlying process. This is a fine place to start, but eventually we will work our way back to a place where we can incorporate those underlying processes into our new statistical frameworks.\n\n\n\n\n\n\nWarning\n\n\n\nThis primer is written by a practicing engineer - not a statistician or academic! It aims to be useful, not perfect, although if anything is particularly incorrect or misleading, it should definitely be corrected. Subsequently please make a comment or pull request on the github repo. Lastly, thank you for reading!!!",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#reading-order",
    "href": "1-intro.html#reading-order",
    "title": "Introduction",
    "section": "Reading Order",
    "text": "Reading Order\nThe primer can obviously be read by just plowing down the table of contents on the left side of the page. This should be the most straightforward but possibly not the most interesting way to proceed. An alternative I recommend you consider is jumping through the first chapter of each major section, then the second chapter of each major section, and so on. This gets you into the material quicker…",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#last-notes",
    "href": "1-intro.html#last-notes",
    "title": "Introduction",
    "section": "Last Notes",
    "text": "Last Notes\nIn case the point hasn’t been driven home with words and equations, I’ll round it out with a diagram:\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngraph TB\n    A[\"Data Generation\"] --&gt;|Probability&lt;br/&gt;of Data| B[\"Data\"]\n    B --&gt;|Likelihood of&lt;br/&gt;Data Generation| A\n\n\n\n\n\n\nPart 1 of this book focuses on ‘probability of the data given the model’ and Part 2 the ‘likelihood of the model given the data’.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "1-intro.html#glossary",
    "href": "1-intro.html#glossary",
    "title": "Introduction",
    "section": "Glossary",
    "text": "Glossary\nGlossary:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData Generating Process\nA theoretical model describing how observed data are produced, often using probability distributions to represent mechanisms.\nRolling a fair six-sided die generates outcomes with equal probabilities for each face (1-6).\n\n\nProbability\nA value from 0 to 1…\n\n\n\nLikelihood\n\n\n\n\nProbability Distribution\nA function that describes the likelihood of different outcomes in a random experiment.\nThe normal distribution models heights in a population with a bell-shaped curve.\n\n\nRandom Variable\nA variable whose values depend on outcomes of a random phenomenon, often defined by a probability distribution.\nThe number of heads obtained in 10 coin flips is a random variable.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html",
    "href": "2.5-bespoke_models.html",
    "title": "Bespoke/Bayesian Models",
    "section": "",
    "text": "When we talk about custom models, these are frequently associated with causal diagrams and prior information… We typically wouldn’t create a custom model if we didn’t have a causal diagram and some prior beliefs in mind, and custom models typically do not work well without causal diagrams and prior beliefs…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html#bespokebayesian",
    "href": "2.5-bespoke_models.html#bespokebayesian",
    "title": "Bespoke/Bayesian Models",
    "section": "",
    "text": "When we talk about custom models, these are frequently associated with causal diagrams and prior information… We typically wouldn’t create a custom model if we didn’t have a causal diagram and some prior beliefs in mind, and custom models typically do not work well without causal diagrams and prior beliefs…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html#all-the-models-under-the-rainbow",
    "href": "2.5-bespoke_models.html#all-the-models-under-the-rainbow",
    "title": "Bespoke/Bayesian Models",
    "section": "All the Models under the Rainbow",
    "text": "All the Models under the Rainbow\nBefore we get on with creating bespoke models… we should mention all the generic statistical and machine learning models that may meet your needs. If you are interested in the many statistical/machine learning tools, I suggest you refer to An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\nThe various statistical/machine learning tools which include generalized linear regression, decision/regression trees, and neural nets have a distinct advantage and disadvantage: 1) So long as some basic criteria are met, you just apply the model to your data. 2) So long as some basic criteria are met, you just apply the model to your data.\nThat was not a typo. The advantage is both in the speed in which you can perform an analysis as well as the simplicity in communicating how you performed the analysis. I say it is a disadvantage because you are not forced to think through your problem, especially the assumptions that each tool is making. For example, in linear regression, you have a implicit prior assumption that the error term is constant and normally distributed.\nIn this primer we will leave all these other tools to other books, and we will instead focus on techniques that allow us to mold the model to our situation. This should only be done when you have at least some knowledge about the data generation process you are modeling - but it doesn’t take much and who tries to model data for a situation they know nothing about?",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html#revisiting-the-diamond-dataset",
    "href": "2.5-bespoke_models.html#revisiting-the-diamond-dataset",
    "title": "Bespoke/Bayesian Models",
    "section": "Revisiting the Diamond Dataset",
    "text": "Revisiting the Diamond Dataset\nIn the diamond dataset there was something quite obvious to humans that is impossible for many statistical or machine learning tools to learn - the price premium when passing the 1.0, 1.5, and 2.0 carat thresholds.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "href": "2.5-bespoke_models.html#causal-model-with-1.0-1.5-and-2.0-cut-points",
    "title": "Bespoke/Bayesian Models",
    "section": "Causal Model with 1.0, 1.5, and 2.0 Cut Points",
    "text": "Causal Model with 1.0, 1.5, and 2.0 Cut Points\nIf we reason about what is influencing the price of a diamond, once we remove the artificial value that is placed for passing the 1.0, 1.5, and 2.0 carat thresholds, the value appears to be a somewhat continuous and rational combination of the relevant characteristics, specifically color, cut, clarity, and size. If we seem to iron out that one wrinkle, we could have a very good model.\nWe summarize our knowledge of the real data generating process in a causal diagram:\n\n\n\n\n\ngraph LR\n    cut --&gt; linear_model\n    color --&gt; linear_model\n    clarity --&gt; linear_model\n    size --&gt; linear_model\n    size --&gt; carat_threshold\n    linear_model --&gt; price\n    carat_threshold --&gt; price\n\n\n\n\n\n\nThis shows that while we have two mechanisms influencing diamond price. While the cut, color, clarity, and size can be combined in a linear fashion to determine price, the size is also an input to a carat threshold model that will influence price.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "2.5-bespoke_models.html#model-flexibility",
    "href": "2.5-bespoke_models.html#model-flexibility",
    "title": "Bespoke/Bayesian Models",
    "section": "Model Flexibility",
    "text": "Model Flexibility\nEngineers are typically familiar with solving systems of equations and the necessity that the system has the appropriate degrees of freedom, such that it is not overly constrained so there is no solution, and it is not underconstrained such that there are an infinite number of solutions.\nStatistical models that have too much flexibility can be optimized into all sorts of nonsensical solutions. Allowing a negative intercept in a linear regression model of the diamond dataset is one example, as it indicates customers will be paid to take small diamonds. I like to visualize this as a truss with beams/members that can contort into all sorts of nonsensical shapes. What we really want are beams/members that bend in reasonable ways.\nWe can use our knowledge of the data generating process to create reasonable priors for our model. This is simply saying that we already know some reasonable range of values, or even a most likely value, for the parameters of the model. Common examples are not letting a parameter take a negative value, or that previous experience suggest the parameter is near a certain value. Setting a prior does not mean we force a parameter value on the model, it just eliminates the impossible values and nudges the model towards our prior knowledge. If the data is different…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Bespoke/Bayesian Models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Engineers in a Hurry (an Illustrated Primer)",
    "section": "",
    "text": "The premise of this Primer is that Engineers do not need to become statisticians to get a basic understanding of what statistics is and how to use it. This means utilizing key subsets of general statistical knowledge combined with lots of computation…\nPlease use the Github repo for the book to make corrections or contributions: https://github.com/Bearsetc/SEH"
  },
  {
    "objectID": "3-parameter_estimation.html",
    "href": "3-parameter_estimation.html",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Likelihood of Model"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "href": "3-parameter_estimation.html#parameter-estimation-from-data-temporary-outline",
    "title": "Untitled",
    "section": "",
    "text": "We know something about the process, i.e. a reasonable place to start\nIt may be as simple as we know the values will not be negative",
    "crumbs": [
      "Home",
      "Likelihood of Model"
    ]
  },
  {
    "objectID": "3-parameter_estimation.html#conclusion",
    "href": "3-parameter_estimation.html#conclusion",
    "title": "Untitled",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf it’s not obvious, this is a loop, and you move to each part of the loop",
    "crumbs": [
      "Home",
      "Likelihood of Model"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html",
    "href": "2.3-linear_data_generation.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Previously our models only had parameters/settings, but now we use models that can also ingest data. We start with the most common generalized linear model, linear regression. Just like in previous chapters, we focus on generating data to understand our model. This will allow us to quickly see the result of the underlying model assumpions that we would have been likely to miss otherwise.\n(note to self that heteroscadicity is what we can fix in the diamond dataset but the step functions will wait until Bespoke Models).\nWhile we avoid model learning \\(P(M|D)\\) until the second half of the primer, we will use insights from generating data to trade up for a better model. After that we’ll be given a model that fits the data as well as possible given our model constraints, and we’ll use it to find the probability of data.\nWe round out the chapter by observing how simple transformations can allow us to apply the linear model methodology to data that otherwise appears non-linear.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#preview",
    "href": "2.3-linear_data_generation.html#preview",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Previously our models only had parameters/settings, but now we use models that can also ingest data. We start with the most common generalized linear model, linear regression. Just like in previous chapters, we focus on generating data to understand our model. This will allow us to quickly see the result of the underlying model assumpions that we would have been likely to miss otherwise.\n(note to self that heteroscadicity is what we can fix in the diamond dataset but the step functions will wait until Bespoke Models).\nWhile we avoid model learning \\(P(M|D)\\) until the second half of the primer, we will use insights from generating data to trade up for a better model. After that we’ll be given a model that fits the data as well as possible given our model constraints, and we’ll use it to find the probability of data.\nWe round out the chapter by observing how simple transformations can allow us to apply the linear model methodology to data that otherwise appears non-linear.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#parameters-vs-data",
    "href": "2.3-linear_data_generation.html#parameters-vs-data",
    "title": "Generalized Linear Models",
    "section": "Parameters vs Data",
    "text": "Parameters vs Data\nUp to this point we have been modeling processes that require only parameters to create the output distribution. We can think of parameters as the settings of the model - examples of parameters we chose earlier were the number of dice to roll at a time, or the mean and variance of a probability distribution.\nGenerally the model parameters are set/constant for any specific data generating process. However, we can have predictor variables (called features in machine learning) that tell us something useful for predicting the outcome. For example, we can model a random human’s height with no other information, but if we’re told the person is female, we can model more accurately. We can do this by adding another model parameter that responds to that data input and adjusts the mean height accordingly.\nThis touches on something we mentioned back in the introduction, Aleatoric Uncertainty (Randomness), and Epistemic Uncertainty (Lack of Knowledge). The uncertainty in most of our models is due to Epistemic Uncertainty, that is we are missing the data or the understanding in process to make an accurate prediction. We can imagine if we had enough predictor variables about a persons genetics, diet, and lifestyle, and we understood the interactions of that data, we could vastly improve a model for their height. We could expect there would still be some Aleatoric uncertainty, but it’s likely to be small in comparison.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#linear-regression",
    "href": "2.3-linear_data_generation.html#linear-regression",
    "title": "Generalized Linear Models",
    "section": "Linear Regression",
    "text": "Linear Regression\nAt some point you have probably been exposed to linear regression, and I expect you may roll your eyes for being exposed to a topic so ‘remedial’. But linear regression, and it’s generalized cousins, are an incredibly useful set of techniques in themselves, and truly understanding them is an enormous boon for understanding the entire suite of statistical and machine learning methods.\nWe’re also going to approach this in a way that is likely backwards from your previous introduction. Typically you’ll be presented with some data and the task will be to find a best fit line through the data. Often this starts with a single explanatory variable, but can it can easily be extended to many more. The actual algorithm of the best fit line is usually hand waved as some sort of magic - tackling that magic, i.e. finding the data generating process that best fits the data, is saved for the second half of this primer.\nHere we will think about it in the other direction, if we already have a linear regression model, what data does it generate? And how likely were we to see that data once generated?\n\nSingle Variable Linear Regression\nThe great thing about data generation with a model is that it is going to show you what it knows and not what you think it knows. Let’s walk through an example with a common dataset made available with the R package ggplot2 or the Python package Seaborn - it is the diamonds dataset which includes sale price among other diamond characteristics. The dataset displayed below as been made smaller via stratified sampling based on carats.\nTake a look at the data, below the app we’ll make some key points.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfrom shiny import App, ui, render, reactive\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\n\n# Set the matplotlib backend explicitly\nplt.switch_backend('agg')\n\n# Just including the data in the code for simplicity\n# It is a stratified sample based on carats\ndata_str = \"\"\"\nID,carat,cut,color,clarity,depth,table,price,x,y,z\n51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66\n34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65\n9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69\n46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67\n31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65\n40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6\n49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68\n30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61\n15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67\n32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69\n16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66\n32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69\n3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48\n16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61\n5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48\n34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67\n28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54\n32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57\n16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71\n31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63\n51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64\n27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71\n37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69\n16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69\n35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71\n30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64\n36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69\n30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69\n32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7\n35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72\n20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64\n34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69\n33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69\n32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68\n32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71\n44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68\n38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74\n43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67\n33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68\n35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72\n37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7\n33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7\n26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71\n33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7\n36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69\n36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74\n13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69\n30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74\n29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73\n30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71\n46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71\n32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7\n20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75\n34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67\n27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72\n29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7\n30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72\n31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74\n35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74\n50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73\n32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75\n29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74\n37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64\n42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75\n30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71\n23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79\n18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78\n31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78\n34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8\n13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79\n36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75\n37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77\n30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73\n36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77\n34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7\n30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9\n33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81\n26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76\n27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76\n34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74\n28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93\n39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96\n33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83\n36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9\n37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87\n41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86\n35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87\n27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83\n41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93\n37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86\n36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88\n37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92\n38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85\n35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89\n31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81\n30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96\n39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85\n30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93\n17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9\n35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88\n33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93\n39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96\n31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94\n33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01\n41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95\n42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88\n30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87\n41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89\n35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98\n24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97\n35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98\n37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98\n36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97\n28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96\n43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91\n41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99\n37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98\n40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94\n37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92\n39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98\n46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15\n38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03\n41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1\n50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13\n43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14\n35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05\n41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01\n37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1\n37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16\n38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16\n33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88\n43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09\n44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13\n32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02\n39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17\n42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05\n49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22\n39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16\n47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05\n38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05\n49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19\n39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16\n44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22\n47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19\n44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23\n49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21\n41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11\n47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15\n48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16\n41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21\n48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17\n43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26\n46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19\n43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25\n36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22\n44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18\n37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17\n48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2\n44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17\n46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16\n46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23\n50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24\n43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38\n47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25\n41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23\n42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35\n48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28\n52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23\n49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28\n52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25\n50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24\n46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22\n46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19\n45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3\n46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31\n41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35\n45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35\n43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26\n45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29\n42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24\n3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47\n51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54\n50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51\n46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32\n53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5\n1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53\n2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47\n52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43\n52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4\n52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48\n44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63\n46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4\n47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5\n2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37\n48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48\n51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57\n46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64\n49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47\n52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46\n2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51\n277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53\n809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47\n52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53\n946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56\n51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48\n48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6\n51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57\n3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59\n51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51\n694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56\n50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57\n45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48\n49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54\n2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55\n1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58\n50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55\n53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55\n48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61\n3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51\n51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53\n53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56\n7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82\n7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8\n8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79\n1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73\n53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66\n1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52\n580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67\n47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59\n1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63\n53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63\n4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74\n4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62\n1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62\n682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6\n113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87\n3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88\n9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86\n53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71\n4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61\n4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89\n9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94\n8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82\n14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97\n12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02\n3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93\n6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7\n9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87\n4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89\n10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93\n9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86\n13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75\n18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81\n3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79\n5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68\n15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98\n10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0\n5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94\n8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91\n4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8\n9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01\n10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83\n15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03\n5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86\n9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04\n15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95\n7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05\n9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94\n15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92\n11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06\n15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95\n10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89\n4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99\n18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0\n15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99\n16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0\n11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94\n16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91\n11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99\n13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95\n13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91\n9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96\n9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02\n10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05\n12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98\n15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0\n8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06\n17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03\n7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03\n16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0\n11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92\n5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03\n15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03\n7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04\n14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0\n8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01\n7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99\n21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04\n48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22\n16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01\n15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08\n18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11\n7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08\n8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07\n20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15\n13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18\n11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06\n19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0\n16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0\n15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05\n17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11\n14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1\n21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04\n4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12\n16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09\n11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05\n16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07\n21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1\n8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11\n18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13\n13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13\n20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16\n12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08\n14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22\n22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23\n8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2\n18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21\n26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22\n20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27\n12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27\n15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11\n18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15\n16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23\n10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24\n12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19\n7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21\n23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22\n20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24\n20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24\n15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23\n24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57\n26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53\n25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49\n6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36\n10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25\n14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39\n15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32\n12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32\n21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61\n22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6\n20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53\n19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52\n16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59\n19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49\n19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31\n15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36\n24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42\n16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38\n20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58\n21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29\n25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77\n24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62\n22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63\n22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55\n19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54\n20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54\n24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59\n21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65\n23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67\n19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45\n19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58\n25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53\n27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75\n27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66\n25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69\n24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64\n15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7\n25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57\n20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6\n17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58\n24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14\n24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69\n22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93\n27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2\n26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88\n21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71\n24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22\n25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98\n25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04\n26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96\n26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93\n20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56\n22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99\n27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15\n26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77\n26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24\n21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79\n24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98\n26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97\n24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93\n\"\"\"\n\n# Read the data using pandas\ndiamonds = pd.read_csv(io.StringIO(data_str))\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Diamond Price Modeling\"),\n    ui.p(\"Explore the relationship between diamond carat and price, comparing real data with model-generated data.\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"num_points\", \"Number of Simulated Points\", \n                           min=100, max=1000, value=200, step=100),\n            ui.input_slider(\"noise_level\", \"Noise Level\", \n                           min=500, max=3000, value=1503, step=100),\n            ui.input_checkbox(\"show_original\", \"Show Original Data\", True),\n            ui.input_checkbox(\"show_simulated\", \"Show Simulated Data\", True),\n            width=250\n        ),\n        ui.output_plot(\"diamondPlot\", height=\"500px\"),\n    ),\n)\n\ndef server(input, output, session):\n    \n    @reactive.Calc\n    def generate_simulated_data():\n        # Fit a linear model to the real data\n        X = diamonds['carat'].values.reshape(-1, 1)\n        y = diamonds['price'].values\n        coefficients = np.polyfit(X.flatten(), y, 1)\n        slope, intercept = coefficients\n        \n        X_new = np.linspace(diamonds['carat'].min(), diamonds['carat'].max(), input.num_points())\n        random_noise = np.random.normal(0, input.noise_level(), len(X_new))\n        Y_new = intercept + slope * X_new + random_noise\n        return pd.DataFrame({'carat': X_new, 'price': Y_new})\n\n    @output\n    @render.plot\n    def diamondPlot():\n        plt.clf()\n        fig = plt.figure(figsize=(10, 6))\n        ax = fig.add_subplot(111)\n        \n        if input.show_original():\n            ax.scatter(diamonds['carat'], diamonds['price'], \n                      alpha=0.6, color='blue', label='Original Data')\n        \n        if input.show_simulated():\n            simulated_data = generate_simulated_data()\n            ax.scatter(simulated_data['carat'], simulated_data['price'], \n                      alpha=0.6, color='orange', label='Simulated Data')\n        \n        ax.set_title('Diamond Carat vs Price: Original and Simulated Data')\n        ax.set_xlabel('Carat')\n        ax.set_ylabel('Price')\n        ax.grid(True)\n        ax.legend()\n        \n        fig.canvas.draw_idle()\n        plt.close()\n        \n        return fig\n\napp = App(app_ui, server)\n\nThere are some interesting features of the data in comparison to the data generated by the linear regression model. But we’d like to point these out first: * The observed data at small carat sizes has very little noise/variation * The observed data at large carat sizes has a lot of noise/variation * The generated data has constant noise/variation across all carat sizes\nThat the generated data has constant noise/variation across all carat sizes is an intrinsic assumption made in practically all the simple linear regression models you’ve witnessed. It is very common for it not to be true - and it has a very important negative impact, when the variation is assumed constant the larger values have far more ‘leverage’ than they really should.\nHere’s a simply X-Y plot that shows us the relationship between diamond size (carats) and price:\n\n\n\n\n\n\n\n\n\nWe’re going to fit the data with a linear regression model, it will have this mathematical form:\n\\[\ny_i = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, \\(\\beta_1\\) is the the slope, and \\(\\epsilon\\) is the error. The variable x will be the diamond size/carat and y will be the price.\nWe’re leaving all the model fitting until part two of this primer, but suffice it to say I fit a linear model using standard techniques and the outputs included a common metric for accuracy called R-squared with a value of about 0.84. It ranges from 0 to 1 and values close to 1 indicate the model is good at explaining the variability in the data, so this is a reasonably good model.\nYou’ve probably seen a linear regression line fit to data many times in your life, and I’m not going to bother you with one more. Instead I’m going to generate data based on what the model has ‘learned’, which I think will be more useful and interesting.\n\n\n\n\n\n\n\n\n\n\n\nModel Flaws\nOK, let’s point out a few things this model is doing wrong:\n\nThe model produces a decent number of negative values at small carats. I don’t think this is real because no one has paid me to take a diamond before.\nThe variation around the mean value is the same regardless of whether it is a 0.3 carat diamond or a 1.5 carat diamond. What’s actually happening in the data is called Heteroscedasticity, a fine English word, and actually quite common.\nThere are some important ‘break points’ in the data that a linear model is not going to capture, like the extra premium placed on a 1.00 carat diamond vs 0.99 carats.\nIt seems large diamonds, of presumably poor or average quality, are not as valuable as the model seems to think.\n\nLinear models are very interpretable, so someone familiar with them probably could have pointed out these flaws very early. However, when more variables are introduced or we use a more flexible modeling technique, data generation is going to be even more valuable.\n\n\nProbability of the Data\nWhile we can expect all models to be flawed to some extent, using a model with some very obvious flaws makes estimating the probability of data quite dubious. For example, we have some data that is 5+ standard deviations away, which is rarer than one in a million. I don’t think the data is that rare, I think the model is just not very good.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\nNEEDS MORE MATH",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#multi-variable-regression",
    "href": "2.3-linear_data_generation.html#multi-variable-regression",
    "title": "Generalized Linear Models",
    "section": "Multi Variable Regression",
    "text": "Multi Variable Regression\nThere are actually some more columns/attributes/variables in the diamond dataset we didn’t use. Often in life we know things like this exist but they are just not available. However, right now we’ll use them and see if we can improve the model.\nNow you’re thinking, well you just didn’t use a complicated enough model… Maybe throw a neural network at it. And while you aren’t all wrong - you are also on the path to the dark side.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.3-linear_data_generation.html#overfitting",
    "href": "2.3-linear_data_generation.html#overfitting",
    "title": "Generalized Linear Models",
    "section": "Overfitting",
    "text": "Overfitting\n… It’s easy to see how you can ‘overfit’ a multi-variate linear regression model, you just include too many parameters, e.g. parameters that are just noise that look good when training the model but actually hurt performance on a real ‘test’ dataset. It’s also possible to overfit a linear regression model with one ‘X’ variable. If you don’t know already I suggest you think about for a second… Alright, it’s assuming there is a relationship to X, i.e. a slope, when in reality the model just varies around a constant mean value.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "2.4-a_word_on_machine_learning.html",
    "href": "2.4-a_word_on_machine_learning.html",
    "title": "A Word on Machine Learning",
    "section": "",
    "text": "I assume most engineers think of machine learning as a sexier discipline than statistics. In reality there is a lot of blur/overlap from one to the other. Since the reader of this primer will inevitably be interested in machine learning, I think it’s best to include a little perspective on it, even if we will generally avoid the topic otherwise.\nDue to the amount of blur/overlap, this may not be a universally agreed distinction, but I define an important difference this way:\nThe confusing thing is that a lot of methods known as being machine learning have loss functions that are purely probabilistic - but the key is they do not have to be.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "A Word on Machine Learning"
    ]
  },
  {
    "objectID": "2.4-a_word_on_machine_learning.html#loss-function",
    "href": "2.4-a_word_on_machine_learning.html#loss-function",
    "title": "A Word on Machine Learning",
    "section": "Loss Function",
    "text": "Loss Function\nA loss function, denoted as \\(L\\), is typically computed as the difference between the true target values \\(y\\) and the predicted values \\(\\hat{y}\\) The generic form of a loss function can be expressed as:\n\\[\nL(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, \\hat{y}_i)\n\\]\n\nVariables:\n\n\\(y_i\\): The true value for the (i)-th data point.\n\\(\\hat{y}_i\\): The predicted value for the (i)-th data point.\n\\(\\ell(y_i, \\hat{y}_i)\\): The individual error for the (i)-th data point, defined by a specific loss function (e.g., squared error, cross-entropy).\n\\(N\\): The total number of data points.\n\n\n\nExplanation:\n\nThe function \\(\\ell(y_i, \\hat{y}_i)\\) depends on the task (regression or classification).\n\nExample for regression: \\(\\ell(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\\) (Mean Squared Error).\nExample for classification: \\(\\ell(y_i, \\hat{y}_i) = -[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\\) (Binary Cross-Entropy).\n\nThe summation aggregates the error across all data points.\nDividing by \\(N\\) ensures the loss represents the average error.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "A Word on Machine Learning"
    ]
  },
  {
    "objectID": "2.4-a_word_on_machine_learning.html#likelihood-function",
    "href": "2.4-a_word_on_machine_learning.html#likelihood-function",
    "title": "A Word on Machine Learning",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe loss function is in contrast to using likelihood functions, which are based in probability theory. We use likelihood to find the best statistical model, specifically by finding the Maximum Likelihood Estimate (MLE).\nIt’s a bit awkward to mention Likelihood here, because we have been trying to wait until the second half of the primer in order to tackle it head on. However, you probably have an expectation that Machine Learning will be covered, at least to some degree, and it makes sense to do that while introducing all the other types of models. I’ll continue to make it awkward and just state that yes this is the difference, but to understand Likelihood we’ll have to wait to the second half…",
    "crumbs": [
      "Home",
      "Probability of Data",
      "A Word on Machine Learning"
    ]
  },
  {
    "objectID": "2.4-a_word_on_machine_learning.html#machine-learning-models",
    "href": "2.4-a_word_on_machine_learning.html#machine-learning-models",
    "title": "A Word on Machine Learning",
    "section": "Machine Learning Models",
    "text": "Machine Learning Models\nThe following are some of the most common machine learning models. Supervised learning means that there is a result or ‘y’ variable to learn from. Unsupervised learning does not have the result or ‘y’ variable, which is a little confusing - instead of trying to predict they are generally attempting some kind of categorization/clustering or simplification.\nYou may note that the first two on the list below, linear regression and logistic regression, would also firmly sit on most lists of common statistical models.\n\nSupervised Learning\n\nLinear Regression: Predicts continuous values.\nLogistic Regression: For binary or multi-class classification.\nDecision Trees: Simple, interpretable models for classification and regression.\nRandom Forest: Ensemble of decision trees for better accuracy.\nGradient Boosting (e.g., XGBoost, LightGBM): Powerful ensemble method for structured data.\nSupport Vector Machines (SVM): For classification and regression.\nNeural Networks: Flexible models, basis of deep learning.\n\n\n\nUnsupervised Learning\n\nk-Means Clustering: Groups similar data points.\nPrincipal Component Analysis (PCA): Reduces dimensionality while preserving variance.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "A Word on Machine Learning"
    ]
  },
  {
    "objectID": "2.4-a_word_on_machine_learning.html#summary",
    "href": "2.4-a_word_on_machine_learning.html#summary",
    "title": "A Word on Machine Learning",
    "section": "Summary",
    "text": "Summary\nThere are plently of resources for this topic… My suggestion is to start with ‘Statistical Learning’….",
    "crumbs": [
      "Home",
      "Probability of Data",
      "A Word on Machine Learning"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Although I already had a rough outline and some content, once I had a decent first draft of the primer’s introduction I asked ChatGPT to sketch out a table of contents. Even with the the introduction giving a very clear direction, ChatGPT’s table of contents mirrored every other statistical book I have read, and was completely different from what I was envisioning. And that’s when I began to think I may be on to something meaningful.\nIt should be no surprise that various large language models helped me write some of the more boilerplate tidbits of the primer and helped mash the code heavy parts into shape, I think that’s now just modern life. However, I grimice when folks suggest using AI to write for them - for example give it a one sentence prompt and have it crank out a 4 paragraph work e-mail. Once AI has enormous context about your life, this may make sense. However, today, it is just a way to sound like the trillions of words already written by the rest of humanity, however polished. There’s no reason to write something a large language model with no context can generate on the spot.\nI hope this helps you avoid a maze of confusion that I wandered through as I tried to grasp statistics. First as a graduate engineering student, and later as a professional engineer working in a risk management program of an energy utility. An even grander vision would be that someday, maybe I’ll be able to feed in my intro to ChatGPT, and it will spit out something much closer to my table of contents.\n-Kevin"
  },
  {
    "objectID": "3.2-with_priors.html",
    "href": "3.2-with_priors.html",
    "title": "With Priors",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#text",
    "href": "3.2-with_priors.html#text",
    "title": "With Priors",
    "section": "",
    "text": "Text.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Likelihood Calculation\"),\n\n    # Row 1: Sliders\n    ui.row(\n        ui.column(\n            4,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            4,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=1, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 10 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=10)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=n)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # When 'MLE' is clicked, update muInput and varInput to MLE estimates\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        y = data_vals()\n        n = len(y)\n        mle_mean = np.mean(y)\n        # MLE for variance uses 1/n factor\n        mle_var = np.sum((y - mle_mean)**2) / n\n        session.send_input_message(\"muInput\", {\"value\": mle_mean})\n        session.send_input_message(\"varInput\", {\"value\": mle_var})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        n = len(y)\n        if var &lt;= 0:\n            return float(\"nan\")\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * np.sum((y - mu)**2) / var\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#likelihood",
    "href": "3.2-with_priors.html#likelihood",
    "title": "With Priors",
    "section": "Likelihood",
    "text": "Likelihood\nWe have finally worked our way up to what I consider to be one of the most important topics… Likelihood…",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "3.2-with_priors.html#probability-distribution-or-uncertainty",
    "href": "3.2-with_priors.html#probability-distribution-or-uncertainty",
    "title": "With Priors",
    "section": "Probability Distribution or Uncertainty?",
    "text": "Probability Distribution or Uncertainty?\nWhile I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.\nThe second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.\nIn the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).\n\n\n\n\n\nflowchart LR\n    subgraph DGD[Data Generating Distribution]\n        subgraph UD[Uncertainty Distributions]\n            GammaMean[Gamma Distribution]\n            GammaVar[Gamma Distribution]\n        end\n        Mean[Mean]\n        Variance[Variance]\n        GammaMean --&gt; Mean\n        GammaVar --&gt; Variance\n        Mean --&gt; Normal\n        Variance --&gt; Normal\n        Normal[Normal Distribution]\n    end\n    Normal --&gt; Height[Height Observations]",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "With Priors"
    ]
  },
  {
    "objectID": "2-data_generation.html",
    "href": "2-data_generation.html",
    "title": "Untitled",
    "section": "",
    "text": "In the first half of the book,\nSelect a subchapter from the left hand menu…\nThere are two recommended ways to work your way through this primer. A) Just go in top down order, this will introduce you first to the probability of data given a model of a data generating process, and it will work into models of greater and greater complexity. B) Do the first chapter in probability of data, then do the first chapter of probability of model, then the second chapter in both, etc. This bounces between the probability of the data and the likelihood of the model.",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2-data_generation.html#notes-to-self",
    "href": "2-data_generation.html#notes-to-self",
    "title": "Untitled",
    "section": "Notes to self",
    "text": "Notes to self\n\n\n\n\n\ngraph TD\n  subgraph ModelSystem[ ]\n    Parameters --&gt;|Used by| Model\n  end\n  Data --&gt;|Feeds into| Model\n  Model --&gt;|Produces| Output",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2-data_generation.html#data-generation-processes-temporary-outline",
    "href": "2-data_generation.html#data-generation-processes-temporary-outline",
    "title": "Untitled",
    "section": "Data Generation Processes TEMPORARY OUTLINE",
    "text": "Data Generation Processes TEMPORARY OUTLINE\nTHEMES:\n\nObviously the loop of P(D|M) and P(M|D)\nComputation instead of analysis\n\nDIFFICULTIES:\n\nIt’s elegant to introduce probability distributions, linear models, etc… as all models of data generating processes.\nWe emphasis data generation and models over analytical probability distributions because nothing in real life actually follows the probability distributions\nHowever, where po\n\nPROBABILITY DISTRIBUTIONS ARE A SPECIAL SUBSET OF MODELS IN WHICH IT’S SIMPLE TO COMPUTE THE PROBABILITY GIVEN ONLY THE PARAMETERS. OTHERWISE YOU HAVE TO GENERATE AN ENTIER DATA SET.",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2-data_generation.html#note-to-self",
    "href": "2-data_generation.html#note-to-self",
    "title": "Untitled",
    "section": "Note to self",
    "text": "Note to self\nModels of wave heights could be a cool one.",
    "crumbs": [
      "Home",
      "Probability of Data"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html",
    "href": "2.2-continuous-probability-distributions.html",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "We continue with \\(P(D|M)\\), the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nLike we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#preview",
    "href": "2.2-continuous-probability-distributions.html#preview",
    "title": "Continuous Probability Distributions",
    "section": "",
    "text": "We continue with \\(P(D|M)\\), the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nLike we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#models-of-continuous-data-generating-processes",
    "href": "2.2-continuous-probability-distributions.html#models-of-continuous-data-generating-processes",
    "title": "Continuous Probability Distributions",
    "section": "Models of Continuous Data Generating Processes",
    "text": "Models of Continuous Data Generating Processes\nLike we said before - when we change the data generating process, the distribution of outcomes changes. Here we’ll examine data generating processes that create continuous data.\n\nThe Random Walk Rocket\nLet’s assume we are shooting a rocket into the sky and letting it land. We have designed a simple guidance system that will correct the rocket to fly vertically after deviating from vertical flight. However, before a correction, the rocket will have wandered slightly from its original launch point. For simplicity in modeling, we’ll assume it only wanders left and right. We also assume that the deviations in flight are totally random - i.e. there is not a tendency to always move in one of the two directions. We want to answer the question, how far away is our rocket likely to land?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Random Rocket Simulator\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"stepSize\", \"Deviation Size\", \n                           min=0.01, max=10, value=1, step=0.01),\n            ui.input_slider(\"numSteps\", \"Number of Deviations\", \n                           min=10, max=1000, value=100, step=10),\n            ui.input_slider(\"numTrials\", \"Number of Trials\", \n                           min=10, max=10000, value=100, step=10),\n            ui.input_slider(\"numBins\", \"Number of Histogram Bins\", \n                           min=5, max=100, value=30, step=5),\n        ),\n        ui.navset_tab(\n            ui.nav_panel(\"Histogram\",\n                ui.output_plot(\"distPlot\", height=\"400px\"),\n            ),\n            ui.nav_panel(\"Percentile Plot\",\n                ui.output_plot(\"percentilePlot\", height=\"400px\"),\n            ),\n        ),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def calculate_distances():\n        distances = []\n        for _ in range(input.numTrials()):\n            steps = np.random.uniform(0, 1, input.numSteps()) * input.stepSize()\n            final_position = np.sum(steps)\n            distances.append(final_position)\n        return distances\n\n    @output\n    @render.plot\n    def distPlot():\n        distances = calculate_distances()\n        fig, ax = plt.subplots()\n        \n        ax.hist(distances, bins=input.numBins(), color=\"steelblue\", edgecolor=\"black\")\n        ax.set_title(\"Distribution of Distances from Launch\")\n        ax.set_xlabel(\"Distance from Start\")\n        ax.set_ylabel(\"Frequency\")\n\n        return fig\n\n    @output\n    @render.plot\n    def percentilePlot():\n        distances = calculate_distances()\n        fig, ax = plt.subplots()\n        \n        # Calculate percentiles\n        sorted_distances = np.sort(distances)\n        percentiles = np.linspace(0, 100, len(distances))\n        \n        ax.plot(sorted_distances, percentiles, color=\"steelblue\")\n        ax.set_title(\"Percentile Plot of Distances\")\n        ax.set_xlabel(\"Distance from Start\")\n        ax.set_ylabel(\"Percentile\")\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\nEven though this is similar to some other outputs we’ve seen, we need to stress a few key points:\n\nThe output values are no longer discrete/integers as we saw previously.\nThe app now lets choose the size of the bin since there is no ‘right’ answer to the interval used.\nThere is now a percentile plot option for the type of graph generated.\nThe percentile plot shows, in a cumulative fashion, what percent of distances are closer to the start.\n\n\n\n\n\n\n\nNote\n\n\n\nThe percentile plot is a foreshadowing of another way to compute continuous probability distributions, called the cumulative distribution function. This approach can remove some confusing properties of continuous probability distributions, however, it tends to make the shape of the distribution harder to interpret as many distributions tend to look similar.\n\n\n\n\nContinuous Distributions as Models of Data Generating Processes\nThe real world is full of data generating processes, each of which has a probability distribution, and very few, if any, would match the named probability distributions used constantly in statistics. That doesn’t mean the named distributions aren’t useful - they are extremely useful approximations due to their ability to model common processes and calculate \\(P(D|M)\\) easily.\n\nThe Normal Distribution\nYou may have spotted a trend in the Random Rocket example, as well as a lot of our earlier examples. Whenever we increased the number of samples, the histograms started to look an awful lot like the well-known normal/gaussian distribution. There’s actually a theorem for that, called the the Central Limit Theorem. You can easily research the details, but we’ll summarize it by saying that any data generating process that is additive in nature tends to produce normal/gaussian distributions. And almost everything we’ve seen so far has utilized additive processes.\nTo restate slightly from our perspective of models of data generating processes, the Normal distribution takes additive processes to the limit, in which they generate the perfect ‘Bell Curve’. Normal distributions are a good approximation for modeling the variability/variation in many things. An example is modeling height - but beware it is the logical extreme - real heights cannot be perfectly Normal, because they cannot have negative values.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import norm\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"mean\", \n                \"Mean (μ)\", \n                min=-100.0, \n                max=100.0, \n                value=0.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"stddev\", \n                \"Standard Deviation (σ)\", \n                min=0.1, \n                max=50.0, \n                value=1.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Number of Trials\",\n                min=100,\n                max=10000,\n                value=1000,\n                step=100\n            ),\n            ui.input_slider(\n                \"num_bins\",\n                \"Number of Bins\",\n                min=10,\n                max=100,\n                value=30,\n                step=5\n            ),\n        ),\n        ui.output_plot(\"normPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def normal_samples():\n        mu = input.mean()\n        sigma = input.stddev()\n        size = input.num_trials()\n        return np.random.normal(mu, sigma, size)\n\n    @output\n    @render.plot\n    def normPlot():\n        samples = normal_samples()\n        \n        # Use the number of bins from the slider\n        num_bins = input.num_bins()\n        \n        # Compute histogram (both count and density)\n        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)\n        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)\n        bin_width = bin_edges[1] - bin_edges[0]\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Create the second y-axis sharing the same x-axis\n        ax2 = ax1.twinx()\n        \n        # Plot histogram with counts on left y-axis\n        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color=\"steelblue\", \n                      alpha=0.6, edgecolor=\"black\", align='center', \n                      label=f'Histogram (n={input.num_trials():,})')\n        \n        # Calculate theoretical normal distribution\n        x = np.linspace(min(samples), max(samples), 100)\n        pdf = norm.pdf(x, input.mean(), input.stddev())\n        \n        # Plot theoretical curve on right y-axis\n        line = ax2.plot(x, pdf, 'r-', lw=2, label='Normal PDF')[0]\n        \n        # Set labels and title\n        ax1.set_xlabel(\"Value\", fontsize=14)\n        ax1.set_ylabel(\"Count\", fontsize=12, color='steelblue')\n        ax2.set_ylabel(\"Density\", fontsize=12, color='red')\n        plt.title(\"Normal Distribution: Histogram and PDF\", fontsize=16)\n        \n        # Color the tick labels to match the respective plots\n        ax1.tick_params(axis='y', labelcolor='steelblue')\n        ax2.tick_params(axis='y', labelcolor='red')\n        \n        # Ensure both axes start at 0\n        ax1.set_ylim(bottom=0)\n        ax2.set_ylim(bottom=0)\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers], rotation=90)\n        \n        # Add legends for both axes\n        lines = [bars, line]\n        labels = [b.get_label() for b in lines]\n        ax1.legend(lines, labels, loc='upper left')\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\nWe’ve also included the exact values of the probability density function (PDF) to suggest that the underlying distribution is continuous.\n\n\nThe Lognormal Distribution\nData generating processes do not need to be additive though, some processes tend to multiply. These kinds of processes will create a notably different distribution, called the log-normal distribution. It has two important differences from the normal distribution:\n\nIt contains only positive values.\nIt has a very long ‘tail’ on the right hand side. Another way to describe this is skewness.\n\nIt’s worth noting that there are many real world problems where values can only be positive. This simple fact also implies that many real distributions skew towards larger values.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 550\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import lognorm\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Log-Normal Distribution Simulation with Binned Histogram\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"mu\", \n                \"Log-mean (μ)\", \n                min=-2.0, \n                max=2.0, \n                value=0.0, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"sigma\", \n                \"Log-standard deviation (σ)\", \n                min=0.1, \n                max=2.0, \n                value=0.5, \n                step=0.1\n            ),\n            ui.input_slider(\n                \"num_trials\",\n                \"Number of Trials\",\n                min=100,\n                max=10000,\n                value=1000,\n                step=100\n            ),\n            ui.input_slider(\n                \"num_bins\",\n                \"Number of Bins\",\n                min=10,\n                max=100,\n                value=30,\n                step=5\n            ),\n        ),\n        ui.output_plot(\"lognormPlot\", height=\"400px\"),\n    ),\n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def lognormal_samples():\n        mu = input.mu()\n        sigma = input.sigma()\n        size = input.num_trials()\n        return np.random.lognormal(mu, sigma, size)\n\n    @output\n    @render.plot\n    def lognormPlot():\n        samples = lognormal_samples()\n        \n        # Use the number of bins from the slider\n        num_bins = input.num_bins()\n        \n        # Compute histogram (both count and density)\n        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)\n        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)\n        bin_width = bin_edges[1] - bin_edges[0]\n        bin_centers = bin_edges[:-1] + bin_width / 2\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        # Create the second y-axis sharing the same x-axis\n        ax2 = ax1.twinx()\n        \n        # Plot histogram with counts on left y-axis\n        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color=\"steelblue\", \n                      alpha=0.6, edgecolor=\"black\", align='center', \n                      label=f'Histogram (n={input.num_trials():,})')\n        \n        # Calculate theoretical log-normal distribution\n        x = np.linspace(min(samples), max(samples), 1000)\n        pdf = lognorm.pdf(x, input.sigma(), scale=np.exp(input.mu()))\n        \n        # Plot theoretical curve on right y-axis\n        line = ax2.plot(x, pdf, 'r-', lw=2, label='Log-Normal PDF')[0]\n        \n        # Set labels and title\n        ax1.set_xlabel(\"Value\", fontsize=14)\n        ax1.set_ylabel(\"Count\", fontsize=12, color='steelblue')\n        ax2.set_ylabel(\"Density\", fontsize=12, color='red')\n        plt.title(\"Log-Normal Distribution: Histogram and PDF\", fontsize=16)\n        \n        # Color the tick labels to match the respective plots\n        ax1.tick_params(axis='y', labelcolor='steelblue')\n        ax2.tick_params(axis='y', labelcolor='red')\n        \n        # Ensure both axes start at 0\n        ax1.set_ylim(bottom=0)\n        ax2.set_ylim(bottom=0)\n        \n        # Set x-axis limits to focus on the main part of the distribution\n        upper_limit = np.percentile(samples, 99)  # Show up to 99th percentile\n        ax1.set_xlim(0, upper_limit)\n        \n        # Set x-axis ticks\n        if len(bin_centers) &gt; 20:\n            step = math.ceil(len(bin_centers) / 20)\n            ax1.set_xticks(bin_centers[::step])\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers[::step]], rotation=90)\n        else:\n            ax1.set_xticks(bin_centers)\n            ax1.set_xticklabels([f\"{x:.2f}\" for x in bin_centers], rotation=90)\n        \n        # Add legends for both axes\n        lines = [bars, line]\n        labels = [b.get_label() for b in lines]\n        ax1.legend(lines, labels, loc='upper right')\n        \n        plt.tight_layout()\n        \n        return fig\n\napp = App(app_ui, server)\nYou may have guessed that if you have a set of values from the lognormal distribution, if you take the the log of their values and replot them, you’ll end up plotting values in a normal distribution.\n\n\nSummary\nAgain we keep this section brief as there are plenty of easily accessible references for continuous probability distributions. Hopefully the point was made though - that each continuous probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcome of the process.\n\n\n\n\n\n\nNote\n\n\n\nWe’ve touched on this briefly, but we should explicitly acknowledge that the named probability distributions can be used in different ways:\n\nGenerate data from a known distribution.\nCalculate the probability of data that has come from a known distribution.\nCommuniciate uncertainty in the value of a parameter.\n\nWe haven’t seen the third bullet yet, it will eventually come up in the second half of the primer. But it is fundamentally different than the other two in that it represents knowledge, and with additional knowledge the uncertainty can become zero. This is not the same as the inherent variation in statistical processes, for example, the variation in peoples height will not become zero with more knowledge.",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "2.2-continuous-probability-distributions.html#probability-of-data",
    "href": "2.2-continuous-probability-distributions.html#probability-of-data",
    "title": "Continuous Probability Distributions",
    "section": "Probability of Data",
    "text": "Probability of Data\nWe return to the main theme of the first half of the primer, the probability of data given a model, \\(P(D|M)\\).\n\nProbability Density Function\nWe’ve been hinting that the probability density function would require some explanation, and we’ve finally come to the right place to tackle it. We’ve created histograms of our continuous data, in which we take multiple exact values and lump them together in a bin of the histogram. It’s possible to use our relative frequency technique to estimate the probability of the bin (just divide the bin count by the total count of all bins). However, how would estimate the probability of a single point within the bin? The problem is that, as we’ve noted earlier, the probability of any exact value on the real number line \\(\\mathbb{R}\\) is effectively zero.\nTo solve this, the probability density function does not give a true probability, it gives a value such that the following properties are true:\n\nThe area under the curve sums to 1, i.e. all possible values have a total probability of 1.\nThe values give the relative probability of that point vs other points.\n\n\n\nSingle Data Point Example\nAs already mentioned, referring to the probability of an exact value on the real number line is meaningless. Instead, we refer to either the relative probability of a value, or the probability of getting a value as extreme or more extreme than the value that we observed. With a single data point we use an example of the ‘as extreme or more extreme’ approach.\nIn the app below, we find where the data point lies on the chart and then find the area, i.e. probability, of sampling points larger than the observation. This is utilizing bullet one from the properties of the probability density function given above. Note that we can get this value more directly if we work from the cumulative distribution, as shown in the other plot tab.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Probability Calculator\"),\n    \n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n            ui.input_numeric(\n                \"xInput\", \"Value (x):\",\n                value=105, min=0, max=200\n            ),\n            ui.br(),\n            ui.h4(\"Probability (P(X ≥ x)):\"),\n            ui.output_text(\"probOutput\"),\n            width=300\n        ),\n        \n        # Main panel with tabset\n        ui.navset_tab(\n            ui.nav_panel(\"PDF Plot\",\n                ui.output_plot(\"normalPlot\", height=\"400px\"),\n            ),\n            ui.nav_panel(\"CDF Plot\",\n                ui.output_plot(\"cdfPlot\", height=\"400px\"),\n            ),\n        ),\n    )\n)\n\ndef server(input, output, session):\n    # Calculate probability\n    @reactive.Calc\n    def calculate_probability():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n        return 1 - stats.norm.cdf(x, mu, sigma)\n\n    # Show the probability\n    @output\n    @render.text\n    def probOutput():\n        prob = calculate_probability()\n        return f\"{prob:.4f}\"\n\n    # Plot the normal PDF with shaded area\n    @output\n    @render.plot\n    def normalPlot():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n\n        x_min = mu - 4 * sigma\n        x_max = mu + 4 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')\n        \n        x_shade = x_vals[x_vals &gt;= x]\n        y_shade = stats.norm.pdf(x_shade, mu, sigma)\n        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, \n                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')\n\n        ax.axvline(x, color='red', linestyle='--', alpha=0.5)\n\n        ax.set_title(f\"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        return fig\n\n    # Plot the CDF\n    @output\n    @render.plot\n    def cdfPlot():\n        mu = input.muInput()\n        var = input.varInput()\n        x = input.xInput()\n        sigma = math.sqrt(var)\n\n        # Generate random data\n        data = np.random.normal(mu, sigma, 1000)\n        \n        # Calculate empirical CDF\n        sorted_data = np.sort(data)\n        empirical_cdf = np.arange(1, len(data) + 1) / len(data)\n        \n        # Calculate theoretical CDF\n        x_vals = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)\n        theoretical_cdf = stats.norm.cdf(x_vals, mu, sigma)\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot empirical and theoretical CDFs\n        ax.plot(sorted_data, empirical_cdf, 'b-', label='Empirical CDF', alpha=0.7)\n        ax.plot(x_vals, theoretical_cdf, 'r--', label='Theoretical CDF')\n        \n        # Add vertical line at x\n        ax.axvline(x, color='red', linestyle='--', alpha=0.5)\n        \n        ax.set_title(f\"Cumulative Distribution Function (μ={mu:.1f}, σ²={var:.1f})\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Cumulative Probability\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        return fig\n\napp = App(app_ui, server)\nWhen we refer to probabilities of “as extreme or more extreme” values, we are effectively computing tail probabilities using integrals. For example, for a right-tailed probability beyond a point \\(c\\), we evaluate:\n\\[\nP(X \\geq c) = \\int_c^\\infty f(x) \\, dx\n\\]\nThose who prefer to avoid calculus (and what engineer doesn’t?), can simply utilize the cumulative (CDF) plot.\n\n\nMultiple Data Points Example\n\nRelative Probability\nFor multiple data points, we are interested in the relative probability of one series of events vs some other series of events. This is utilizing bullet two from the properties of the probability density function given above. In the example below we will multiply the probability densities similar to how we multiplied actual probabilities in the previous chapter on discrete probability distributions - this should given you reason to pause - the reason we can do this is that we will only use the results for relative comparison, and not as an absolute probability.\nAgain, we assume that in the series of events each event is independent. And again, this typically cannot be proven to be strictly true - however avoid the cases where it is obviously not true, such as time series. Also, we use only log probabilities and addition (in contrast with the more obvious multiplication of non-log probabilities) as they are more convenient.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Relative Probability of Normally Distributed Data\"),\n\n    # Row 1: Sliders and New Sample button\n    ui.row(\n        ui.column(\n            5,\n            ui.input_slider(\n                \"muInput\", \"Mean (μ):\",\n                min=50, max=150, value=100, step=0.1\n            ),\n        ),\n        ui.column(\n            5,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=1, max=200, value=10, step=1\n            ),\n        ),\n        ui.column(\n            2,\n            ui.br(),\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: All data and probabilities\n    ui.row(\n        ui.column(\n            4,\n            ui.h4(\"Current Data (Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            5,\n            ui.h4(\"Log Relative Probabilities:\"),\n            ui.output_text_verbatim(\"pointLogProbs\"),\n        ),\n        ui.column(\n            3,\n            ui.h4(\"Log Sum:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"normalPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize data with 5 random points\n    data_vals = reactive.Value(\n        np.random.normal(loc=100, scale=np.sqrt(10), size=5)\n    )\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        data_vals.set(\n            np.random.normal(loc=100, scale=np.sqrt(10), size=5)\n        )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        y = data_vals()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # Calculate log probability for each point\n    @reactive.Calc\n    def point_log_probs():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        if var &lt;= 0:\n            return [float(\"nan\")] * len(y)\n        \n        log_probs = []\n        for yi in y:\n            term1 = -0.5 * math.log(2 * math.pi * var)\n            term2 = -0.5 * ((yi - mu)**2) / var\n            log_probs.append(term1 + term2)\n        return log_probs\n\n    # Display individual log probabilities\n    @output\n    @render.text\n    def pointLogProbs():\n        probs = point_log_probs()\n        return \", \".join(f\"{p:.2f}\" for p in probs)\n\n    # Reactive expression for total log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        return sum(point_log_probs())\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the normal PDF and data points\n    @output\n    @render.plot\n    def normalPlot():\n        y = data_vals()\n        mu = input.muInput()\n        var = input.varInput()\n        sigma = math.sqrt(var)\n\n        x_min = min(y) - 3 * sigma\n        x_max = max(y) + 3 * sigma\n        x_vals = np.linspace(x_min, x_max, 200)\n        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(\n            -0.5 * ((x_vals - mu) / sigma)**2\n        )\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n        ax.plot(\n            x_vals, pdf_vals,\n            color=\"blue\",\n            label=f\"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})\"\n        )\n\n        # Scatter the data at y=0 with some jitter\n        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))\n        ax.scatter(jittered, np.zeros_like(y), color=\"darkgreen\", alpha=0.7, label=\"Data points\")\n\n        ax.axvline(mu, color=\"gray\", linestyle=\"--\")\n        ax.set_title(\"Normal PDF vs. Observed Data\")\n        ax.set_xlabel(\"Y\")\n        ax.set_ylabel(\"Density\")\n        ax.legend()\n        ax.set_ylim(bottom=0)\n\n        return fig\n\napp = App(app_ui, server)\nAs you adjust the parameters of the probability distribution, the relative probability of seeing the data changes. When the probability distribution is far away from the points, we see the sum of the probabilities becomes smaller (less likely). When the probability distribution is near the points, the sum gets larger, showing it is comparitively more likely.\n\n\n\n\n\n\nWarning\n\n\n\nI’ve reserved the use of likelihood until the second half of this primer during which we’ll find the best model based on the data from the data generating process. However, in other texts you will also find descriptions of the relative probability of continuous distributions called the relative likelihood.\n\n\n\n\nApproximate P-Value\nIn the following app, there will be 100 samples of a series of 10 events. Each of those will have been generated from a Normal distribution with a mean of 100 and a variance of 10. You can then create your own series of 10 events, sampled from a Normal distribution with a mean and variance of your choosing. The relative probability of your series of 10 events will be calculated as if they had come from a distribution with a mean of 100 and a variance of 10. The percentile shown will indicate how unusual your series of events appears to be.\nLike the similar app for discrete distributions, this is an approximation of your data sets p-value. If your dataset is in a reasonable percentile, you have little evidence to assume it was not created by the null model with mean of 100 and variance of 10. If it is at an extreme percentile, you may reasonably suspect it was generated by a different data generating process.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 800\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Normal Distribution Approximate P-Value Calculator\"),\n    ui.p(\"Generate your own series of 10 events from a Normal distribution with your chosen mean and variance. The probability will be calculated assuming the events came from a Normal(μ=100, σ²=10) distribution.\"),\n\n    # Row 1: Sliders and New Sample button\n    ui.row(\n        ui.column(\n            5,\n            ui.input_slider(\n                \"muInput\", \"Your Mean (μ):\",\n                min=80, max=120, value=95, step=0.1\n            ),\n        ),\n        ui.column(\n            5,\n            ui.input_slider(\n                \"varInput\", \"Your Variance (σ²):\",\n                min=1, max=100, value=20, step=1\n            ),\n        ),\n        ui.column(\n            2,\n            ui.br(),\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLES\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Data and percentile\n    ui.row(\n        ui.column(\n            9,\n            ui.h4(\"Your Data:\"),\n            ui.output_text_verbatim(\"dataText1\"),\n        ),\n        ui.column(\n            3,\n            ui.h4(\"Percentile:\"),\n            ui.output_text(\"percentileOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"cumulativePlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Initialize user's data and reference datasets\n    data_vals1 = reactive.Value(None)\n    reference_data = reactive.Value(None)\n\n    # Generate new samples when parameters change or button is pressed\n    @reactive.Effect\n    @reactive.event(input.muInput, input.varInput, input.newSampleBtn)\n    def _():\n        data_vals1.set(\n            np.random.normal(loc=input.muInput(), scale=np.sqrt(input.varInput()), size=10)\n        )\n        reference_data.set(\n            [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]\n        )\n\n    # Initial data generation\n    @reactive.Effect\n    def _():\n        if data_vals1() is None:\n            data_vals1.set(\n                np.random.normal(loc=95, scale=np.sqrt(20), size=10)\n            )\n        if reference_data() is None:\n            reference_data.set(\n                [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]\n            )\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText1():\n        y = data_vals1()\n        return \", \".join(str(round(val, 1)) for val in y)\n\n    # Calculate cumulative log probabilities\n    def calc_cum_log_probs(data, mu=100, var=10):  # Default parameters set to true distribution\n        cum_probs = []\n        running_sum = 0\n        \n        for yi in data:\n            term1 = -0.5 * math.log(2 * math.pi * var)\n            term2 = -0.5 * ((yi - mu)**2) / var\n            running_sum += (term1 + term2)\n            cum_probs.append(running_sum)\n            \n        return cum_probs\n\n    # Calculate and show the percentile\n    @output\n    @render.text\n    def percentileOutput():\n        user_final_prob = calc_cum_log_probs(data_vals1())[-1]\n        ref_final_probs = [calc_cum_log_probs(ref_data)[-1] \n                          for ref_data in reference_data()]\n        percentile = sum(1 for x in ref_final_probs if x &lt; user_final_prob) / len(ref_final_probs) * 100\n        return f\"{percentile:.1f}%\"\n\n    # Plot the cumulative log probabilities\n    @output\n    @render.plot\n    def cumulativePlot():\n        # Calculate probabilities for user's data\n        user_probs = calc_cum_log_probs(data_vals1())\n        \n        # Calculate probabilities for reference data\n        ref_probs_list = [calc_cum_log_probs(data) \n                         for data in reference_data()]\n\n        events = range(1, 11)  # 10 events\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        \n        # Plot reference lines in gray\n        for ref_probs in ref_probs_list:\n            ax.plot(events, ref_probs, 'gray', alpha=0.1, linewidth=1)\n\n        # Plot user's line in red\n        ax.plot(events, user_probs, 'r-', label='Your samples', \n                linewidth=2, alpha=1.0)\n\n        # Add a dummy plot for the reference distribution legend\n        ax.plot([], [], 'gray', alpha=0.5, label='100 samples from N(100,10)')\n\n        ax.set_title(\"Cumulative Log Probability vs. Number of Events\")\n        ax.set_xlabel(\"Number of Events\")\n        ax.set_ylabel(\"Cumulative Log Probability\")\n        ax.grid(True, alpha=0.3)\n        ax.legend()\n\n        return fig\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Probability of Data",
      "Continuous Probability Distributions"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html",
    "href": "3.x-reverse_it.html",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "href": "3.x-reverse_it.html#probability-of-a-data-generating-process",
    "title": "Test of new Dice Rolling App",
    "section": "",
    "text": "If we assume a certain model of a data generating process - but it never seems to generate any data close the what is observed from the real data generating process, we can conclude that the model is bad.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nfrom shiny import App, ui, render, reactive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Dice Rolling Demo\"),\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_slider(\"numDice\", \"Number of Dice\", min=1, max=10, value=2, step=1),\n            ui.input_slider(\"numRolls\", \"Number of Rolls\", min=1, max=10000, value=100, step=1),\n        ),\n        ui.output_plot(\"dicePlot\", height=\"400px\"),\n    ),\n    ui.row(\n        ui.column(4,\n            ui.input_select(\"selectedTotal\", \"Select Dice Total\", choices=[\"\"])\n        ),\n        ui.column(8,\n            ui.output_text(\"probability\")\n        )\n    )\n)\n\ndef server(input, output, session):\n    # Define a reactive calculation that depends on numDice and numRolls\n    @reactive.Calc\n    def dice_sums():\n        return [\n            np.random.randint(1, 7, input.numDice()).sum()\n            for _ in range(input.numRolls())\n        ]\n\n    # Update the choices in the select input based on the current dice sums\n    @reactive.Effect\n    def _():\n        current_sums = dice_sums()\n        unique_sums = sorted(np.unique(current_sums))\n        ui.update_select(\n            \"selectedTotal\",\n            choices=[str(s) for s in unique_sums],\n            selected=str(unique_sums[0]) if len(unique_sums) &gt; 0 else \"\"\n        )\n\n    @output\n    @render.plot\n    def dicePlot():\n        current_sums = dice_sums()\n        fig, ax = plt.subplots()\n\n        unique_sums, counts = np.unique(current_sums, return_counts=True)\n        ax.bar([str(s) for s in unique_sums], counts, color=\"steelblue\")\n\n        ax.set_title(\"Frequency of Dice Totals\")\n        ax.set_xlabel(\"Dice Total\")\n        ax.set_ylabel(\"Frequency\")\n        plt.xticks(rotation=90)\n\n        return fig\n\n    @output\n    @render.text\n    def probability():\n        if not input.selectedTotal():\n            return \"\\nPlease select a dice total\"\n        \n        current_sums = dice_sums()\n        selected_total = int(input.selectedTotal())\n        count = sum(1 for x in current_sums if x == selected_total)\n        prob = count / len(current_sums)\n        \n        return f\"\\nApproximate probability of rolling a total of {selected_total}: {prob:.4f}\"\n\napp = App(app_ui, server)",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Test of new Dice Rolling App"
    ]
  },
  {
    "objectID": "3.3-linear-regression.html",
    "href": "3.3-linear-regression.html",
    "title": "Test of new Linear Regression App",
    "section": "",
    "text": "Text description here.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, ui, reactive, render\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Linear Regression Likelihood\"),\n\n    # Row 1: Sliders for alpha, beta, sigma^2, and n\n    ui.row(\n        ui.column(\n            3,\n            ui.input_slider(\n                \"alphaInput\", \"Intercept (α):\",\n                min=-10, max=10, value=0, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"betaInput\", \"Slope (β):\",\n                min=-5, max=5, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"varInput\", \"Variance (σ²):\",\n                min=0.1, max=10, value=1, step=0.1\n            ),\n        ),\n        ui.column(\n            3,\n            ui.input_slider(\n                \"nInput\", \"Number of samples:\",\n                min=5, max=100, value=10, step=1\n            ),\n        ),\n    ),\n\n    ui.br(),\n\n    # Row 2: Buttons, current data, and log-likelihood\n    ui.row(\n        ui.column(\n            2,\n            ui.input_action_button(\"mleBtn\", \"MLE\"),\n        ),\n        ui.column(\n            2,\n            ui.input_action_button(\"newSampleBtn\", \"NEW SAMPLE\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Current Data (X, Y):\"),\n            ui.output_text_verbatim(\"dataText\"),\n        ),\n        ui.column(\n            4,\n            ui.h4(\"Log-Likelihood:\"),\n            ui.output_text(\"llOutput\"),\n        ),\n    ),\n\n    ui.br(),\n\n    # Plot\n    ui.output_plot(\"regressionPlot\", height=\"400px\"),\n)\n\ndef server(input, output, session):\n    # Reactive value to store X and Y\n    data_vals = reactive.Value(None)\n\n    # Function to generate linear-regression data\n    def generate_data(n, alpha, beta, var):\n        # For simplicity, let X be a random uniform(0, 10)\n        X = np.random.uniform(0, 10, size=n)\n        # Y = alpha + beta*X + noise\n        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)\n        return X, Y\n\n    # Initialize data once\n    data_vals.set(generate_data(10, 0, 1, 1))\n\n    # Generate a new sample when 'NEW SAMPLE' is pressed\n    @reactive.Effect\n    @reactive.event(input.newSampleBtn)\n    def _():\n        n = input.nInput()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        data_vals.set(generate_data(n, alpha, beta, var))\n\n    # Display the current data\n    @output\n    @render.text\n    def dataText():\n        X, Y = data_vals()\n        # Show a few decimal places\n        pairs_str = [\n            f\"({round(xi,1)}, {round(yi,1)})\" for xi, yi in zip(X, Y)\n        ]\n        return \", \".join(pairs_str)\n\n    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var\n    @reactive.Effect\n    @reactive.event(input.mleBtn)\n    def _():\n        X, Y = data_vals()\n        n = len(Y)\n\n        # Compute MLE (which in classical linear regression is the OLS solution)\n        X_mean = np.mean(X)\n        Y_mean = np.mean(Y)\n\n        # beta_hat = Cov(X,Y)/Var(X)\n        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)\n\n        # alpha_hat = mean(Y) - beta_hat*mean(X)\n        alpha_hat = Y_mean - beta_hat*X_mean\n\n        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)\n        residuals = Y - (alpha_hat + beta_hat*X)\n        var_hat = np.sum(residuals**2) / n\n\n        # Update the UI sliders\n        session.send_input_message(\"alphaInput\", {\"value\": alpha_hat})\n        session.send_input_message(\"betaInput\", {\"value\": beta_hat})\n        session.send_input_message(\"varInput\", {\"value\": var_hat})\n\n    # Reactive expression for log-likelihood\n    @reactive.Calc\n    def log_likelihood():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n        n = len(Y)\n\n        if var &lt;= 0:\n            return float(\"nan\")\n\n        # Compute sum of squared residuals\n        residuals = Y - (alpha + beta*X)\n        ssr = np.sum(residuals**2)\n\n        # log-likelihood for linear regression\n        term1 = -0.5 * n * math.log(2 * math.pi * var)\n        term2 = -0.5 * (ssr / var)\n        return term1 + term2\n\n    # Show the log-likelihood\n    @output\n    @render.text\n    def llOutput():\n        ll = log_likelihood()\n        return str(round(ll, 2))\n\n    # Plot the data and the regression line\n    @output\n    @render.plot\n    def regressionPlot():\n        X, Y = data_vals()\n        alpha = input.alphaInput()\n        beta = input.betaInput()\n        var = input.varInput()\n\n        fig, ax = plt.subplots(figsize=(6, 4))\n\n        # Plot data points\n        ax.scatter(X, Y, color=\"blue\", alpha=0.7, label=\"Data\")\n\n        # Plot regression line from min(X) to max(X)\n        x_min, x_max = np.min(X), np.max(X)\n        x_vals = np.linspace(x_min, x_max, 100)\n        y_vals = alpha + beta * x_vals\n        ax.plot(x_vals, y_vals, color=\"red\", label=f\"Line (α={round(alpha,2)}, β={round(beta,2)})\")\n\n        ax.set_title(\"Linear Regression Fit\")\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.legend()\n        ax.grid(True)\n\n        return fig\n\napp = App(app_ui, server)\n\nOrdinary Least Squares via Analytical Solution\nThe closed-form solution for linear regression minimizes the sum of squared residuals directly. Given a dataset with feature matrix X and target vector y :\n\\(\\beta = (X^T X)^{-1} X^T y\\)\nWe can directly compute the coefficients, however, we are assuming normally distributed errors…\n\n\nOther Methods\nA model of our data generating process will essentially try to predict our response variable.",
    "crumbs": [
      "Home",
      "Likelihood of Model",
      "Test of new Linear Regression App"
    ]
  }
]