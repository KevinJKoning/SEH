<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>With Priors – Statistics for Engineers in a Hurry (an Illustrated Primer)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e8e9342e845377dfc7513ec3bb59cd67.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<meta name="shinylive:serviceworker_dir" content=".">
<script src="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/load-shinylive-sw.js" type="module"></script>
<script src="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/run-python-blocks.js" type="module"></script>
<link href="site_libs/quarto-contrib/shinylive-0.9.3/shinylive/shinylive.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/shinylive-quarto-css/shinylive-quarto.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Statistics for Engineers in a Hurry (an Illustrated Primer)</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-parameter_estimation.html">Likelihood of Model</a></li><li class="breadcrumb-item"><a href="./3.2-with_priors.html">With Priors</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./2-data_generation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.1-discrete_probability_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discrete Probability Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.2-continuous-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Continuous Probability Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.3-statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2.4-machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./3-parameter_estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Likelihood of Model</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.1-without_priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Without Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3.2-with_priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">With Priors</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preview" id="toc-preview" class="nav-link active" data-scroll-target="#preview">Preview</a></li>
  <li><a href="#why-use-priors" id="toc-why-use-priors" class="nav-link" data-scroll-target="#why-use-priors">Why Use Priors?</a>
  <ul class="collapse">
  <li><a href="#posterior-distribution" id="toc-posterior-distribution" class="nav-link" data-scroll-target="#posterior-distribution">Posterior Distribution</a></li>
  <li><a href="#probabilities-about-probabilities" id="toc-probabilities-about-probabilities" class="nav-link" data-scroll-target="#probabilities-about-probabilities">Probabilities about Probabilities</a></li>
  </ul></li>
  <li><a href="#the-argument-for-and-against-priors" id="toc-the-argument-for-and-against-priors" class="nav-link" data-scroll-target="#the-argument-for-and-against-priors">The Argument For and Against Priors</a></li>
  <li><a href="#the-math-of-bayesian-updating" id="toc-the-math-of-bayesian-updating" class="nav-link" data-scroll-target="#the-math-of-bayesian-updating">The Math of Bayesian Updating</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">…</a></li>
  <li><a href="#maximum-likelihood-estimation-mle-vs.-maximum-a-posteriori-map" id="toc-maximum-likelihood-estimation-mle-vs.-maximum-a-posteriori-map" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-mle-vs.-maximum-a-posteriori-map">Maximum Likelihood Estimation (MLE) vs.&nbsp;Maximum A Posteriori (MAP)</a>
  <ul class="collapse">
  <li><a href="#low-dimensional-search-grid" id="toc-low-dimensional-search-grid" class="nav-link" data-scroll-target="#low-dimensional-search-grid">Low Dimensional Search (Grid)</a></li>
  <li><a href="#high-dimensional-search-mcmc" id="toc-high-dimensional-search-mcmc" class="nav-link" data-scroll-target="#high-dimensional-search-mcmc">High Dimensional Search (MCMC)</a></li>
  <li><a href="#parameter-find-app" id="toc-parameter-find-app" class="nav-link" data-scroll-target="#parameter-find-app">Parameter Find App</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#text" id="toc-text" class="nav-link" data-scroll-target="#text">Text</a></li>
  <li><a href="#old-thoughts" id="toc-old-thoughts" class="nav-link" data-scroll-target="#old-thoughts">Old Thoughts</a></li>
  <li><a href="#probability-distribution-or-uncertainty" id="toc-probability-distribution-or-uncertainty" class="nav-link" data-scroll-target="#probability-distribution-or-uncertainty">Probability Distribution or Uncertainty?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-parameter_estimation.html">Likelihood of Model</a></li><li class="breadcrumb-item"><a href="./3.2-with_priors.html">With Priors</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">With Priors</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="preview" class="level2">
<h2 class="anchored" data-anchor-id="preview">Preview</h2>
<p>We finally come to the last chapter, which is focused on finding the <span class="math inline">\(\mathcal{L}(M \mid D)\)</span> with prior values for the parameters.</p>
<p>In more statistical terms, this chapter utilizes a <strong>Bayesian</strong> perspective. We will introduce Bayes Theorem later…</p>
</section>
<section id="why-use-priors" class="level2">
<h2 class="anchored" data-anchor-id="why-use-priors">Why Use Priors?</h2>
<p>Let’s say we are flipping a coin that, upon basic examination, appears to be fair (produces 50% heads). However, let’s say the first three flips are: Heads, Heads, and Heads. Now what should we estimate the true propability of heads to be after the three flips?</p>
<ul>
<li>In the Frequentist view (under which most of the primer has operated), the probability of heads with the maximum likelihood is simply P=1, i.e.&nbsp;we expect to always get heads.</li>
<li>In the Bayesian view, the probability of heads is greater than 0.5 and less than 1, depending on how strongly you had a prior belief that the coin was fair.</li>
</ul>
<p>Here’s an app that let’s you set the real probability of the coin and the strength of your prior belief. The more narrowly you set the prior distribution around 0.5, the more strongly you believe 0.5 is the correct probability.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.output_plot("posteriorPlot", height="500px"),
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd()
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig            
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Track posterior probabilities at p=0.5 for each flip count
        flip_counts = []
        posterior_values = []
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posteriors for each flip count
        for idx, data in enumerate(history):
            total_flips = data["total_flips"]
            total_heads = data["total_heads"]
            
            # Calculate posterior using Beta distribution (conjugate prior approximation)
            # We convert the normal prior to an approximate beta prior
            alpha_prior = 0.5 / prior_sd**2
            beta_prior = alpha_prior
            
            # Update with observed data
            alpha_posterior = alpha_prior + total_heads
            beta_posterior = beta_prior + (total_flips - total_heads)
            
            # Calculate and normalize posterior
            posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
            posterior = posterior / np.max(posterior)  # Normalize for better display
            
            # Plot with darker lines for more recent posteriors
            alpha = 0.3 + 0.7 * (idx + 1) / len(history)
            lw = 1 + idx * 0.5
            if idx == len(history) - 1:
                ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                        label=f'Posterior after {total_flips} flips ({total_heads} heads)')
            
            # Store values for the evolution plot
            flip_counts.append(total_flips)
            posterior_values.append(stats.beta.pdf(input.true_prob(), alpha_posterior, beta_posterior) / 
                                np.max(stats.beta.pdf(x, alpha_posterior, beta_posterior)))
        
        # Add true probability line
        ax.axvline(x=history[-1]["true_prob"], color='red', linestyle='--', 
                label=f'True probability: {history[-1]["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        # Add an inset plot showing the evolution of posterior at the true probability
        if len(flip_counts) &gt; 1:
            # Use subplot method instead of add_axes
            inset_ax = fig.add_subplot(3, 3, 9)
            inset_ax.plot(flip_counts, posterior_values, 'ko-', ms=4)
            inset_ax.set_xlabel('Number of Flips', fontsize=9)
            inset_ax.set_ylabel('Posterior Density', fontsize=9)
            inset_ax.set_title('Evolution at True p', fontsize=10)
            inset_ax.grid(True, linestyle='--', alpha=0.7)
            inset_ax.tick_params(axis='both', labelsize=8)
        
        plt.tight_layout()
        return fig

app = App(app_ui, server)</code></pre>
<p>If you knew the coin had just come from the bank, you may give a strong prior for a fair coin such that even after the three heads, the probability would be only slightly above 0.5. However, if you didn’t know the origin of the coin, you may have a much weaker prior - but you also know it would be virtually impossible to create a coin that could only land heads - and therefore you would still give a prior that prevented P=1.</p>
<section id="posterior-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-distribution">Posterior Distribution</h3>
<p>Hopefully you noticed that after flipping coins the chart in the app showed a ‘posterior’ distribution. This distribution shows what we believe about the probability a random flip shows heads after we’ve seen a number of flips. We may say the posterior is a revised belief after witnessing additional evidence/data.</p>
</section>
<section id="probabilities-about-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="probabilities-about-probabilities">Probabilities about Probabilities</h3>
<p>One of the very confusing thing about Bayesian statistics is it gets meta. We start to talk about probabilities of parameters that describe probabilities. Like treating addiction, the first step is to explicitly acknowledge what’s happening:</p>
<ol type="1">
<li>We have a probability distribution that describes a parameter P. The probability distribution says if we sampled P randomly, here’s the more and less likely values.</li>
<li>Let’s assume we’ve sampled P and we have a value, like 0.578.</li>
<li>We then use the value for P, which was 0.578 to generate random data.
<ul>
<li>This will randomly generate 0’s for tails and 1’s for heads</li>
<li>It will do so at a rate that is approximately 0.578 heads</li>
</ul></li>
<li>We could resample P, it will have a new value like 0.489.
<ul>
<li>This will randomly generate 0’s for tails and 1’s for heads</li>
<li>It will do so at a rate that is approximately 0.489 heads</li>
</ul></li>
</ol>
<p>We can have the model sample the value of parameter P based on either what we believe before we saw data (the prior) or after we saw data (the posterior). The distribution for P is different in the prior and posterior, but the process of generating data described above is the same.</p>
<p>Here’s an updated coin flipping app to try to make this more obvious. We can now see some examples (four) of the value P may take when sampled, and then subsequently how flip data would be generated from the sampled P. There is a seperate tab for sampling from the prior, and another for sampling from the posterior.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.navset_tab(
            ui.nav_panel("Prior and Posterior", 
                 ui.output_plot("posteriorPlot", height="500px")
            ),
            ui.nav_panel("Prior P Histograms", 
                 ui.output_plot("fixedPHistograms", height="500px")
            ),
            ui.nav_panel("Posterior P Histograms", 
                 ui.output_plot("posteriorPHistograms", height="500px")
            )
        )
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd(),
            "raw_flips": flips
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig

    @output
    @render.plot
    def fixedPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
            
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        
        # Calculate prior parameters
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Sample P values from the prior distribution with truncation between 0 and 1
        prior_p_samples = []
        for _ in range(4):
            sample = np.random.normal(prior_mean, prior_sd)
            # Truncate to [0,1]
            sample = min(max(sample, 0.001), 0.999)
            prior_p_samples.append(sample)
            
        n = int(input.num_flips())
        
        for i, p in enumerate(prior_p_samples):
            # Generate data with prior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='skyblue', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Prior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Prior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig
    
    @output
    @render.plot
    def posteriorPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
        
        data = history[0]
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        prior_sd = data["prior_sd"]
        
        # Calculate posterior parameters
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Draw 4 samples from the posterior
        posterior_p_samples = np.random.beta(alpha_posterior, beta_posterior, 4)
        
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        n = int(input.num_flips())
        
        for i, p in enumerate(posterior_p_samples):
            # Generate data with posterior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='lightgreen', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Posterior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Posterior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig

app = App(app_ui, server)
</code></pre>
<p>Picking a larger sample size and choosing a true probability of heads that differs notably from 0.5 can help make the difference between the prior and posterior more obvious. In small sample sizes you are much more at the mercy of random chance, which will occassionally have some unexpected results.</p>
</section>
</section>
<section id="the-argument-for-and-against-priors" class="level2">
<h2 class="anchored" data-anchor-id="the-argument-for-and-against-priors">The Argument For and Against Priors</h2>
<p>These are generally the ‘Bayesian’ arguments for priors:</p>
<ol type="1">
<li>If you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.</li>
<li>If you <em>do</em> have an overwhelming amount of data, the priors you set will have little difference in the final statistical model.</li>
</ol>
<p>The argument against priors is that they can introduce arbitrary practitioner bias into an otherwise sound statistical model. This is certainly possible, but explanation and disclosure of any priors used should mitigate these concerns. Finally, with regards to 2) given above, this is generally true unless your priors precluded a valid part of the sample space. For instance, a uniform prior (constant probability between two points) can be a way to minimize practitioner bias, however, if a valid parameter value is outside of the bounds, it is completely inaccessible to the model.</p>
</section>
<section id="the-math-of-bayesian-updating" class="level2">
<h2 class="anchored" data-anchor-id="the-math-of-bayesian-updating">The Math of Bayesian Updating</h2>
<p>The updating of probabilities/beliefs in Bayesian analysis is based on Bayes’ Theorem:</p>
<p><span class="math display">\[
P(\theta | \text{data}) = \frac{P(\text{data} | \theta) P(\theta)}{P(\text{data})}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(\theta | \text{data})\)</span> is the posterior</li>
<li><span class="math inline">\(P(\text{data} | \theta)\)</span> is the likelihood</li>
<li><span class="math inline">\(P(\theta)\)</span> is the prior</li>
<li><span class="math inline">\(P(\text{data})\)</span> is the marginal likelihood (normalizing constant)</li>
</ul>
<p>Since <span class="math inline">\(P(\text{data})\)</span> in practical applications is just a normalizing constant, it can be much simpler conceptually to think about Bayes’ Theorem this way:</p>
<p><span class="math display">\[
P(\theta | \text{data}) \propto P(\text{data} | \theta) P(\theta)
\]</span></p>
<p>Which states that posterior value of the model parameters (<span class="math inline">\(P(\theta | \text{data})\)</span>) after seeing the data is proportional to the likelihood of the data based on the model parameters (<span class="math inline">\(P(\text{data} | \theta)\)</span>) multiplied by the prior belief (<span class="math inline">\(P(\theta)\)</span>) in the parameter values.</p>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">…</h2>
<p>…</p>
</section>
<section id="maximum-likelihood-estimation-mle-vs.-maximum-a-posteriori-map" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation-mle-vs.-maximum-a-posteriori-map">Maximum Likelihood Estimation (MLE) vs.&nbsp;Maximum A Posteriori (MAP)</h2>
<p>In machine learning, MAP (Maximum A Posteriori) estimation is a Bayesian approach to parameter estimation. It finds the most probable parameter values given the observed data and a prior distribution.</p>
<p>Mathematically, it maximizes the posterior probability:</p>
<p><span class="math display">\[
\theta_{MAP} = \arg\max_{\theta} P(\theta | X)
\]</span></p>
<p>Using Bayes’ theorem:</p>
<p><span class="math display">\[
P(\theta | X) = \frac{P(X | \theta) P(\theta)}{P(X)}
\]</span></p>
<p>Since P(X) is constant for optimization, MAP simplifies to:</p>
<p><span class="math display">\[
\theta_{MAP} = \arg\max_{\theta} P(X | \theta) P(\theta)
\]</span></p>
<p>This contrasts with Maximum Likelihood Estimation (MLE), which maximizes only P(X | ) (ignoring the prior P() ). MAP is useful when incorporating prior knowledge into the learning process.</p>
<section id="low-dimensional-search-grid" class="level3">
<h3 class="anchored" data-anchor-id="low-dimensional-search-grid">Low Dimensional Search (Grid)</h3>
<p>Just like we did in the previous chapter on <span class="math inline">\(\mathcal{L}(M|D)\)</span> without priors, we can find parameter values with grid search. However, we should acknowledge two major differences:</p>
<ol type="1">
<li>Previously we did grid search to find the parameter with the maximum likelihood. Now we do grid search to find the distribution of likelihoods for the parameter <span class="math inline">\((P(\text{data})\)</span>. The results will include a maximum likelihood value of the parameter, but also other lower likelihood values of the parameter.</li>
<li>We multiply the values found in the grid search by our prior belief in the parameter values (<span class="math inline">\(P(\theta)\)</span>). If we do not have a strong prior belief, we can choose values that have little to no effect.</li>
</ol>
<p><strong>NOTE TO SELF, NEED TO UNIFY THE LIKELIHOOD AND PROB NOTATION</strong></p>
</section>
<section id="high-dimensional-search-mcmc" class="level3">
<h3 class="anchored" data-anchor-id="high-dimensional-search-mcmc">High Dimensional Search (MCMC)</h3>
<p>Like before, grid search does not scale well to many parameters/dimensions. To solve this previously we introduced Gradient Ascent, which uses slopes/gradients to guide us to the maximum value.</p>
<p>There’s a major problem, however, with using Gradient Ascent in Bayesian analysis - we do not want to find just the most likely value of the parameter - we want to find the correct probability distribution for the parameter (the likelihood of any value the parameter could take).</p>
<p>The solution to this is to use random sampling methods, such that we search through the possible combinations of the parameters and see how likely they are. As the algorithm starts, we can think of it a little like grid search - we check some combination of parameters and calculate a likelihood for those parameters based on how well they fit the data. The problem is that in high dimensional space, the extreme majority of the parameter combinations will have virtually zero likelihood. (Picture a model with random parameter values generating data that isn’t even close to what we observe).</p>
<p>To solve the problem of constantly testing parameters that have virtually zero likelihood, modern sampling algorithms know to head towards areas of higher likelihood. However, importantly they do not simply head towards the peak like they would in Gradient Ascent - instead they strategically wander around the edges and peaks of the high likelihood space, mapping out those regions like a cartographer mapping a mountain ridge.</p>
</section>
<section id="parameter-find-app" class="level3">
<h3 class="anchored" data-anchor-id="parameter-find-app">Parameter Find App</h3>
<p>To get a feel for what it’s like to search the parameter space for parameter combinations with a high likelihood, play around with the app below.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Parameter Estimation"),
    ui.row(
        ui.column(3, 
            ui.input_select("search_method", "Search Method:", 
                choices=["Grid Search", "Likelihood Search"], selected="Grid Search"),
            ui.input_select("num_samples", "Number of Samples:", 
                choices=["10", "100", "1000"], selected="100"),
            ui.input_action_button("generate", "Generate Samples", class_="btn-primary"),
            ui.p(""),
            ui.input_checkbox("show_samples", "Show sample points", value=True)
        ),
        ui.column(9,
            ui.output_plot("likelihoodPlot", height="500px")
        )
    )
)

def server(input, output, session):
    # Store the current samples
    samples = reactive.Value(None)
    
    @reactive.effect
    @reactive.event(input.generate)
    def handle_generate():
        try:
            n = int(input.num_samples())
            search_method = input.search_method()
            
            # True parameters
            true_x, true_y = 0.6, 0.4
            
            if search_method == "Grid Search":
                # Create evenly spaced grid
                grid_size = int(np.sqrt(n))
                x_grid = np.linspace(0, 1, grid_size)
                y_grid = np.linspace(0, 1, grid_size)
                X, Y = np.meshgrid(x_grid, y_grid)
                theta1 = X.flatten()[:n]  # Ensure exactly n samples
                theta2 = Y.flatten()[:n]
                
                # Add small noise to make it look more realistic
                theta1 += 0.01 * np.random.randn(len(theta1))
                theta2 += 0.01 * np.random.randn(len(theta2))
                
                # Ensure values stay within bounds
                theta1 = np.clip(theta1, 0, 1)
                theta2 = np.clip(theta2, 0, 1)
                
            else:  # Likelihood Search
                # First, generate random samples across the parameter space
                theta1_initial = np.random.uniform(0, 1, n*2)  # Generate extra samples
                theta2_initial = np.random.uniform(0, 1, n*2)
                
                # Calculate likelihood based on distance from true parameters
                # (simulate a likelihood function with a peak at true parameters)
                likelihoods = np.exp(-10 * ((theta1_initial - true_x)**2 + (theta2_initial - true_y)**2))
                
                # Sample from these points with probability proportional to likelihood
                selection_probs = likelihoods / np.sum(likelihoods)
                selected_indices = np.random.choice(len(theta1_initial), size=n, p=selection_probs, replace=False)
                
                theta1 = theta1_initial[selected_indices]
                theta2 = theta2_initial[selected_indices]
            
            samples.set((theta1, theta2))
        except Exception as e:
            print(f"Error generating samples: {e}")
            samples.set(None)

    @output
    @render.plot
    def likelihoodPlot():
        # Create basic plot
        fig, ax = plt.subplots(figsize=(8, 6))
        
        # Get current samples
        current_samples = samples.get()
        
        if current_samples is None:
            ax.text(0.5, 0.5, "Click 'Generate Samples' to create data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            return fig
        
        theta1, theta2 = current_samples
        n = len(theta1)
        
        try:
            # Create a 2D histogram
            H, xedges, yedges = np.histogram2d(
                theta1, theta2, 
                bins=20, 
                range=[[0, 1], [0, 1]]
            )
            
            # Use pcolormesh for better rendering
            pcm = ax.pcolormesh(xedges, yedges, H.T, cmap='viridis')
            
            # Add colorbar
            plt.colorbar(pcm, ax=ax, label='Count')
            
            # Plot the samples if requested
            if input.show_samples():
                ax.scatter(theta1, theta2, s=5, color='red', alpha=0.3, label=f'Samples (n={n})')
            
            # Mark true parameters
            ax.plot(0.6, 0.4, 'y*', markersize=15, markeredgecolor='black', label='True parameters')
            
            # Labels and layout
            ax.set_xlabel('Parameter θ₁')
            ax.set_ylabel('Parameter θ₂')
            
            search_method = input.search_method().split()[0]
            ax.set_title(f'Parameter Estimation using {search_method} ({n} samples)')
            
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.legend(loc='upper right')
            
        except Exception as e:
            print(f"Error plotting: {e}")
            ax.text(0.5, 0.5, f"Error creating plot: {str(e)}", 
                   ha='center', va='center', fontsize=12)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
        
        return fig

app = App(app_ui, server)</code></pre>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>As stated in the title, this is intended to be a ‘primer’ to introduce you to the topic. We hope you have some sense for the important foundations of statistics. We encourage you to learn by experimenting - creating your own data and examples will be the fastest path to understanding for the majority of people.</p>
<p>There are also many excellent texts in both statistics and machine learning, hopefully with some intuition on the basics, the rationale and math will be much easier to understand.</p>
</section>
<section id="text" class="level2">
<h2 class="anchored" data-anchor-id="text">Text</h2>
<p>Text.</p>
<pre class="shinylive-python" data-engine="python"><code>#| '!! shinylive warning !!': |
#|   shinylive does not work in self-contained HTML documents.
#|   Please set `embed-resources: false` in your metadata.
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        # MLE for variance uses 1/n factor
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var &lt;= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)</code></pre>
</section>
<section id="old-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="old-thoughts">Old Thoughts</h2>
<p>For with priors, consider: - Low Dimensional Search (Grid) - High Dimensional Search (MCMC)</p>
</section>
<section id="probability-distribution-or-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="probability-distribution-or-uncertainty">Probability Distribution or Uncertainty?</h2>
<p>While I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.</p>
<p>The second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don’t have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.</p>
<p>In the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph DGD[Data Generating Distribution]
        subgraph UD[Uncertainty Distributions]
            GammaMean[Gamma Distribution]
            GammaVar[Gamma Distribution]
        end
        Mean[Mean]
        Variance[Variance]
        GammaMean --&gt; Mean
        GammaVar --&gt; Variance
        Mean --&gt; Normal
        Variance --&gt; Normal
        Normal[Normal Distribution]
    end
    Normal --&gt; Height[Height Observations]

</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>