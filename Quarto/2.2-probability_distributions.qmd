---
title: Likelihood
format: html
filters:
  - shinylive
---


## More Data Generating Processes (Probability Distributions)

When we change the data generating process, the distribution of outcomes changes. Some data generating processes are quite common, so humanity has named them and [mostly] standardized their inputs/parameters. These common data generating processes are called *probability distributions*. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you'll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of rolls.

**Some kind of graphic here?**

A 'coin-flip' process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn't have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there's no fixed maximum.

Additive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn't noticed in the dice example, if you keep adding dice and enough rolls, the outcomes look awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you'll get the log-normal distribution.

The point here is that each probability distribution is the basic form of:
A) Each data generating process has inherent characteristics
B) These characteristics determine the shape/frequency of the outcomes

## Normal/Gaussian Likelihood

Hopefully it was fairly intuitive how to find the probability/likelihood of a certain dice-roll total. One advantage of that data generating process is the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One common example is height.


```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        # MLE for variance uses 1/n factor
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```


A simple way to understand the likelihood of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.

It will be useful for us to also estimate the probability density of exact values in continuous distributions...

## Likelihood of Multiple Events

So far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes.

**Equation for multiple outcomes, and the useful alternative of log-probabilty**

When determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is [mostly] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.

This is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.