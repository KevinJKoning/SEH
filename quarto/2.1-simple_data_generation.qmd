---
title: Discrete Probability Distributions
format: html
filters:
  - shinylive
---

## Preview

Here we begin the first half of the primer, which focuses on $P(D|M)$, the probability of the data given a model of a data generating process. Before we can talk about data from a model, we need to introduce models. The first chapter considers only models with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce 'discrete probability distributions' as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.

Once we have have some discussion of models under our belt, we will change our focus to the probability of data. We will start with the dice example and use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. We then show how discrete probability distributions can generate samples similar to our dice rolling model but for other processes. We then show how we could use the relative frequency technique with discrete probability distributions to find approximate probabilities. We then point out, like someone already performing an infinite number of samples, the exact probabilities have already been calculated, if we need to use them√ü.

Finally, we show how we can use computation to calculate the approximate probability of multiple events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls.

## Data Generation with Dice

We'd prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we'll use rolling dice as our first example of a data generating process. It is also convenient that the mathematical model we'll use is an awfully good approximation of the real data generating process, so long as you're OK with ignoring all the physical bouncing of the dice and are content with just the result after a roll.

Play around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls. Note how a larger number of rolls seems to give us smoother and more consistent results.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Dice Rolling Demo"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("numDice", "Number of Dice", min=1, max=10, value=2, step=1),
            ui.input_slider("numRolls", "Number of Rolls", min=1, max=10000, value=100, step=1),
        ),
        ui.output_plot("dicePlot", height="400px"),
    ),
)

def server(input, output, session):
    # Define a reactive calculation that depends on numDice and numRolls
    @reactive.Calc
    def dice_sums():
        return [
            np.random.randint(1, 7, input.numDice()).sum()
            for _ in range(input.numRolls())
        ]

    @output
    @render.plot
    def dicePlot():
        current_sums = dice_sums()
        fig, ax = plt.subplots()

        unique_sums, counts = np.unique(current_sums, return_counts=True)
        ax.bar([str(s) for s in unique_sums], counts, color="steelblue")

        ax.set_title("Frequency of Dice Totals")
        ax.set_xlabel("Dice Total")
        ax.set_ylabel("Frequency")
        plt.xticks(rotation=90)

        return fig

app = App(app_ui, server)
```

## Discrete Probability Distributions as Data Generating Models

In the last section, we used dice rolling as our data generating process, however there are other discrete processes, such as flipping a coin. As you can imagine, when you change the data generating process, the relative frequency of the possible events/outcomes changes. There are a number of discrete processes that have been interesting to statisticians, and they have been formalized into mathematical models called **discrete probability distributions**.

### Binomial Distribution

We use coin flipping as the basis of the binomial distribution, but be aware that the 'coin' does not need to be fair, or even close to fair.

Some kinds of 

### Poisson Distribution


## Dice Totals Probability

Our goal here is not to think too much about the data generating process or the model (that comes later!), what we really want to know now is the likelihood of a particular dice total given a dice model. To estimate the probability $P(E)$ of an event $E$, we can use the relative frequency approach. This involves counting the number of occurrences of the event E and dividing it by the total number of trials. For example, if we observed a total of twelve occur in 40 out of 5,000 dice rolls, the probability estimate is:

$$
P(E) \approx \frac{\text{Number of times event } E \text{ occurs}}{\text{Total number of trials}} = \frac{40}{5000} = 0.008
$$

The accuracy of this estimate depends on the total number of trials as governed by the Law of Large Numbers. The standard error, which gives a measure of uncertainty in the estimate of a mean value, is proportional to one over the square root of the number of samples:

$$
P_{\text{error}} \propto \frac{1}{\sqrt{N_{\text{total}}}}
$$

Which indicates there are diminishing returns to just making the sample size larger. Now I know you're smart, and you're saying to yourself, I can figure out the *exact* probability of rolling a certain dice total. Of course you can for this example - but you probably can't for more realistic examples, and we want to learn techiques that work well for real problems. In general, if you are concerned with the quality of an estimate with this approach, just rerun the model and see if the outcome changes meaningfully - if it does, increase the number of times we run the model until the output is stable enough for your application. If that's still not enough, dig into exact/analytic methods.

::: callout-note
Throughout this primer there will be several opportunities for exact/analytical solutions, but we will generally ignore them. Instead we will choose computation whenever it is plausible. Why? It's simply a lot faster than figuring out some unknown depth of analysis, and we can usually have more confidence in the results because we've reduced the human error element. (Some may know an essay called the 'The Bitter Lesson' by Rich Sutton, which I think has a strong engineering corollary...). Be assured that solving problems through raw computation has it's limits - there's no way to compute through a bad algorithm, and often the right next step in that situation is to gain a more fundamental understanding through analysis. But we're professionals in a hurry, we choose computation when it's plausible.
:::

## Summary

What we intended to show here is we can build a model of a process, and use that model to determine how probable any outcome of the process is. We also see that the reliability of that estimate is related to the total number of samples from the process, with more samples leading to more reliable estimates.

## Wait Not so Fast

You may have a thought lingering in the back of your brain - why am I trusting you that your dice rolling model represents reality? Well, in general you shouldn't trust any model. This is where real data is incredibly important. Preferably we'd have the data from real dice rolls, and although we shouldn't expect a perfect match, we'd be able to at least get an intuition for whether the model was reasonable. Since this problem was so simple, and is relatively easy to verify analytically, we skipped this otherwise important step.

## Wait, One More!

You may also have asked yourself - why can't (or when should) I just use the real data to find the probability of an event? This would use the same technique as before, except the data would give count/frequency of the event divided by the total number of observations.

The answer to that is - if you have enough data, you should do it that way! For example, if you've observed something about a dozen or more times in a dataset, dividing the count/frequency of that event by the total number of observations will give you quite a good estimate for its probability, given the same conditions as the dataset. It's more complicated if you've only observed the event a couple times, and obviously impossible if you've observed it zero times. It's also not possible if you want to make a change to the conditions that were used to generate the dataset... (discussion of uncertainty in those estimates at low counts????????, appendix material????)

It's quite common to be interested in the likelihood of an event in the tails (the far ends) of a distribution, however this is also where we need the most caution. Generally we will use some knowledge about the problem and the kind of data it is likely to produce combined with some sample of data to ground-truth the model parameters... However, since the tails of the distribution are rarely or never observed, we need a lot of caution and humility if we try to predict them.

