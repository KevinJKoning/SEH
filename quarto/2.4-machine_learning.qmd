---
title: Machine Learning
format: 
    html:
        mermaid:
            theme: neutral
filters:
  - shinylive
execute:
  echo: false
---

## Preview

Due to popular demand, we move on from traditional statistical models to those considered *machine learning*. We touch on the differences and similarities between the two categories before choosing to continue with a machine learning model that allows for statistical (i.e. probabilistic) interpretation.

The chapter is focused on Bayesian Neural Networks, and although they are a relatively advanced topic, you may be surprised to learn the major building blocks have already been covered. In our version, each neuron is just logistic regression (which is just generalized linear regression) built into a network. Once we build this network we will allow it to generate data like in previous chapters. Because this type of machine learning is probabilistic, we can calculate P(D|M).

## Statistical Models vs. Machine Learning

I assume most engineers, a priori, consider *machine learning* models as somehow 'better' than statistical models. In reality there is a *lot* of overlap between them, and the situation should dictate the model selection. However, since we assume the reader will inevitably be interested in machine learning, we think it's best to include it, at the very least so we can understand the similarities and differences when compared to statistical models.

Due to the amount of blur/overlap, this may not be a universally agreed distinction, but we define an important difference this way:

* Statistical models are built around probability theory
* Machine learning models are built around an objective/**loss function**

The confusing thing is that a lot of methods known as being machine learning have objective/loss functions that are purely probabilistic - but the key is they *do not have to be*.

There are other common differences that are also blurry - like that machine learning is focused on larger datasets and often have more flexible (i.e. more parameters) methods. This tends to also mean overfitting is a larger concern in machine learning. You would also typically not define a causal model as a foundation for a machine learning model, as they tend to use predefined algorithms.

## Loss Function

A loss function, denoted as $L$, is typically computed as the difference between the true target values $y$ and the predicted values $\hat{y}$ The generic form of a loss function can be expressed as:

$$
L(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} \ell(y_i, \hat{y}_i)
$$

#### Variables:
- $y_i$: The true value for the \(i\)-th data point.
- $\hat{y}_i$: The predicted value for the \(i\)-th data point.
- $\ell(y_i, \hat{y}_i)$: The individual error for the \(i\)-th data point, defined by a specific loss function (e.g., squared error, cross-entropy).
- $N$: The total number of data points.

#### Explanation:
- The function $\ell(y_i, \hat{y}_i)$ depends on the task (regression or classification). 
  - Example for regression: $\ell(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$ (Mean Squared Error).
  - Example for classification: $\ell(y_i, \hat{y}_i) = -[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$ (Binary Cross-Entropy).
- The summation aggregates the error across all data points.
- Dividing by $N$ ensures the loss represents the average error.

## Likelihood Function

The loss function is in contrast to using likelihood functions, which are based in probability theory. We use likelihood to find the best statistical model, specifically by finding the Maximum Likelihood Estimate (MLE). We will explore these more in the second half of the primer, $\mathscr{L}(M|D)$.

## Machine Learning Models

The following are some of the most common machine learning models. *Supervised learning* means that there is a result or 'y' variable to learn from. *Unsupervised learning* does not have the result or 'y' variable, which is a little confusing - instead of trying to predict they are generally attempting some kind of categorization/clustering or simplification.

You may note that the first two on the list below, linear regression and logistic regression, would also firmly sit on most lists of common statistical models.

#### Supervised Learning
1. **Linear Regression**: Predicts continuous values.
2. **Logistic Regression**: For binary or multi-class classification.
3. **Decision Trees**: Simple, interpretable models for classification and regression.
4. **Random Forest**: Ensemble of decision trees for better accuracy.
5. **Gradient Boosting (e.g., XGBoost, LightGBM)**: Powerful ensemble method for structured data.
6. **Support Vector Machines (SVM)**: For classification and regression.
7. **Neural Networks**: Flexible models, basis of deep learning.

#### Unsupervised Learning
1. **k-Means Clustering**: Groups similar data points.
2. **Principal Component Analysis (PCA)**: Reduces dimensionality while preserving variance.


## Bayesian Neural Net

There are many machine learning models we could focus on, however, we'll choose Bayesian Neural Nets for a few reasons:

1) They make probabilistic predictions just like our statistical models
2) We have already learned most of the basic building blocks
3) [Standard] Neural nets are the basis of modern AI

### 'Standard' Neural Net

Before we layer in the Bayesian piece, we start with 'standard' neural nets. Although there are many variations, we can think of a neural net as layers of artificial neurons that simply perform logistic regression on the previous layer of neurons to determine their activation state.

#### Structure

To explain that further it helps immensely to have a diagram that points out the layers, the neurons, and what information is being fed into each neuron.

```{mermaid}
graph LR
  %% Define classes with desired styles
  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;
  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;
  classDef hiddenLayer2 fill:#d8c2f9,stroke:#6a00d6;
  classDef outputLayer fill:#c2f9c9,stroke:#00d640;

  subgraph Input Layer
    N1(["N1"])
    class N1 inputLayer;
  end

  subgraph Hidden Layer 1
    N2(["N2"])
    N3(["N3"])
    N4(["N4"])
    class N2,N3,N4 hiddenLayer1;
  end

  subgraph Hidden Layer 2
    N5(["N5"])
    N6(["N6"])
    N7(["N7"])
    class N5,N6,N7 hiddenLayer2;
  end

  subgraph Output Layer
    N8(["N8"])
    class N8 outputLayer;
  end

  N1 -->|C1| N2
  N1 -->|C2| N3
  N1 -->|C3| N4
  
  N2 -->|C4| N5
  N2 -->|C5| N6
  N2 -->|C6| N7
  
  N3 -->|C7| N5
  N3 -->|C8| N6
  N3 -->|C9| N7
  
  N4 -->|C10| N5
  N4 -->|C11| N6
  N4 -->|C12| N7
  
  N5 -->|C13| N8
  N6 -->|C14| N8
  N7 -->|C15| N8
```

As shown in the diagram above, there are the following pieces with the following purpose:

1) **The Input Layer**, which takes in data. The number of neurons match the number of variables (or dimensions) of the data we want to input. We are going to input one variable (the size/carats of a diamond) so we only need one input neuron.
2) That input goes to **Hidden Layer 1**. Each neuron will take the input and be activated according to the input value and a weight/coefficient associated with the neuron.
3) At **Hidden Layer 2** things get interesting. Each of these neurons take the values from the first layer of neurons and performs another logistic regression to determine their activation. From the diagram you can see there are lots of connections here. N6 will take the activation value from N2 and apply it's own weight/coefficient, same with N3, N4, and N5, and use logistic regression to get it's own activation. That same process repeats with N7, N8, and N9, each with their own weights/coefficients.
4) The **Output Layer** which is a final regression using the activation of N6, N7, N8, and N9 to get the output value of the neural net.

#### Weights/Coefficients

While the structure of a neural net influences the types of problems it is most appropriate for, the knowledge of the neural net is held in the weights. Determining the weights of a neural network is generally accomplished by training on data via backpropogation, for which numerous resources are available elsewhere. Here, we will jump past that the details of that step so that we can proceed with a reasonably trained neural net.

## Stan Neural Network Sim

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# --- Stan Parameter Summaries ---
weights_in_means   = np.array([-3.66,  -17.54,  10.90,  16.47])
weights_in_sds     = np.array([36.38,   13.83,  15.57,  15.74])

bias_in_means      = np.array([0.39, 23.51, -12.30, -16.51])
bias_in_sds        = np.array([58.16, 23.08, 23.47, 19.09])

weights_hidden_means = np.array([
    [-6.52,  15.34, -18.64, -18.16],
    [ 5.09,  -6.29, -23.97,  -2.87],
    [ 1.61,   0.52,   0.60, -36.48],
    [ 1.48, -18.81,   7.46,  24.19]
])
weights_hidden_sds = np.array([
    [ 8.49, 21.82, 15.54, 20.49],
    [16.63, 21.01, 29.03, 27.77],
    [35.43, 19.57, 17.53, 28.02],
    [26.89, 27.62, 26.75, 27.26]
])

bias_hidden_means  = np.array([-0.60, 10.05, 17.50,  6.19])
bias_hidden_sds    = np.array([9.18, 13.59, 28.24,  5.59])

weights_out_means  = np.array([3661.48, 1975.56, 4818.68, 3557.60])
weights_out_sds    = np.array([388.84, 2510.70, 1523.12, 492.29])

bias_out_mean      = 688.15
bias_out_sd        = 1084.63

sigma_mean         = 160.61
sigma_sd           = 1.08

# --- Neural Network Forward Pass ---
def neural_net_forward(x, weights_in, bias_in, weights_hidden, bias_hidden, weights_out, bias_out):
    z1 = np.dot(x, weights_in) + bias_in      # first hidden layer linear combination
    a1 = 1 / (1 + np.exp(-z1))                # sigmoid activation
    z2 = np.dot(a1, weights_hidden) + bias_hidden
    a2 = 1 / (1 + np.exp(-z2))                # second hidden layer activation
    mu = np.dot(a2, weights_out) + bias_out   # output layer (linear)
    return mu

# --- Simulation Function ---
def simulate_random_predictions(nsim):
    """
    For each simulation:
      - Randomly sample a carat value between 0.1 and 2,
      - Sample the neural network parameters from Normal distributions,
      - Compute the predicted diamond price (with noise added).
    Returns arrays of simulated carat values and predicted prices.
    """
    sim_carat = []
    sim_price = []
    for _ in range(nsim):
        # Randomly choose a carat value in [0.1, 2]
        carat_val = np.random.uniform(0.1, 2.0)
        x_val = np.array([[carat_val]])
        
        # Sample NN parameters
        wi = np.random.normal(weights_in_means, weights_in_sds).reshape(1, 4)
        bi = np.random.normal(bias_in_means, bias_in_sds).reshape(1, 4)
        wh = np.random.normal(weights_hidden_means, weights_hidden_sds)
        bh = np.random.normal(bias_hidden_means, bias_hidden_sds)
        wo = np.random.normal(weights_out_means, weights_out_sds)
        bo = np.random.normal(bias_out_mean, bias_out_sd)
        sigma_sample = np.abs(np.random.normal(sigma_mean, sigma_sd))
        
        # Compute network prediction and add noise
        mu = neural_net_forward(x_val, wi, bi, wh, bh, wo, bo)
        price_pred = mu + np.random.normal(0, sigma_sample, size=mu.shape)
        
        sim_carat.append(carat_val)
        # Use price_pred[0] since price_pred is 1-dimensional
        sim_price.append(price_pred[0])
    return np.array(sim_carat), np.array(sim_price)

# --- Load Diamond Data ---
data_str = """ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""

df = pd.read_csv(StringIO(data_str))
# Restrict to diamonds with carat values between 0.1 and 2
df_filtered = df[(df["carat"] >= 0.1) & (df["carat"] <= 2)]

# --- Shiny UI ---
app_ui = ui.page_fluid(
    ui.h2("Random Diamond Price Simulation"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("nsim", "Number of Simulations", min=1, max=100, value=20, step=1),
            ui.input_checkbox("show_actual", "Show Actual Diamond Prices", value=True)
        ),
        ui.navset_tab(
            ui.nav_panel("Predicted Prices",
                ui.output_plot("predictionPlot", height="400px")
            )
        )
    )
)

# --- Shiny Server ---
def server(input, output, session):
    
    @reactive.Calc
    def simulation_data():
        nsim = input.nsim()
        sim_carat, sim_price = simulate_random_predictions(nsim)
        return {"sim_carat": sim_carat, "sim_price": sim_price}
    
    @output
    @render.plot
    def predictionPlot():
        data_sim = simulation_data()
        sim_carat = data_sim["sim_carat"]
        sim_price = data_sim["sim_price"]
        
        fig, ax = plt.subplots(figsize=(8,6))
        ax.scatter(sim_carat, sim_price, color="red", label="Simulated Price", alpha=0.7)
        
        if input.show_actual():
            ax.scatter(df_filtered["carat"], df_filtered["price"], color="blue", label="Actual Price", alpha=0.5)
        
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.set_title("Simulated Diamond Prices vs. Carat Size")
        ax.legend()
        return fig

app = App(app_ui, server)

```

## Stan Sim v2

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --- Helper Functions ---

def inv_logit(z):
    """Sigmoid function."""
    return 1 / (1 + np.exp(-z))

def generate_parameters():
    """
    Draw a single set of neural network parameters using the Stan summary's means and sds.
    """
    params = {}
    # Input -> first hidden layer (1 x 4)
    params["weights_in"] = np.array([
         np.random.normal(-0.84,  0.01),
         np.random.normal(2.71, 0.01),
         np.random.normal(-0.82, 0.01),
         np.random.normal(-9.16, 0.01)
    ])
    params["bias_in"] = np.array([
         np.random.normal(1.03, 0.01),
         np.random.normal(-3.78, 0.01),
         np.random.normal(0.92, 0.01),
         np.random.normal(13.41, 0.014)
    ])
    
    # First hidden layer -> second hidden layer (4 x 4)
    params["weights_hidden"] = np.array([
        [np.random.normal(3.16, 0.01), np.random.normal(-4.52, 0.01),
         np.random.normal(-0.40, 0.01), np.random.normal(0.07, 0.01)],
        [np.random.normal(-8.63, 0.01), np.random.normal(7.35, 0.01),
         np.random.normal(2.56, 0.01), np.random.normal(-1.52, 0.01)],
        [np.random.normal(0.05, 0.01), np.random.normal(-1.25, 0.01),
         np.random.normal(-0.73, 0.01), np.random.normal(-2.39, 0.01)],
        [np.random.normal(-7.14, 0.01), np.random.normal(-1.35, 0.01),
         np.random.normal(-7.77, 0.01), np.random.normal(4.00, 0.01)]
    ])
    params["bias_hidden"] = np.array([
         np.random.normal(3.71, 0.01),
         np.random.normal(-3.22, 0.01),
         np.random.normal(2.19, 0.01),
         np.random.normal(-1.57, 0.01)
    ])
    
    # Second hidden layer -> output layer (vector of length 4)
    params["weights_out"] = np.array([
         np.random.normal(3856.76, 495.69),
         np.random.normal(3715.54, 513.76),
         np.random.normal(3879.81, 467.75),
         np.random.normal(3916.50, 487.24)
    ])
    params["bias_out"] = np.random.normal(528.46, 120.71)
    
    # Noise standard deviation (sigma)
    params["sigma"] = np.random.normal(470.55, 5.22)
    return params

def simulate_network(x, params):
    """
    Given input array x (carat values) and a set of parameters,
    compute the network output and add noise (sigma scales with x).
    """
    # First hidden layer: apply sigmoid( x * weights_in + bias_in )
    h1 = inv_logit( x[:, None] * params["weights_in"][None, :] + params["bias_in"][None, :] )
    # Second hidden layer: apply sigmoid( h1 dot weights_hidden + bias_hidden )
    h2 = inv_logit( np.dot(h1, params["weights_hidden"]) + params["bias_hidden"] )
    # Output: linear combination (no activation)
    mu = np.dot(h2, params["weights_out"]) + params["bias_out"]
    # Add noise with standard deviation proportional to carat
    # noise = np.random.normal(0, params["sigma"] * x)
    # return mu + noise
    return mu

# --- Define the UI ---

app_ui = ui.page_fluid(
    ui.h2("Bayesian Neural Network Data Generator"),
    ui.p("Generate simulated diamond prices from a Bayesian neural network."),
    ui.input_slider("n_points", "Total number of points:",
                    min=100, max=1000, value=100, step=100),
    ui.input_action_button("add", "Add to Plot"),
    ui.input_action_button("reset", "Reset"),
    ui.output_plot("plot")
)

# --- Define the Server ---

def server(input, output, session):
    # Reactive value to store accumulated data across "Add to Plot" clicks.
    accumulated_data = reactive.Value(pd.DataFrame(columns=["carat", "price", "block"]))
    # A counter to number each block uniquely.
    block_counter = reactive.Value(1)
    
    @reactive.Effect
    @reactive.event(input.add)
    def add_points():
        n_points = input.n_points()
        blocks = n_points // 100  # Each block is 100 points.
        current_df = accumulated_data.get()
        counter = block_counter.get()
        new_blocks = []
        for _ in range(blocks):
            # Generate 100 carat values between 0.5 and 2.5.
            x = np.linspace(0.5, 2.5, 100)
            params = generate_parameters()  # Draw new parameters for each block.
            y = simulate_network(x, params)
            block_label = f"Block {counter}"
            counter += 1
            df = pd.DataFrame({"carat": x, "price": y, "block": block_label})
            new_blocks.append(df)
        # Append the new blocks to the accumulated data.
        new_data = pd.concat(new_blocks, ignore_index=True)
        accumulated_data.set(pd.concat([current_df, new_data], ignore_index=True))
        block_counter.set(counter)
    
    @reactive.Effect
    @reactive.event(input.reset)
    def reset_data():
        # Clear the accumulated data and reset the block counter.
        accumulated_data.set(pd.DataFrame(columns=["carat", "price", "block"]))
        block_counter.set(1)
    
    @output
    @render.plot
    def plot():
        df = accumulated_data.get()
        fig, ax = plt.subplots(figsize=(6,4))
        if not df.empty:
            # Plot each block's points as scatter (no connecting lines)
            for block, group in df.groupby("block"):
                ax.scatter(group["carat"], group["price"], label=block)
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.set_title("Simulated Data from Bayesian Neural Network")
        if not df.empty:
            ax.legend()
        return fig

# --- Create the App ---

app = App(app_ui, server)

```


## Point-Estimate Neural Network

### Training a Simple Neural Network


```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# Data (an excerpt from the diamond dataset)
data_str = """ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67
32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69
16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66
32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69
3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48
16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61
5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48
34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67
28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54
32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57
16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71
31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63
51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64
27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71
37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69
16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69
35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71
30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64
36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69
30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69
32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7
35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72
20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64
34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69
33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69
32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68
32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71
44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68
38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74
43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67
33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68
35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72
37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7
33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7
26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71
33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7
36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69
36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74
13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69
30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74
29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73
30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71
46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71
32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7
20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75
34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67
27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72
29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7
30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72
31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74
35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74
50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73
32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75
29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74
37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64
42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75
30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71
23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79
18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78
31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78
34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8
13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79
36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75
37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77
30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73
36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77
34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7
30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9
33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81
26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76
27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76
34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74
28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93
39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96
33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83
36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9
37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87
41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86
35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87
27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83
41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93
37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86
36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88
37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92
38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85
35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89
31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81
30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96
39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85
30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93
17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9
35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88
33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93
39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96
31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94
33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01
41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95
42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88
30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87
41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89
35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98
24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97
35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98
37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98
36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97
28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96
43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91
41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99
37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98
40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94
37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92
39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98
46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15
38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03
41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1
50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13
43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14
35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05
41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01
37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1
37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16
38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16
33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88
43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09
44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13
32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02
39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17
42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05
49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22
39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16
47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05
38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05
49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19
39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16
44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22
47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19
44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23
49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21
41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11
47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15
48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16
41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21
48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17
43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26
46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19
43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25
36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22
44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18
37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17
48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2
44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17
46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16
46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23
50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24
43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38
47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25
41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23
42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35
48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28
52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23
49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28
52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25
50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24
46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22
46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19
45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3
46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31
41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35
45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35
43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26
45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29
42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24
3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47
51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54
50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51
46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32
53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5
1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53
2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47
52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43
52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4
52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48
44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63
46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4
47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5
2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37
48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48
51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57
46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64
49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47
52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46
2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51
277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53
809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47
52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53
946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56
51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48
48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6
51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57
3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59
51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51
694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56
50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57
45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48
49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54
2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55
1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58
50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55
53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55
48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61
3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51
51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53
53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56
7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82
7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8
8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79
1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73
53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66
1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52
580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67
47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59
1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63
53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63
4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74
4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62
1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62
682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6
113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87
3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88
9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86
53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71
4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61
4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89
9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94
8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82
14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97
12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02
3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93
6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7
9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87
4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89
10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93
9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86
13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75
18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81
3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79
5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68
15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98
10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0
5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94
8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91
4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8
9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01
10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83
15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03
5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86
9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04
15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95
7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05
9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94
15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92
11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06
15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95
10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89
4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99
18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0
15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99
16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0
11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94
16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91
11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99
13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95
13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91
9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96
9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02
10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05
12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98
15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0
8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06
17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03
7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03
16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0
11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92
5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03
15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03
7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04
14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0
8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01
7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99
21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04
48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22
16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01
15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08
18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11
7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08
8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07
20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15
13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18
11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06
19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0
16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0
15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05
17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11
14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1
21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04
4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12
16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09
11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05
16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07
21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1
8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11
18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13
13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13
20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16
12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08
14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22
22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23
8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2
18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21
26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22
20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27
12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27
15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11
18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15
16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23
10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24
12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19
7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21
23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22
20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24
20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24
15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23
24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57
26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53
25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49
6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36
10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25
14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39
15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32
12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32
21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61
22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6
20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53
19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52
16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59
19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49
19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31
15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36
24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42
16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38
20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58
21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29
25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77
24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62
22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63
22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55
19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54
20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54
24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59
21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65
23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67
19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45
19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58
25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53
27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75
27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66
25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69
24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64
15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7
25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57
20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6
17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58
24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14
24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69
22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93
27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2
26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# --- Activation Functions ---
def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x > 0).astype(float)

def logistic(x):
    return 1 / (1 + np.exp(-x))

def logistic_deriv(x):
    sig = logistic(x)
    return sig * (1 - sig)

# --- Neural Network Class ---
# Now accepts an 'activation' parameter ("relu" or "logistic") and hidden layer size.
class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, activation="relu"):
        self.activation = activation  # "relu" or "logistic"
        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size1))
        
        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)
        self.b2 = np.zeros((1, hidden_size2))
        
        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)
        self.b3 = np.zeros((1, output_size))
    
    def forward(self, X):
        self.Z1 = np.dot(X, self.W1) + self.b1
        if self.activation == "relu":
            self.A1 = relu(self.Z1)
        else:
            self.A1 = logistic(self.Z1)
        
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        if self.activation == "relu":
            self.A2 = relu(self.Z2)
        else:
            self.A2 = logistic(self.Z2)
        
        self.Z3 = np.dot(self.A2, self.W3) + self.b3
        return self.Z3
    
    def compute_loss(self, y_pred, y_true):
        return np.mean((y_pred - y_true)**2)
    
    def backward(self, X, y_true, y_pred, learning_rate):
        m = y_true.shape[0]
        dZ3 = (2.0 / m) * (y_pred - y_true)
        dW3 = np.dot(self.A2.T, dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        
        dA2 = np.dot(dZ3, self.W3.T)
        if self.activation == "relu":
            dZ2 = dA2 * relu_deriv(self.Z2)
        else:
            dZ2 = dA2 * logistic_deriv(self.Z2)
        dW2 = np.dot(self.A1.T, dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        
        dA1 = np.dot(dZ2, self.W2.T)
        if self.activation == "relu":
            dZ1 = dA1 * relu_deriv(self.Z1)
        else:
            dZ1 = dA1 * logistic_deriv(self.Z1)
        dW1 = np.dot(X.T, dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)
        
        # Update parameters.
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
    
    def train(self, X, y, epochs, learning_rate):
        losses = []
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = self.compute_loss(y_pred, y)
            losses.append(loss)
            self.backward(X, y, y_pred, learning_rate)
        return losses

# --- Shiny UI ---
app_ui = ui.page_fluid(
    ui.h2("Neural Network Training"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("epochs", "Number of Training Epochs", min=100, max=5000, value=1000, step=100),
            ui.input_select("activation", "Activation Function", choices=["ReLu", "Logistic"], selected="Logistic"),
            ui.input_slider("hidden_neurons", "Number of Neurons in Hidden Layers", min=1, max=20, value=4, step=1)
        ),
        ui.navset_tab(
            ui.nav_panel("Training Loss",
                ui.output_plot("lossPlot", height="400px")
            ),
            ui.nav_panel("Predictions",
                ui.output_plot("predictionPlot", height="400px")
            )
        )
    )
)

# --- Shiny Server ---
def server(input, output, session):
    
    @reactive.Calc
    def train_model():
        # Read the diamond dataset.
        df = pd.read_csv(StringIO(data_str))
        X = df["carat"].values.reshape(-1, 1)
        y = df["price"].values.reshape(-1, 1)
        
        # Standardize the data.
        X_mean, X_std = X.mean(), X.std()
        X_norm = (X - X_mean) / X_std
        
        y_mean, y_std = y.mean(), y.std()
        y_norm = (y - y_mean) / y_std
        
        # Choose the activation function.
        activation = input.activation().lower()
        
        # Get the number of neurons for the hidden layers.
        hidden_neurons = input.hidden_neurons()
        
        # Create and train the neural network.
        nn = NeuralNetwork(input_size=1, hidden_size1=hidden_neurons, hidden_size2=hidden_neurons, 
                           output_size=1, activation=activation)
        epochs = input.epochs()
        learning_rate = 0.01
        losses = nn.train(X_norm, y_norm, epochs, learning_rate)
        
        # Generate predictions.
        y_pred_norm = nn.forward(X_norm)
        y_pred = y_pred_norm * y_std + y_mean
        
        return {"losses": losses, "X": X, "y": y, "y_pred": y_pred}
    
    @output
    @render.plot
    def lossPlot():
        result = train_model()
        losses = result["losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="blue")
        ax.set_title("Training Loss over Epochs")
        ax.set_xlabel("Epoch")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def predictionPlot():
        result = train_model()
        X = result["X"]
        y = result["y"]
        y_pred = result["y_pred"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(X, y, color="blue", label="Actual Price")
        ax.scatter(X, y_pred, color="red", label="Predicted Price")
        ax.set_title("Actual vs. Predicted Diamond Price")
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.legend()
        return fig

app = App(app_ui, server)

```

In training a 'standard' neural network, we hope to obtain a point estimate of the parameter values that will have the most accuracy in predicting data.



## Bayesian Neural Network (Simplified Implementation)

The difference between a 'standard' neural network and a Bayesian one is that the Bayesian neural network will have uncertainty associated with the parameter values. (It could also have meaningful prior values for the parameters, but this is generally not relevant when training a new model). Here we will assume this uncertainty follows a normal/gaussian distribution. This uncertainty is key to making this method statistical/probabilistic - otherwise an input will always produce the same output. If the structure of the neural net is appropriate for the model and it has been well trained, a Bayesian neural network should mimic the variation/uncertainty in the outputs.



### More control on noise

```{shinylive-python}
#| standalone: true
#| viewerHeight: 900

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# --- Diamond Dataset ---
data_str = """ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67
32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69
16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66
32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69
3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48
16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61
5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48
34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67
28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54
32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57
16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71
31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63
51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64
27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71
37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69
16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69
35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71
30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64
36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69
30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69
32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7
35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72
20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64
34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69
33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69
32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68
32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71
44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68
38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74
43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67
33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68
35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72
37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7
33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7
26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71
33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7
36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69
36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74
13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69
30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74
29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73
30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71
46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71
32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7
20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75
34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67
27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72
29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7
30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72
31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74
35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74
50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73
32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75
29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74
37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64
42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75
30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71
23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79
18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78
31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78
34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8
13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79
36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75
37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77
30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73
36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77
34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7
30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9
33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81
26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76
27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76
34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74
28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93
39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96
33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83
36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9
37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87
41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86
35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87
27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83
41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93
37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86
36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88
37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92
38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85
35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89
31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81
30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96
39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85
30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93
17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9
35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88
33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93
39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96
31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94
33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01
41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95
42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88
30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87
41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89
35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98
24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97
35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98
37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98
36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97
28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96
43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91
41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99
37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98
40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94
37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92
39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98
46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15
38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03
41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1
50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13
43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14
35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05
41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01
37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1
37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16
38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16
33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88
43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09
44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13
32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02
39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17
42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05
49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22
39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16
47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05
38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05
49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19
39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16
44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22
47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19
44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23
49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21
41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11
47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15
48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16
41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21
48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17
43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26
46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19
43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25
36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22
44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18
37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17
48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2
44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17
46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16
46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23
50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24
43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38
47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25
41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23
42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35
48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28
52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23
49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28
52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25
50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24
46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22
46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19
45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3
46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31
41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35
45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35
43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26
45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29
42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24
3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47
51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54
50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51
46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32
53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5
1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53
2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47
52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43
52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4
52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48
44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63
46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4
47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5
2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37
48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48
51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57
46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64
49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47
52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46
2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51
277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53
809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47
52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53
946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56
51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48
48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6
51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57
3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59
51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51
694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56
50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57
45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48
49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54
2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55
1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58
50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55
53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55
48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61
3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51
51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53
53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56
7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82
7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8
8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79
1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73
53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66
1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52
580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67
47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59
1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63
53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63
4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74
4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62
1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62
682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6
113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87
3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88
9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86
53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71
4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61
4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89
9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94
8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82
14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97
12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02
3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93
6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7
9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87
4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89
10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93
9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86
13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75
18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81
3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79
5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68
15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98
10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0
5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94
8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91
4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8
9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01
10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83
15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03
5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86
9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04
15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95
7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05
9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94
15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92
11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06
15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95
10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89
4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99
18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0
15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99
16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0
11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94
16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91
11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99
13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95
13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91
9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96
9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02
10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05
12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98
15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0
8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06
17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03
7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03
16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0
11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92
5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03
15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03
7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04
14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0
8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01
7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99
21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04
48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22
16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01
15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08
18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11
7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08
8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07
20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15
13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18
11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06
19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0
16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0
15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05
17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11
14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1
21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04
4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12
16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09
11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05
16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07
21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1
8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11
18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13
13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13
20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16
12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08
14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22
22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23
8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2
18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21
26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22
20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27
12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27
15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11
18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15
16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23
10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24
12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19
7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21
23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22
20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24
20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24
15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23
24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57
26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53
25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49
6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36
10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25
14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39
15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32
12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32
21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61
22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6
20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53
19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52
16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59
19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49
19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31
15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36
24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42
16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38
20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58
21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29
25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77
24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62
22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63
22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55
19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54
20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54
24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59
21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65
23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67
19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45
19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58
25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53
27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75
27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66
25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69
24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64
15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7
25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57
20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6
17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58
24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14
24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69
22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93
27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2
26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""



# --- Activation Function (ReLU) and Its Derivative ---
def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x > 0).astype(float)

# --- Neural Network Class with SGLD and Customizable Noise & LR Decay ---
# Fixed architecture: 1-4-4-1 using ReLU.
class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):
        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size1))
        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)
        self.b2 = np.zeros((1, hidden_size2))
        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)
        self.b3 = np.zeros((1, output_size))
    
    def forward(self, X):
        self.Z1 = np.dot(X, self.W1) + self.b1
        self.A1 = relu(self.Z1)
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = relu(self.Z2)
        self.Z3 = np.dot(self.A2, self.W3) + self.b3
        return self.Z3
    
    def compute_loss(self, y_pred, y_true):
        return np.mean((y_pred - y_true)**2)
    
    def backward(self, X, y_true, y_pred, lr, noise_multiplier, noise_exponent):
        m = y_true.shape[0]
        dZ3 = (2.0 / m) * (y_pred - y_true)
        dW3 = np.dot(self.A2.T, dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        
        dA2 = np.dot(dZ3, self.W3.T)
        dZ2 = dA2 * relu_deriv(self.Z2)
        dW2 = np.dot(self.A1.T, dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        
        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * relu_deriv(self.Z1)
        dW1 = np.dot(X.T, dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)
        
        # Customizable SGLD update:
        # Compute noise standard deviation based on the user-specified exponent and multiplier.
        noise_std = noise_multiplier * (lr ** noise_exponent)
        self.W3 = self.W3 - (lr/2)*dW3 + np.random.normal(0, noise_std, self.W3.shape)
        self.b3 = self.b3 - (lr/2)*db3 + np.random.normal(0, noise_std, self.b3.shape)
        self.W2 = self.W2 - (lr/2)*dW2 + np.random.normal(0, noise_std, self.W2.shape)
        self.b2 = self.b2 - (lr/2)*db2 + np.random.normal(0, noise_std, self.b2.shape)
        self.W1 = self.W1 - (lr/2)*dW1 + np.random.normal(0, noise_std, self.W1.shape)
        self.b1 = self.b1 - (lr/2)*db1 + np.random.normal(0, noise_std, self.b1.shape)
    
    def train(self, X, y, epochs, learning_rate, burn_in, sample_interval, noise_multiplier, noise_exponent, lr_decay_factor):
        burn_in_losses = []
        sampling_losses = []
        samples = []
        
        lr = learning_rate
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = self.compute_loss(y_pred, y)
            if epoch < burn_in:
                burn_in_losses.append(loss)
            else:
                sampling_losses.append(loss)
            self.backward(X, y, y_pred, lr, noise_multiplier, noise_exponent)
            # At the end of burn-in, decay the learning rate.
            if epoch == burn_in:
                lr = learning_rate * lr_decay_factor
            # After burn-in, record weight samples at intervals.
            if epoch >= burn_in and (epoch - burn_in) % sample_interval == 0:
                sample = {
                    "W1": self.W1.copy(),
                    "b1": self.b1.copy(),
                    "W2": self.W2.copy(),
                    "b2": self.b2.copy(),
                    "W3": self.W3.copy(),
                    "b3": self.b3.copy()
                }
                samples.append(sample)
        return burn_in_losses, sampling_losses, samples

# --- Helper: Forward Pass with a Given Weight Sample ---
def forward_with_sample(X, sample):
    Z1 = np.dot(X, sample["W1"]) + sample["b1"]
    A1 = relu(Z1)
    Z2 = np.dot(A1, sample["W2"]) + sample["b2"]
    A2 = relu(Z2)
    Z3 = np.dot(A2, sample["W3"]) + sample["b3"]
    return Z3

# --- Shiny UI ---
app_ui = ui.page_fluid(
    ui.h2("Bayesian Neural Network with SGLD - Custom Noise and LR Decay"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("epochs", "Training Epochs", min=100, max=3000, value=1000, step=100),
            ui.input_slider("learning_rate", "Learning Rate", min=0.0001, max=0.01, value=0.01, step=0.0001),
            ui.input_slider("burn_in", "Burn-in Epochs", min=0, max=1000, value=500, step=50),
            ui.input_slider("sample_interval", "Sample Interval", min=1, max=50, value=10, step=1),
            ui.input_slider("noise_exponent", "Noise Exponent", min=0.1, max=1.0, value=0.5, step=0.05),
            ui.input_slider("noise_multiplier", "Noise Multiplier", min=0.1, max=10.0, value=1.0, step=0.1),
            ui.input_slider("lr_decay", "Learning Rate Decay Factor", min=0.1, max=1.0, value=0.1, step=0.05)
        ),
        ui.navset_tab(
            ui.nav_panel("Burn-in Loss",
                ui.output_plot("burnInLossPlot", height="400px")
            ),
            ui.nav_panel("Sampling Loss",
                ui.output_plot("samplingLossPlot", height="400px")
            ),
            ui.nav_panel("Predictions",
                ui.output_plot("predictionPlot", height="400px")
            )
        )
    )
)

# --- Shiny Server ---
def server(input, output, session):
    
    @reactive.Calc
    def train_model():
        # Read and preprocess data.
        df = pd.read_csv(StringIO(data_str))
        X = df["carat"].values.reshape(-1, 1)
        y = df["price"].values.reshape(-1, 1)
        X_mean, X_std = X.mean(), X.std()
        X_norm = (X - X_mean) / X_std
        y_mean, y_std = y.mean(), y.std()
        y_norm = (y - y_mean) / y_std
        
        # Get hyperparameters from the UI.
        epochs = input.epochs()
        learning_rate = input.learning_rate()
        burn_in = input.burn_in()
        sample_interval = input.sample_interval()
        noise_exponent = input.noise_exponent()
        noise_multiplier = input.noise_multiplier()
        lr_decay = input.lr_decay()
        
        # Create and train the network using SGLD.
        nn = NeuralNetwork(input_size=1, hidden_size1=4, hidden_size2=4, output_size=1)
        burn_in_losses, sampling_losses, samples = nn.train(
            X_norm, y_norm, epochs, learning_rate, burn_in, sample_interval,
            noise_multiplier, noise_exponent, lr_decay
        )
        
        # Compute predictions from the collected samples.
        preds = []
        for sample in samples:
            pred_norm = forward_with_sample(X_norm, sample)
            preds.append(pred_norm)
        preds = np.array(preds)
        pred_mean_norm = preds.mean(axis=0)
        pred_std_norm = preds.std(axis=0)
        pred_mean = pred_mean_norm * y_std + y_mean
        pred_std = pred_std_norm * y_std
        
        return {"burn_in_losses": burn_in_losses,
                "sampling_losses": sampling_losses,
                "X": X, "y": y,
                "pred_mean": pred_mean, "pred_std": pred_std}
    
    @output
    @render.plot
    def burnInLossPlot():
        result = train_model()
        losses = result["burn_in_losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="green")
        ax.set_title("Burn-in Phase Loss")
        ax.set_xlabel("Epoch (burn-in)")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def samplingLossPlot():
        result = train_model()
        losses = result["sampling_losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="purple")
        ax.set_title("Sampling Phase Loss (Post Burn-in)")
        ax.set_xlabel("Epoch (sampling)")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def predictionPlot():
        result = train_model()
        X = result["X"].flatten()
        y = result["y"].flatten()
        pred_mean = result["pred_mean"].flatten()
        pred_std = result["pred_std"].flatten()
        order = np.argsort(X, axis=0)
        X_sorted = X[order]
        pred_mean_sorted = pred_mean[order]
        pred_std_sorted = pred_std[order]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(X, y, color="blue", label="Actual Price")
        ax.plot(X_sorted, pred_mean_sorted, color="red", label="Predicted Mean")
        ax.fill_between(X_sorted,
                        pred_mean_sorted - 2*pred_std_sorted,
                        pred_mean_sorted + 2*pred_std_sorted,
                        color="red", alpha=0.3, label="Uncertainty (±2σ)")
        ax.set_title("Actual vs. Predicted Diamond Price (SGLD)")
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.legend()
        return fig

app = App(app_ui, server)


```

Training a Bayesian neural network occurs in effectively two steps:

1) The 'burn-in' phase, this is similar in some sense to the point estimate of the parameters in a 'standard' neural network. Here we want to ensure we have found the parameter space that is somewhat close to the optimal parameter values. This is contrast to being in some random combination of parameters that have practically zero chance of being correct.
2) The inference phase should explore the parameter values from their near-optimum to where they start to become totally unlikely. If done well we can be confident we know not just the most likely value of the parameter, but how uncertain we are in that parameter value.

We can expect that parameters with impactful values to the results and small uncertainty are very important to the model. The opposite is true as well, if a parameter has a large range of uncertainty, and is not impactful, which typically means the estimated range of the parameter includes zero, then we can conclude the parameter is not improtant to the model.


## Discussion

This second section is only a rough approximation of Bayesian updating. A more formal approach involves a well-defined posterior, which typically requires advanced methods. However, this snippet shows how the concept of a prior on weights can be approximated by combining a prior-based penalty with noise around the final solution to visualize uncertainty.

**Key takeaways and caveats:**
- Implementing a neural network from scratch with NumPy requires manual forward/backprop calculations.
- True Bayesian neural networks often rely on MCMC or variational inference.
- Weight decay alone is not fully Bayesian, but it is consistent with a Gaussian prior if used carefully.
- Adding noise around learned weights is only a heuristic approach to capturing model uncertainty, not a rigorous posterior sampling method.

## Conclusion

We have demonstrated how to implement (1) a basic neural network using only NumPy and manual gradient-based learning and (2) a very simplified approximation of Bayesian neural networks by imposing a Gaussian prior on the weights. While this example is primarily for educational purposes and lacks the efficiency, stability, and full correctness of modern Bayesian frameworks, it should help illustrate the main ideas in a more “bare-metal” approach.




## With Diamond Dataset


```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# ---------------------------
# 1. Load the diamond dataset from the provided CSV string
data_str = """
ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67
32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69
16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66
32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69
3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48
16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61
5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48
34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67
28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54
32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57
16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71
31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63
51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64
27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71
37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69
16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69
35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71
30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64
36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69
30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69
32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7
35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72
20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64
34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69
33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69
32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68
32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71
44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68
38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74
43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67
33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68
35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72
37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7
33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7
26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71
33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7
36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69
36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74
13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69
30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74
29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73
30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71
46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71
32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7
20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75
34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67
27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72
29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7
30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72
31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74
35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74
50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73
32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75
29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74
37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64
42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75
30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71
23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79
18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78
31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78
34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8
13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79
36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75
37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77
30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73
36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77
34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7
30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9
33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81
26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76
27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76
34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74
28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93
39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96
33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83
36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9
37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87
41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86
35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87
27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83
41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93
37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86
36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88
37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92
38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85
35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89
31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81
30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96
39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85
30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93
17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9
35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88
33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93
39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96
31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94
33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01
41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95
42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88
30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87
41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89
35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98
24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97
35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98
37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98
36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97
28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96
43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91
41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99
37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98
40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94
37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92
39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98
46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15
38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03
41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1
50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13
43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14
35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05
41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01
37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1
37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16
38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16
33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88
43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09
44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13
32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02
39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17
42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05
49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22
39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16
47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05
38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05
49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19
39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16
44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22
47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19
44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23
49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21
41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11
47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15
48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16
41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21
48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17
43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26
46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19
43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25
36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22
44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18
37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17
48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2
44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17
46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16
46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23
50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24
43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38
47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25
41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23
42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35
48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28
52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23
49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28
52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25
50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24
46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22
46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19
45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3
46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31
41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35
45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35
43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26
45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29
42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24
3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47
51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54
50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51
46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32
53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5
1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53
2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47
52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43
52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4
52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48
44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63
46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4
47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5
2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37
48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48
51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57
46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64
49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47
52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46
2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51
277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53
809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47
52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53
946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56
51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48
48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6
51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57
3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59
51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51
694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56
50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57
45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48
49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54
2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55
1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58
50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55
53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55
48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61
3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51
51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53
53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56
7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82
7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8
8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79
1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73
53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66
1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52
580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67
47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59
1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63
53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63
4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74
4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62
1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62
682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6
113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87
3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88
9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86
53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71
4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61
4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89
9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94
8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82
14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97
12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02
3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93
6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7
9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87
4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89
10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93
9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86
13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75
18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81
3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79
5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68
15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98
10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0
5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94
8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91
4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8
9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01
10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83
15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03
5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86
9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04
15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95
7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05
9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94
15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92
11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06
15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95
10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89
4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99
18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0
15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99
16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0
11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94
16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91
11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99
13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95
13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91
9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96
9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02
10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05
12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98
15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0
8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06
17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03
7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03
16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0
11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92
5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03
15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03
7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04
14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0
8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01
7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99
21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04
48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22
16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01
15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08
18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11
7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08
8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07
20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15
13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18
11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06
19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0
16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0
15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05
17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11
14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1
21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04
4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12
16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09
11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05
16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07
21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1
8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11
18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13
13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13
20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16
12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08
14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22
22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23
8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2
18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21
26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22
20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27
12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27
15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11
18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15
16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23
10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24
12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19
7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21
23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22
20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24
20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24
15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23
24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57
26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53
25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49
6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36
10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25
14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39
15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32
12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32
21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61
22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6
20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53
19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52
16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59
19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49
19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31
15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36
24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42
16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38
20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58
21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29
25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77
24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62
22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63
22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55
19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54
20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54
24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59
21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65
23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67
19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45
19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58
25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53
27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75
27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66
25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69
24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64
15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7
25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57
20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6
17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58
24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14
24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69
22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93
27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2
26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""


# Read the CSV data into a DataFrame and extract the features and target.
data = pd.read_csv(StringIO(data_str))
# For this example, we use 'carat' as our single input feature and 'price' as the target.
X = data['carat'].values.reshape(-1, 1)  # shape: (n_samples, 1)
y = data['price'].values.reshape(-1, 1)    # shape: (n_samples, 1)
n_samples = X.shape[0]

# ---------------------------
# 2. Define helper functions and network components

# Activation: ReLU
def relu(z):
    return np.maximum(0, z)

# Forward pass given weights (so we can re-use for standard and Bayesian nets)
def forward_pass(x, W1, b1, W2, b2):
    z1 = np.dot(x, W1) + b1   # hidden pre-activation
    a1 = relu(z1)             # hidden activation
    z2 = np.dot(a1, W2) + b2   # output (regression)
    return z1, a1, z2

# Mean Squared Error loss
def mse_loss(pred, target):
    return 0.5 * np.mean((pred - target) ** 2)

# ---------------------------
# 3. Standard Neural Network (NumPy implementation)

# Architecture: 1 input, 16 hidden ReLU units, 1 output.
n_input = 1
n_hidden = 16
n_output = 1

# Initialize weights (small random values) and biases (zeros)
np.random.seed(42)
W1 = 0.01 * np.random.randn(n_input, n_hidden)
b1 = np.zeros(n_hidden)
W2 = 0.01 * np.random.randn(n_hidden, n_output)
b2 = np.zeros(n_output)

# Training parameters
learning_rate = 1e-2
nepochs = 1000
loss_history = []

# Standard NN training loop (full batch)
for epoch in range(nepochs):
    # Forward pass
    z1, a1, z2 = forward_pass(X, W1, b1, W2, b2)
    loss = mse_loss(z2, y)
    loss_history.append(loss)
    
    # Backpropagation (gradients)
    dz2 = (z2 - y) / n_samples
    dW2 = a1.T.dot(dz2)
    db2 = np.sum(dz2, axis=0)
    
    da1 = dz2.dot(W2.T)
    dz1 = da1.copy()
    dz1[z1 <= 0] = 0  # gradient through ReLU
    dW1 = X.T.dot(dz1)
    db1 = np.sum(dz1, axis=0)
    
    # Update parameters
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1

# Plot training loss for Standard NN
plt.figure(figsize=(7, 4))
plt.plot(loss_history)
plt.title("Standard NN Training Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.show()

# Plot predictions: scatter of actual data and NN regression curve.
# Create a grid over the range of carat values.
X_grid = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
_, _, y_pred = forward_pass(X_grid, W1, b1, W2, b2)
plt.figure(figsize=(7, 4))
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X_grid, y_pred, color='red', label='NN Prediction')
plt.title("Standard NN: Price vs Carat")
plt.xlabel("Carat")
plt.ylabel("Price")
plt.legend()
plt.show()

# ---------------------------
# 4. Bayesian Neural Network (Simple Implementation)
#
# Here we “inject” a Gaussian prior into the training by penalizing deviation
# from the standard NN weights (which serve as our prior means).
# Note: This is a rough approximation to illustrate the idea.

# Save standard NN weights as the prior means.
prior_W1 = W1.copy()
prior_b1 = b1.copy()
prior_W2 = W2.copy()
prior_b2 = b2.copy()

# Reinitialize Bayesian network weights from the prior (you could also add extra noise)
W1_bayes = prior_W1.copy()
b1_bayes = prior_b1.copy()
W2_bayes = prior_W2.copy()
b2_bayes = prior_b2.copy()

# Prior variance (a hyperparameter controlling how strongly we “pull” toward the prior)
prior_variance = 0.1

# Define functions for the negative log prior and its gradient.
def neg_log_prior(w, w_prior, var):
    return 0.5 * np.sum((w - w_prior) ** 2) / var

def grad_neg_log_prior(w, w_prior, var):
    return (w - w_prior) / var

# Bayesian training parameters
learning_rate_bayes = 1e-2
nepochs_bayes = 200
loss_history_bayes = []

# Bayesian NN training loop.
for epoch in range(nepochs_bayes):
    # Forward pass using Bayesian weights.
    z1, a1, z2 = forward_pass(X, W1_bayes, b1_bayes, W2_bayes, b2_bayes)
    data_loss = mse_loss(z2, y)
    
    # Compute the prior loss over all weights.
    prior_loss = (neg_log_prior(W1_bayes, prior_W1, prior_variance) +
                  neg_log_prior(b1_bayes, prior_b1, prior_variance) +
                  neg_log_prior(W2_bayes, prior_W2, prior_variance) +
                  neg_log_prior(b2_bayes, prior_b2, prior_variance))
    
    # Total loss (data loss + scaled prior loss)
    loss = data_loss + prior_loss / n_samples
    loss_history_bayes.append(loss)
    
    # Backpropagation for data loss:
    dz2 = (z2 - y) / n_samples
    dW2 = a1.T.dot(dz2)
    db2 = np.sum(dz2, axis=0)
    
    da1 = dz2.dot(W2_bayes.T)
    dz1 = da1.copy()
    dz1[z1 <= 0] = 0
    dW1 = X.T.dot(dz1)
    db1 = np.sum(dz1, axis=0)
    
    # Add gradients from the prior (for each parameter, scaled by 1/n_samples)
    dW1 += grad_neg_log_prior(W1_bayes, prior_W1, prior_variance) / n_samples
    db1 += grad_neg_log_prior(b1_bayes, prior_b1, prior_variance) / n_samples
    dW2 += grad_neg_log_prior(W2_bayes, prior_W2, prior_variance) / n_samples
    db2 += grad_neg_log_prior(b2_bayes, prior_b2, prior_variance) / n_samples
    
    # Update Bayesian parameters
    W1_bayes -= learning_rate_bayes * dW1
    b1_bayes -= learning_rate_bayes * db1
    W2_bayes -= learning_rate_bayes * dW2
    b2_bayes -= learning_rate_bayes * db2

# Plot training loss for Bayesian NN
plt.figure(figsize=(7, 4))
plt.plot(loss_history_bayes)
plt.title("Bayesian NN Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

# ---------------------------
# 5. Uncertainty Predictions for the Bayesian NN
#
# To illustrate uncertainty, we will sample several sets of weights by perturbing
# the final Bayesian weights with Gaussian noise (with standard deviation sqrt(prior_variance)).
n_samples_pred = 100
predictions = []
std_weight = np.sqrt(prior_variance)  # sampling std deviation

# Create prediction grid
X_grid = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
for i in range(n_samples_pred):
    sample_W1 = W1_bayes + np.random.randn(*W1_bayes.shape) * std_weight
    sample_b1 = b1_bayes + np.random.randn(*b1_bayes.shape) * std_weight
    sample_W2 = W2_bayes + np.random.randn(*W2_bayes.shape) * std_weight
    sample_b2 = b2_bayes + np.random.randn(*b2_bayes.shape) * std_weight
    _, _, y_sample = forward_pass(X_grid, sample_W1, sample_b1, sample_W2, sample_b2)
    predictions.append(y_sample.flatten())

predictions = np.array(predictions)
mean_pred = predictions.mean(axis=0)
std_pred = predictions.std(axis=0)

# Plot the Bayesian NN predictions with uncertainty bands.
plt.figure(figsize=(7, 4))
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X_grid, mean_pred, color='red', label='Mean Prediction')
plt.fill_between(X_grid.flatten(), mean_pred - std_pred, mean_pred + std_pred,
                 color='red', alpha=0.2, label='Prediction Uncertainty')
plt.title("Bayesian NN: Price vs Carat with Uncertainty")
plt.xlabel("Carat")
plt.ylabel("Price")
plt.legend()
plt.show()

# ---------------------------
# 6. Additional Chart: Weight Distributions
#
# Compare the learned weights for the first layer (W1) in the Bayesian NN
# with the prior weights (saved from the standard NN).
plt.figure(figsize=(7, 4))
plt.hist(W1_bayes.flatten(), bins=20, alpha=0.7, label='W1 (Bayesian)')
plt.hist(prior_W1.flatten(), bins=20, alpha=0.7, label='Prior W1')
plt.title("Histogram of W1 Values: Bayesian vs Prior")
plt.xlabel("Weight value")
plt.ylabel("Frequency")
plt.legend()
plt.show()

```
