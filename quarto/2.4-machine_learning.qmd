---
title: Machine Learning
format: 
    html:
        mermaid:
            theme: neutral
filters:
  - shinylive
execute:
  echo: false
---

::: {.callout-warning}
## Warning
This chapter is still rough. It's interesting material, but for the core purpose of the primer, it's hard to prioritize for a v0 release.
:::

## Preview

Due to popular demand, we move on from traditional statistical models to those considered *machine learning*. We touch on the differences and similarities between the two categories before choosing to continue with a machine learning model that allows for statistical (i.e. probabilistic) interpretation.

Although the chapter is entitled Machine Learning, we make no attempt to cover the topic broadly. We simply choose a method that fits well in both statistical and machine learning contexts and explore it in detail. The resources for more comprehensive introductions to Machine Learning are easily found elsewhere.

The chapter is focused on [Bayesian] Neural Networks, and although they are an advanced topic, you may be surprised to learn the major building blocks have already been covered. In the flavor we use, each hidden layer neuron is just logistic regression (which is just generalized linear regression) built into a network. Once we build this network we will allow it to generate data like in previous chapters. We can choose to create ensembles of Neural Networks, or probabilistically sample likely parameter values, which will allow us to calculate P(D\|M).

### Interactive Neural Network Visualization

This visualization shows a neural network with 1 input (carat), 2 hidden layers (3 neurons each), 
and 1 output (price). The network performs regression with sigmoid-activated hidden layers.

```{=html}
<style>
  .container {
    margin-top: 20px;
  }
  .controls {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    margin-bottom: 1rem;
    padding: 1rem;
    background-color: #f5f5f5;
    border-radius: 4px;
  }
  .control {
    display: flex;
    flex-direction: column;
  }
  .control label {
    margin-bottom: 5px;
    font-weight: bold;
  }
  .tabs {
    display: flex;
    border-bottom: 1px solid #ccc;
    margin-top: 20px;
  }
  .tab {
    padding: 0.5rem 1rem;
    cursor: pointer;
    border: 1px solid transparent;
    border-bottom: none;
    margin-bottom: -1px;
  }
  .tab.active {
    border-color: #ccc;
    border-bottom-color: white;
    background-color: white;
  }
  .tab-content {
    display: none;
    padding: 1rem;
    border: 1px solid #ccc;
    border-top: none;
  }
  .tab-content.active {
    display: block;
  }
  table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 20px;
  }
  th, td {
    padding: 8px;
    text-align: left;
    border-bottom: 1px solid #ddd;
  }
  th {
    font-weight: bold;
  }
  .neuronsContainer {
    margin-top: 20px;
  }
</style>

<div class="controls">
  <div class="control">
    <label for="learningRate">Learning Rate: <span id="learningRateValue">0.01</span></label>
    <input type="range" id="learningRate" min="0.001" max="0.1" step="0.001" value="0.01">
  </div>
  <div class="control">
    <label for="epochs">Training Epochs: <span id="epochsValue">100</span></label>
    <input type="range" id="epochs" min="1" max="500" step="10" value="100">
  </div>
  <div class="control">
    <label for="trainingPct">Training Data %: <span id="trainingPctValue">70%</span></label>
    <input type="range" id="trainingPct" min="0.5" max="0.9" step="0.05" value="0.7">
  </div>
  <div class="control">
    <button id="trainButton">Train Network</button>
  </div>
</div>

<div class="tabs">
  <div class="tab active" data-tab="network">Neural Network</div>
  <div class="tab" data-tab="metrics">Performance Metrics</div>
  <div class="tab" data-tab="predictions">Predictions</div>
  <div class="tab" data-tab="data">Data</div>
</div>

<div id="network" class="tab-content active">
  <div id="neuralNetworkViz"></div>
</div>

<div id="metrics" class="tab-content">
  <div id="metricsTable"></div>
  <div id="trainingChart"></div>
</div>

<div id="predictions" class="tab-content">
  <div id="predictionsChart"></div>
</div>

<div id="data" class="tab-content">
  <div id="dataTable"></div>
</div>

<script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
<script>
  // Global variables
  let data = [];
  let trainingData = [];
  let testData = [];
  let network;
  let carat_mean, carat_std, price_mean, price_std;
  let trainingErrors = [];
  let predictionLine = [];
  let metrics = {};

  // Set up tabs
  document.querySelectorAll('.tab').forEach(tab => {
    tab.addEventListener('click', () => {
      document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
      document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
      
      tab.classList.add('active');
      document.getElementById(tab.dataset.tab).classList.add('active');
    });
  });

  // Set up sliders
  document.getElementById('learningRate').addEventListener('input', function() {
    document.getElementById('learningRateValue').textContent = this.value;
  });
  
  document.getElementById('epochs').addEventListener('input', function() {
    document.getElementById('epochsValue').textContent = this.value;
  });
  
  document.getElementById('trainingPct').addEventListener('input', function() {
    document.getElementById('trainingPctValue').textContent = (this.value * 100) + '%';
  });

  // Data from sampled_diamonds.csv (carat and price only)
  const sampleData = [
    { carat: 0.3, price: 545 },
    { carat: 0.3, price: 878 },
    { carat: 0.3, price: 590 },
    { carat: 0.3, price: 526 },
    { carat: 0.3, price: 776 },
    { carat: 0.27, price: 500 },
    { carat: 0.3, price: 540 },
    { carat: 0.3, price: 447 },
    { carat: 0.3, price: 605 },
    { carat: 0.3, price: 789 },
    { carat: 0.3, price: 421 },
    { carat: 0.3, price: 789 },
    { carat: 0.27, price: 567 },
    { carat: 0.3, price: 608 },
    { carat: 0.25, price: 575 },
    { carat: 0.3, price: 873 },
    { carat: 0.27, price: 682 },
    { carat: 0.3, price: 796 },
    { carat: 0.3, price: 608 },
    { carat: 0.3, price: 453 },
    { carat: 0.31, price: 544 },
    { carat: 0.31, price: 651 },
    { carat: 0.31, price: 979 },
    { carat: 0.31, price: 421 },
    { carat: 0.31, price: 914 },
    { carat: 0.31, price: 725 },
    { carat: 0.31, price: 921 },
    { carat: 0.31, price: 742 },
    { carat: 0.31, price: 802 },
    { carat: 0.31, price: 907 },
    { carat: 0.31, price: 625 },
    { carat: 0.31, price: 871 },
    { carat: 0.31, price: 462 },
    { carat: 0.31, price: 802 },
    { carat: 0.31, price: 802 },
    { carat: 0.31, price: 523 },
    { carat: 0.31, price: 1046 },
    { carat: 0.31, price: 507 },
    { carat: 0.31, price: 816 },
    { carat: 0.31, price: 891 },
    { carat: 0.32, price: 990 },
    { carat: 0.32, price: 828 },
    { carat: 0.32, price: 645 },
    { carat: 0.32, price: 814 },
    { carat: 0.32, price: 477 },
    { carat: 0.32, price: 477 },
    { carat: 0.32, price: 602 },
    { carat: 0.32, price: 449 },
    { carat: 0.32, price: 442 },
    { carat: 0.32, price: 720 },
    { carat: 0.32, price: 528 },
    { carat: 0.32, price: 803 },
    { carat: 0.32, price: 421 },
    { carat: 0.32, price: 854 },
    { carat: 0.32, price: 653 },
    { carat: 0.32, price: 716 },
    { carat: 0.32, price: 720 },
    { carat: 0.32, price: 758 },
    { carat: 0.32, price: 918 },
    { carat: 0.32, price: 544 },
    { carat: 0.33, price: 797 },
    { carat: 0.33, price: 713 },
    { carat: 0.33, price: 984 },
    { carat: 0.33, price: 1312 },
    { carat: 0.34, price: 727 },
    { carat: 0.33, price: 631 },
    { carat: 0.35, price: 620 },
    { carat: 0.34, price: 760 },
    { carat: 0.35, price: 868 },
    { carat: 0.35, price: 601 },
    { carat: 0.34, price: 961 },
    { carat: 0.33, price: 965 },
    { carat: 0.33, price: 743 },
    { carat: 0.34, price: 477 },
    { carat: 0.33, price: 854 },
    { carat: 0.35, price: 738 },
    { carat: 0.35, price: 820 },
    { carat: 0.35, price: 644 },
    { carat: 0.33, price: 646 },
    { carat: 0.33, price: 854 },
    { carat: 0.4, price: 666 },
    { carat: 0.4, price: 1080 },
    { carat: 0.36, price: 835 },
    { carat: 0.4, price: 945 },
    { carat: 0.4, price: 982 },
    { carat: 0.38, price: 1257 },
    { carat: 0.4, price: 912 },
    { carat: 0.37, price: 649 },
    { carat: 0.4, price: 1238 },
    { carat: 0.38, price: 998 },
    { carat: 0.37, price: 936 },
    { carat: 0.4, price: 980 },
    { carat: 0.36, price: 1018 },
    { carat: 0.38, price: 899 },
    { carat: 0.37, price: 749 },
    { carat: 0.4, price: 737 },
    { carat: 0.37, price: 491 },
    { carat: 0.4, price: 737 },
    { carat: 0.39, price: 614 },
    { carat: 0.38, price: 898 },
    { carat: 0.41, price: 827 },
    { carat: 0.41, price: 1079 },
    { carat: 0.42, price: 773 },
    { carat: 0.41, price: 844 },
    { carat: 0.41, price: 1243 },
    { carat: 0.41, price: 1286 },
    { carat: 0.41, price: 719 },
    { carat: 0.41, price: 1230 },
    { carat: 0.41, price: 904 },
    { carat: 0.41, price: 638 },
    { carat: 0.42, price: 898 },
    { carat: 0.41, price: 969 },
    { carat: 0.42, price: 963 },
    { carat: 0.41, price: 671 },
    { carat: 0.42, price: 1400 },
    { carat: 0.41, price: 1186 },
    { carat: 0.42, price: 992 },
    { carat: 0.41, price: 1122 },
    { carat: 0.41, price: 1007 },
    { carat: 0.41, price: 1076 },
    { carat: 0.5, price: 1747 },
    { carat: 0.45, price: 1046 },
    { carat: 0.46, price: 1227 },
    { carat: 0.5, price: 2243 },
    { carat: 0.5, price: 1415 },
    { carat: 0.43, price: 894 },
    { carat: 0.44, price: 1253 },
    { carat: 0.5, price: 978 },
    { carat: 0.5, price: 982 },
    { carat: 0.5, price: 1015 },
    { carat: 0.43, price: 813 },
    { carat: 0.46, price: 1412 },
    { carat: 0.5, price: 1624 },
    { carat: 0.43, price: 792 },
    { carat: 0.5, price: 1080 },
    { carat: 0.46, price: 1299 },
    { carat: 0.5, price: 2070 },
    { carat: 0.48, price: 1088 },
    { carat: 0.5, price: 1819 },
    { carat: 0.47, price: 1021 },
    { carat: 0.51, price: 2146 },
    { carat: 0.53, price: 1070 },
    { carat: 0.53, price: 1607 },
    { carat: 0.53, price: 1881 },
    { carat: 0.53, price: 1607 },
    { carat: 0.51, price: 2185 },
    { carat: 0.51, price: 1204 },
    { carat: 0.52, price: 1878 },
    { carat: 0.52, price: 1988 },
    { carat: 0.52, price: 1227 },
    { carat: 0.51, price: 2041 },
    { carat: 0.53, price: 1395 },
    { carat: 0.51, price: 1781 },
    { carat: 0.52, price: 1385 },
    { carat: 0.51, price: 959 },
    { carat: 0.51, price: 1577 },
    { carat: 0.52, price: 971 },
    { carat: 0.52, price: 1936 },
    { carat: 0.51, price: 1574 },
    { carat: 0.51, price: 1776 },
    { carat: 0.54, price: 1774 },
    { carat: 0.54, price: 2202 },
    { carat: 0.58, price: 1430 },
    { carat: 0.56, price: 1819 },
    { carat: 0.54, price: 1259 },
    { carat: 0.59, price: 1265 },
    { carat: 0.55, price: 2030 },
    { carat: 0.54, price: 2479 },
    { carat: 0.56, price: 2118 },
    { carat: 0.55, price: 2499 },
    { carat: 0.54, price: 2271 },
    { carat: 0.54, price: 1725 },
    { carat: 0.54, price: 1772 },
    { carat: 0.56, price: 1712 },
    { carat: 0.58, price: 1761 },
    { carat: 0.6, price: 1250 },
    { carat: 0.59, price: 1652 },
    { carat: 0.54, price: 1392 },
    { carat: 0.56, price: 1698 },
    { carat: 0.56, price: 1287 },
    { carat: 0.7, price: 3359 },
    { carat: 0.7, price: 2362 },
    { carat: 0.7, price: 2317 },
    { carat: 0.63, price: 1736 },
    { carat: 0.7, price: 2730 },
    { carat: 0.7, price: 3001 },
    { carat: 0.7, price: 3201 },
    { carat: 0.7, price: 2563 },
    { carat: 0.7, price: 2525 },
    { carat: 0.7, price: 2473 },
    { carat: 0.7, price: 1564 },
    { carat: 0.64, price: 1811 },
    { carat: 0.7, price: 1843 },
    { carat: 0.63, price: 3181 },
    { carat: 0.7, price: 2039 },
    { carat: 0.7, price: 2394 },
    { carat: 0.7, price: 1749 },
    { carat: 0.7, price: 2176 },
    { carat: 0.7, price: 2444 },
    { carat: 0.7, price: 3300 },
    { carat: 0.71, price: 2795 },
    { carat: 0.71, price: 2863 },
    { carat: 0.72, price: 2583 },
    { carat: 0.72, price: 2889 },
    { carat: 0.71, price: 2400 },
    { carat: 0.72, price: 1942 },
    { carat: 0.72, price: 2398 },
    { carat: 0.72, price: 3465 },
    { carat: 0.71, price: 2343 },
    { carat: 0.71, price: 2853 },
    { carat: 0.72, price: 2311 },
    { carat: 0.71, price: 1717 },
    { carat: 0.72, price: 2148 },
    { carat: 0.72, price: 3124 },
    { carat: 0.71, price: 2930 },
    { carat: 0.71, price: 2294 },
    { carat: 0.71, price: 2633 },
    { carat: 0.71, price: 2036 },
    { carat: 0.71, price: 3431 },
    { carat: 0.71, price: 2423 },
    { carat: 0.74, price: 2709 },
    { carat: 0.9, price: 4198 },
    { carat: 0.9, price: 4314 },
    { carat: 0.9, price: 4435 },
    { carat: 0.8, price: 2914 },
    { carat: 0.75, price: 2613 },
    { carat: 0.76, price: 2937 },
    { carat: 0.78, price: 2834 },
    { carat: 0.74, price: 1913 },
    { carat: 0.77, price: 2988 },
    { carat: 0.76, price: 2680 },
    { carat: 0.84, price: 3577 },
    { carat: 0.76, price: 3671 },
    { carat: 0.78, price: 3055 },
    { carat: 0.75, price: 2850 },
    { carat: 0.9, price: 2761 },
    { carat: 0.9, price: 3350 },
    { carat: 0.9, price: 4592 },
    { carat: 0.83, price: 2666 },
    { carat: 0.74, price: 3537 },
    { carat: 0.91, price: 3567 },
    { carat: 1.0, price: 4626 },
    { carat: 0.95, price: 4341 },
    { carat: 1.0, price: 5914 },
    { carat: 1.0, price: 5148 },
    { carat: 1.0, price: 3465 },
    { carat: 0.97, price: 4063 },
    { carat: 1.0, price: 4626 },
    { carat: 0.92, price: 3684 },
    { carat: 1.0, price: 4816 },
    { carat: 0.91, price: 4668 },
    { carat: 1.0, price: 5484 },
    { carat: 1.0, price: 7500 },
    { carat: 0.91, price: 3423 },
    { carat: 1.0, price: 3830 },
    { carat: 1.0, price: 6377 },
    { carat: 1.0, price: 4861 },
    { carat: 1.0, price: 3920 },
    { carat: 0.91, price: 4389 },
    { carat: 0.91, price: 3546 },
    { carat: 1.01, price: 4588 },
    { carat: 1.01, price: 4821 },
    { carat: 1.01, price: 6097 },
    { carat: 1.01, price: 3945 },
    { carat: 1.01, price: 4559 },
    { carat: 1.01, price: 6075 },
    { carat: 1.01, price: 4276 },
    { carat: 1.01, price: 4513 },
    { carat: 1.01, price: 6295 },
    { carat: 1.01, price: 4989 },
    { carat: 1.01, price: 6105 },
    { carat: 1.01, price: 4899 },
    { carat: 1.01, price: 3682 },
    { carat: 1.01, price: 7652 },
    { carat: 1.01, price: 6221 },
    { carat: 1.01, price: 6516 },
    { carat: 1.01, price: 4916 },
    { carat: 1.01, price: 6697 },
    { carat: 1.01, price: 4977 },
    { carat: 1.01, price: 5543 },
    { carat: 1.02, price: 5553 },
    { carat: 1.03, price: 4522 },
    { carat: 1.02, price: 4540 },
    { carat: 1.03, price: 4764 },
    { carat: 1.02, price: 5287 },
    { carat: 1.02, price: 6169 },
    { carat: 1.03, price: 4401 },
    { carat: 1.04, price: 7220 },
    { carat: 1.04, price: 4181 },
    { carat: 1.03, price: 6783 },
    { carat: 1.02, price: 4958 },
    { carat: 1.03, price: 3922 },
    { carat: 1.02, price: 6047 },
    { carat: 1.04, price: 4240 },
    { carat: 1.03, price: 5804 },
    { carat: 1.02, price: 4449 },
    { carat: 1.02, price: 4162 },
    { carat: 1.03, price: 9881 },
    { carat: 1.04, price: 2037 },
    { carat: 1.02, price: 6652 },
    { carat: 1.09, price: 6225 },
    { carat: 1.11, price: 7639 },
    { carat: 1.06, price: 4255 },
    { carat: 1.06, price: 4452 },
    { carat: 1.11, price: 8843 },
    { carat: 1.13, price: 5526 },
    { carat: 1.07, price: 5093 },
    { carat: 1.09, price: 8454 },
    { carat: 1.08, price: 6769 },
    { carat: 1.05, price: 6181 },
    { carat: 1.09, price: 6934 },
    { carat: 1.11, price: 5962 },
    { carat: 1.07, price: 9532 },
    { carat: 1.13, price: 3669 },
    { carat: 1.1, price: 6535 },
    { carat: 1.05, price: 5101 },
    { carat: 1.07, price: 6730 },
    { carat: 1.12, price: 9634 },
    { carat: 1.09, price: 4372 },
    { carat: 1.12, price: 7716 },
    { carat: 1.16, price: 5678 },
    { carat: 1.23, price: 8855 },
    { carat: 1.15, price: 5257 },
    { carat: 1.2, price: 5699 },
    { carat: 1.2, price: 11021 },
    { carat: 1.21, price: 4472 },
    { carat: 1.24, price: 7701 },
    { carat: 1.2, price: 16256 },
    { carat: 1.24, price: 8584 },
    { carat: 1.24, price: 5221 },
    { carat: 1.21, price: 6092 },
    { carat: 1.22, price: 7738 },
    { carat: 1.2, price: 6416 },
    { carat: 1.21, price: 4791 },
    { carat: 1.2, price: 5226 },
    { carat: 1.21, price: 4310 },
    { carat: 1.21, price: 11572 },
    { carat: 1.22, price: 8975 },
    { carat: 1.2, price: 8545 },
    { carat: 1.2, price: 6250 },
    { carat: 1.51, price: 12831 },
    { carat: 1.5, price: 15240 },
    { carat: 1.5, price: 13528 },
    { carat: 1.25, price: 3990 },
    { carat: 1.25, price: 4900 },
    { carat: 1.4, price: 5723 },
    { carat: 1.26, price: 6277 },
    { carat: 1.26, price: 5306 },
    { carat: 1.5, price: 9533 },
    { carat: 1.5, price: 10428 },
    { carat: 1.5, price: 8770 },
    { carat: 1.5, price: 8490 },
    { carat: 1.5, price: 6770 },
    { carat: 1.5, price: 8161 },
    { carat: 1.33, price: 7982 },
    { carat: 1.32, price: 6079 },
    { carat: 1.5, price: 12247 },
    { carat: 1.33, price: 6482 },
    { carat: 1.51, price: 9116 },
    { carat: 1.25, price: 9933 },
    { carat: 1.7, price: 13823 },
    { carat: 1.62, price: 12429 },
    { carat: 1.52, price: 10664 },
    { carat: 1.52, price: 10968 },
    { carat: 1.55, price: 8056 },
    { carat: 1.54, price: 8652 },
    { carat: 1.53, price: 12791 },
    { carat: 1.54, price: 9285 },
    { carat: 1.67, price: 11400 },
    { carat: 1.56, price: 8048 },
    { carat: 1.56, price: 8324 },
    { carat: 1.52, price: 13799 },
    { carat: 1.7, price: 17892 },
    { carat: 1.7, price: 18279 },
    { carat: 1.7, price: 13737 },
    { carat: 1.7, price: 12190 },
    { carat: 1.7, price: 6397 },
    { carat: 1.52, price: 13768 },
    { carat: 1.55, price: 8678 },
    { carat: 1.53, price: 7240 },
    { carat: 2.14, price: 12400 },
    { carat: 1.71, price: 13097 },
    { carat: 2.0, price: 11036 },
    { carat: 2.32, price: 18026 },
    { carat: 2.0, price: 15312 },
    { carat: 1.73, price: 9271 },
    { carat: 2.3, price: 12316 },
    { carat: 2.06, price: 14982 },
    { carat: 2.01, price: 14998 },
    { carat: 2.05, price: 16357 },
    { carat: 2.02, price: 15996 },
    { carat: 1.71, price: 9193 },
    { carat: 2.02, price: 10412 },
    { carat: 2.15, price: 17221 },
    { carat: 1.77, price: 15278 },
    { carat: 2.28, price: 16369 },
    { carat: 1.75, price: 9890 },
    { carat: 2.06, price: 13317 }
  ];

  // Activation functions
  function sigmoid(x) {
    return 1 / (1 + Math.exp(-x));
  }

  function dsigmoid(y) {
    return y * (1 - y);
  }

  // Neural Network class
  class NeuralNetwork {
    constructor(learningRate) {
      // Architecture: 1 input, 2 hidden layers with 3 neurons each, 1 output
      this.weights = {
        // Input to hidden layer 1
        ih1: Array.from({length: 3}, () => Array.from({length: 1}, () => Math.random() * 2 - 1)),
        // Hidden layer 1 to hidden layer 2
        h1h2: Array.from({length: 3}, () => Array.from({length: 3}, () => Math.random() * 2 - 1)),
        // Hidden layer 2 to output
        h2o: Array.from({length: 1}, () => Array.from({length: 3}, () => Math.random() * 2 - 1))
      };
      
      this.biases = {
        h1: Array.from({length: 3}, () => Math.random() * 2 - 1),
        h2: Array.from({length: 3}, () => Math.random() * 2 - 1),
        o: Array.from({length: 1}, () => Math.random() * 2 - 1)
      };
      
      this.learning_rate = learningRate;
    }
    
    feedforward(input) {
      // Nodes in hidden layer 1
      const hidden1 = Array(3).fill(0);
      for (let i = 0; i < 3; i++) {
        let sum = this.biases.h1[i];
        sum += input * this.weights.ih1[i][0];
        hidden1[i] = sigmoid(sum);
      }
      
      // Nodes in hidden layer 2
      const hidden2 = Array(3).fill(0);
      for (let i = 0; i < 3; i++) {
        let sum = this.biases.h2[i];
        for (let j = 0; j < 3; j++) {
          sum += hidden1[j] * this.weights.h1h2[i][j];
        }
        hidden2[i] = sigmoid(sum);
      }
      
      // Output layer (linear activation)
      let output = this.biases.o[0];
      for (let i = 0; i < 3; i++) {
        output += hidden2[i] * this.weights.h2o[0][i];
      }
      
      return {
        input: input,
        hidden1: hidden1,
        hidden2: hidden2,
        output: output
      };
    }
    
    train(input, target) {
      // Feedforward
      const result = this.feedforward(input);
      const { hidden1, hidden2, output } = result;
      
      // Calculate errors
      // Output error
      const output_error = target - output;
      
      // Hidden layer 2 error
      const hidden2_error = Array(3).fill(0);
      for (let i = 0; i < 3; i++) {
        hidden2_error[i] = output_error * this.weights.h2o[0][i];
      }
      
      // Hidden layer 1 error
      const hidden1_error = Array(3).fill(0);
      for (let i = 0; i < 3; i++) {
        for (let j = 0; j < 3; j++) {
          hidden1_error[i] += hidden2_error[j] * this.weights.h1h2[j][i];
        }
      }
      
      // Backpropagation
      // Update weights and biases for output layer
      for (let i = 0; i < 3; i++) {
        this.weights.h2o[0][i] += this.learning_rate * output_error * hidden2[i];
      }
      this.biases.o[0] += this.learning_rate * output_error;
      
      // Update weights and biases for hidden layer 2
      for (let i = 0; i < 3; i++) {
        for (let j = 0; j < 3; j++) {
          this.weights.h1h2[i][j] += this.learning_rate * hidden2_error[i] * dsigmoid(hidden2[i]) * hidden1[j];
        }
        this.biases.h2[i] += this.learning_rate * hidden2_error[i] * dsigmoid(hidden2[i]);
      }
      
      // Update weights and biases for hidden layer 1
      for (let i = 0; i < 3; i++) {
        this.weights.ih1[i][0] += this.learning_rate * hidden1_error[i] * dsigmoid(hidden1[i]) * input;
        this.biases.h1[i] += this.learning_rate * hidden1_error[i] * dsigmoid(hidden1[i]);
      }
      
      return output_error;
    }
  }

  // Initialize data
  function initializeData() {
    data = sampleData;
    
    // Calculate statistics
    carat_mean = data.reduce((sum, d) => sum + d.carat, 0) / data.length;
    price_mean = data.reduce((sum, d) => sum + d.price, 0) / data.length;
    
    carat_std = Math.sqrt(data.reduce((sum, d) => sum + Math.pow(d.carat - carat_mean, 2), 0) / data.length);
    price_std = Math.sqrt(data.reduce((sum, d) => sum + Math.pow(d.price - price_mean, 2), 0) / data.length);
    
    // Normalize data
    const normalizedData = data.map(d => ({
      carat: (d.carat - carat_mean) / carat_std,
      price: (d.price - price_mean) / price_std,
      carat_raw: d.carat,
      price_raw: d.price
    }));
    
    // Split data
    const trainingPct = parseFloat(document.getElementById('trainingPct').value);
    const shuffled = [...normalizedData].sort(() => Math.random() - 0.5);
    const splitIndex = Math.floor(shuffled.length * trainingPct);
    trainingData = shuffled.slice(0, splitIndex);
    testData = shuffled.slice(splitIndex);
    
    // Show data table
    createDataTable();
  }

  // Create data table
  function createDataTable() {
    const table = document.createElement('table');
    
    // Create header
    const thead = document.createElement('thead');
    const headerRow = document.createElement('tr');
    
    const th1 = document.createElement('th');
    th1.textContent = 'Carat';
    headerRow.appendChild(th1);
    
    const th2 = document.createElement('th');
    th2.textContent = 'Price';
    headerRow.appendChild(th2);
    
    thead.appendChild(headerRow);
    table.appendChild(thead);
    
    // Create body
    const tbody = document.createElement('tbody');
    
    // Show first 20 rows
    const displayData = data.slice(0, 20);
    displayData.forEach(d => {
      const row = document.createElement('tr');
      
      const td1 = document.createElement('td');
      td1.textContent = d.carat.toFixed(2);
      row.appendChild(td1);
      
      const td2 = document.createElement('td');
      td2.textContent = '$' + d.price.toFixed(0);
      row.appendChild(td2);
      
      tbody.appendChild(row);
    });
    
    table.appendChild(tbody);
    
    // Add to page
    const dataTableDiv = document.getElementById('dataTable');
    dataTableDiv.innerHTML = '';
    dataTableDiv.appendChild(table);
  }

  // Train the neural network
  function trainNetwork() {
    const learningRate = parseFloat(document.getElementById('learningRate').value);
    const epochs = parseInt(document.getElementById('epochs').value);
    
    network = new NeuralNetwork(learningRate);
    trainingErrors = [];
    
    const startTime = Date.now();
    
    // Training loop
    for (let epoch = 0; epoch < epochs; epoch++) {
      let sumError = 0;
      
      for (const d of trainingData) {
        const error = network.train(d.carat, d.price);
        sumError += error * error;
      }
      
      const mse = sumError / trainingData.length;
      trainingErrors.push({
        epoch: epoch,
        error: Math.sqrt(mse)
      });
    }
    
    const endTime = Date.now();
    
    // Calculate metrics
    // RMSE
    let sumSqError = 0;
    for (const d of testData) {
      const { output } = network.feedforward(d.carat);
      sumSqError += Math.pow(d.price - output, 2);
    }
    const testRmse = Math.sqrt(sumSqError / testData.length);
    
    // R-squared
    const predictions = testData.map(d => {
      const normalizedCarat = (d.carat_raw - carat_mean) / carat_std;
      const normalizedPrediction = network.feedforward(normalizedCarat).output;
      return normalizedPrediction * price_std + price_mean;
    });
    
    const meanPrice = testData.reduce((sum, d) => sum + d.price_raw, 0) / testData.length;
    const totalSumSquares = testData.reduce((sum, d) => sum + Math.pow(d.price_raw - meanPrice, 2), 0);
    const residualSumSquares = testData.reduce((sum, d, i) => sum + Math.pow(d.price_raw - predictions[i], 2), 0);
    const rSquared = 1 - (residualSumSquares / totalSumSquares);
    
    metrics = {
      rmse: testRmse * price_std,
      rSquared: rSquared,
      trainingTime: endTime - startTime
    };
    
    // Generate prediction line
    const caratMin = Math.min(...data.map(d => d.carat));
    const caratMax = Math.max(...data.map(d => d.carat));
    const step = (caratMax - caratMin) / 50;
    
    predictionLine = [];
    for (let carat = caratMin; carat <= caratMax; carat += step) {
      const normalizedCarat = (carat - carat_mean) / carat_std;
      const normalizedPrediction = network.feedforward(normalizedCarat).output;
      const prediction = normalizedPrediction * price_std + price_mean;
      
      predictionLine.push({
        carat: carat,
        price: prediction
      });
    }
    
    // Update visualizations
    createNeuralNetworkViz();
    createMetricsViz();
    createPredictionsViz();
  }

  // Create neural network visualization
  function createNeuralNetworkViz() {
    const width = 800;
    const height = 400;
    const margin = { top: 40, right: 40, bottom: 40, left: 40 };
    
    // Clear previous visualization
    const container = document.getElementById('neuralNetworkViz');
    container.innerHTML = '';
    
    // Create SVG
    const svg = d3.create('svg')
      .attr('width', width)
      .attr('height', height)
      .attr('viewBox', [0, 0, width, height])
      .attr('style', 'max-width: 100%; height: auto;');
    
    // Layer coordinates
    const layers = [
      { name: 'Input', nodes: 1, x: margin.left + 50 },
      { name: 'Hidden 1', nodes: 3, x: margin.left + 230 },
      { name: 'Hidden 2', nodes: 3, x: margin.left + 410 },
      { name: 'Output', nodes: 1, x: margin.left + 590 }
    ];
    
    // Add layer labels
    svg.selectAll('.layer-label')
      .data(layers)
      .join('text')
      .attr('class', 'layer-label')
      .attr('x', d => d.x)
      .attr('y', margin.top - 15)
      .attr('text-anchor', 'middle')
      .attr('font-weight', 'bold')
      .text(d => d.name);
    
    // Function to get node vertical position
    function getNodeY(layerIndex, nodeIndex, layerNodes) {
      const layerHeight = height - margin.top - margin.bottom;
      const nodeSpacing = layerHeight / (layerNodes + 1);
      return margin.top + nodeSpacing * (nodeIndex + 1);
    }
    
    // Create all nodes
    const nodeData = [];
    const connections = [];
    
    layers.forEach((layer, layerIndex) => {
      for (let i = 0; i < layer.nodes; i++) {
        const nodeY = getNodeY(layerIndex, i, layer.nodes);
        
        nodeData.push({
          id: `${layerIndex}-${i}`,
          x: layer.x,
          y: nodeY,
          layer: layerIndex,
          index: i
        });
        
        // Create connections
        if (layerIndex > 0) {
          const prevLayer = layers[layerIndex - 1];
          for (let j = 0; j < prevLayer.nodes; j++) {
            let weight;
            if (layerIndex === 1) {
              weight = network.weights.ih1[i][j];
            } else if (layerIndex === 2) {
              weight = network.weights.h1h2[i][j];
            } else if (layerIndex === 3) {
              weight = network.weights.h2o[i][j];
            }
            
            connections.push({
              source: `${layerIndex-1}-${j}`,
              target: `${layerIndex}-${i}`,
              weight: weight
            });
          }
        }
      }
    });
    
    // Draw connections
    svg.append('g')
      .attr('class', 'connections')
      .selectAll('line')
      .data(connections)
      .join('line')
      .attr('x1', d => nodeData.find(n => n.id === d.source).x)
      .attr('y1', d => nodeData.find(n => n.id === d.source).y)
      .attr('x2', d => nodeData.find(n => n.id === d.target).x)
      .attr('y2', d => nodeData.find(n => n.id === d.target).y)
      .attr('stroke', d => d.weight > 0 ? 'blue' : 'red')
      .attr('stroke-width', d => Math.abs(d.weight) * 3 + 0.5)
      .attr('opacity', 0.6);
    
    // Draw nodes
    const nodeGroup = svg.append('g').attr('class', 'nodes');
    
    nodeGroup.selectAll('circle')
      .data(nodeData)
      .join('circle')
      .attr('cx', d => d.x)
      .attr('cy', d => d.y)
      .attr('r', 25)
      .attr('fill', d => {
        if (d.layer === 0) return '#8dd3c7';  // Input (teal)
        if (d.layer === 3) return '#fb8072';  // Output (red)
        return '#80b1d3';                     // Hidden (blue)
      })
      .attr('stroke', 'black')
      .attr('stroke-width', 2);
    
    // Add bias and weight labels
    nodeGroup.selectAll('text.node-label')
      .data(nodeData)
      .join('text')
      .attr('class', 'node-label')
      .attr('x', d => d.x)
      .attr('y', d => d.y)
      .attr('text-anchor', 'middle')
      .attr('dominant-baseline', 'middle')
      .attr('font-size', '12px')
      .attr('font-weight', 'bold')
      .text(d => {
        if (d.layer === 0) return 'Carat';
        if (d.layer === 3) return 'Price';
        
        // Get bias for hidden nodes
        let bias;
        if (d.layer === 1) bias = network.biases.h1[d.index];
        if (d.layer === 2) bias = network.biases.h2[d.index];
        
        return `b:${bias.toFixed(2)}`;
      });
    
    // Add weight labels to connections
    svg.append('g')
      .attr('class', 'weight-labels')
      .selectAll('text')
      .data(connections)
      .join('text')
      .attr('x', d => {
        const source = nodeData.find(n => n.id === d.source);
        const target = nodeData.find(n => n.id === d.target);
        return (source.x + target.x) / 2;
      })
      .attr('y', d => {
        const source = nodeData.find(n => n.id === d.source);
        const target = nodeData.find(n => n.id === d.target);
        return (source.y + target.y) / 2 - 8;
      })
      .attr('text-anchor', 'middle')
      .attr('font-size', '10px')
      .text(d => d.weight.toFixed(2));
    
    container.appendChild(svg.node());
  }

  // Create metrics visualization
  function createMetricsViz() {
    // Create metrics table
    const tableContainer = document.getElementById('metricsTable');
    tableContainer.innerHTML = '';
    
    const table = document.createElement('table');
    const tbody = document.createElement('tbody');
    
    // RMSE
    const row1 = document.createElement('tr');
    const cell1_1 = document.createElement('td');
    cell1_1.textContent = 'Test RMSE';
    const cell1_2 = document.createElement('td');
    cell1_2.textContent = `$${metrics.rmse.toFixed(2)}`;
    cell1_2.style.textAlign = 'right';
    row1.appendChild(cell1_1);
    row1.appendChild(cell1_2);
    tbody.appendChild(row1);
    
    // R-squared
    const row2 = document.createElement('tr');
    const cell2_1 = document.createElement('td');
    cell2_1.textContent = 'R² Score';
    const cell2_2 = document.createElement('td');
    cell2_2.textContent = metrics.rSquared.toFixed(4);
    cell2_2.style.textAlign = 'right';
    row2.appendChild(cell2_1);
    row2.appendChild(cell2_2);
    tbody.appendChild(row2);
    
    // Training time
    const row3 = document.createElement('tr');
    const cell3_1 = document.createElement('td');
    cell3_1.textContent = 'Training Time';
    const cell3_2 = document.createElement('td');
    cell3_2.textContent = `${metrics.trainingTime}ms`;
    cell3_2.style.textAlign = 'right';
    row3.appendChild(cell3_1);
    row3.appendChild(cell3_2);
    tbody.appendChild(row3);
    
    table.appendChild(tbody);
    tableContainer.appendChild(table);
    
    // Create training error chart
    const chartContainer = document.getElementById('trainingChart');
    chartContainer.innerHTML = '';
    
    const width = 600;
    const height = 300;
    const margin = { top: 40, right: 30, bottom: 40, left: 60 };
    
    const svg = d3.create('svg')
      .attr('width', width)
      .attr('height', height)
      .attr('viewBox', [0, 0, width, height])
      .attr('style', 'max-width: 100%; height: auto;');
    
    // Add title
    svg.append('text')
      .attr('x', width / 2)
      .attr('y', margin.top / 2)
      .attr('text-anchor', 'middle')
      .attr('font-weight', 'bold')
      .text('Training Error Over Epochs');
    
    // Create scales
    const x = d3.scaleLinear()
      .domain([0, d3.max(trainingErrors, d => d.epoch)])
      .range([margin.left, width - margin.right]);
    
    const y = d3.scaleLinear()
      .domain([0, d3.max(trainingErrors, d => d.error) * 1.1])
      .range([height - margin.bottom, margin.top]);
    
    // Add axes
    svg.append('g')
      .attr('transform', `translate(0,${height - margin.bottom})`)
      .call(d3.axisBottom(x).ticks(5))
      .append('text')
      .attr('x', width - margin.right)
      .attr('y', 30)
      .attr('fill', 'black')
      .text('Epoch');
    
    svg.append('g')
      .attr('transform', `translate(${margin.left},0)`)
      .call(d3.axisLeft(y))
      .append('text')
      .attr('transform', 'rotate(-90)')
      .attr('x', -height / 2)
      .attr('y', -40)
      .attr('fill', 'black')
      .attr('text-anchor', 'middle')
      .text('RMSE');
    
    // Create line
    const line = d3.line()
      .x(d => x(d.epoch))
      .y(d => y(d.error));
    
    // Add the line
    svg.append('path')
      .datum(trainingErrors)
      .attr('fill', 'none')
      .attr('stroke', 'steelblue')
      .attr('stroke-width', 1.5)
      .attr('d', line);
    
    chartContainer.appendChild(svg.node());
  }

  // Create predictions visualization
  function createPredictionsViz() {
    const container = document.getElementById('predictionsChart');
    container.innerHTML = '';
    
    const width = 600;
    const height = 400;
    const margin = { top: 40, right: 30, bottom: 40, left: 60 };
    
    const svg = d3.create('svg')
      .attr('width', width)
      .attr('height', height)
      .attr('viewBox', [0, 0, width, height])
      .attr('style', 'max-width: 100%; height: auto;');
    
    // Create scales
    const x = d3.scaleLinear()
      .domain([d3.min(data, d => d.carat), d3.max(data, d => d.carat)])
      .range([margin.left, width - margin.right]);
    
    const y = d3.scaleLinear()
      .domain([0, d3.max(data, d => d.price) * 1.1])
      .range([height - margin.bottom, margin.top]);
    
    // Add axes
    svg.append('g')
      .attr('transform', `translate(0,${height - margin.bottom})`)
      .call(d3.axisBottom(x))
      .append('text')
      .attr('x', width - margin.right)
      .attr('y', 30)
      .attr('fill', 'black')
      .text('Carat');
    
    svg.append('g')
      .attr('transform', `translate(${margin.left},0)`)
      .call(d3.axisLeft(y))
      .append('text')
      .attr('transform', 'rotate(-90)')
      .attr('x', -height / 2)
      .attr('y', -40)
      .attr('fill', 'black')
      .attr('text-anchor', 'middle')
      .text('Price ($)');
    
    // Add grid
    svg.append('g')
      .attr('stroke', 'lightgray')
      .attr('stroke-opacity', 0.3)
      .call(g => g.append('g')
        .selectAll('line')
        .data(x.ticks(10))
        .join('line')
        .attr('x1', d => x(d))
        .attr('x2', d => x(d))
        .attr('y1', margin.top)
        .attr('y2', height - margin.bottom))
      .call(g => g.append('g')
        .selectAll('line')
        .data(y.ticks(10))
        .join('line')
        .attr('y1', d => y(d))
        .attr('y2', d => y(d))
        .attr('x1', margin.left)
        .attr('x2', width - margin.right));
    
    // Draw test data points
    svg.append('g')
      .selectAll('circle.test')
      .data(testData)
      .join('circle')
      .attr('class', 'test')
      .attr('cx', d => x(d.carat_raw))
      .attr('cy', d => y(d.price_raw))
      .attr('r', 3)
      .attr('fill', 'gray')
      .attr('opacity', 0.5);
    
    // Draw prediction line
    const line = d3.line()
      .x(d => x(d.carat))
      .y(d => y(d.price));
    
    svg.append('path')
      .datum(predictionLine)
      .attr('fill', 'none')
      .attr('stroke', 'red')
      .attr('stroke-width', 2)
      .attr('d', line);
    
    // Add legend
    const legend = svg.append('g')
      .attr('transform', `translate(${width - 120}, ${margin.top + 20})`);
    
    legend.append('circle')
      .attr('cx', 0)
      .attr('cy', 0)
      .attr('r', 4)
      .attr('fill', 'gray')
      .attr('opacity', 0.5);
    
    legend.append('text')
      .attr('x', 10)
      .attr('y', 4)
      .text('Actual');
    
    legend.append('line')
      .attr('x1', -10)
      .attr('x2', 5)
      .attr('y1', 20)
      .attr('y2', 20)
      .attr('stroke', 'red')
      .attr('stroke-width', 2);
    
    legend.append('text')
      .attr('x', 10)
      .attr('y', 24)
      .text('Predicted');
    
    container.appendChild(svg.node());
  }

  // Initialize
  document.addEventListener('DOMContentLoaded', function() {
    // Load data
    initializeData();
    
    // Set up training button
    document.getElementById('trainButton').addEventListener('click', trainNetwork);
    
    // Train network with default parameters
    trainNetwork();
  });
</script>
```

## Statistical Models vs. Machine Learning Models

We assume most engineers, a priori, consider *machine learning* models as somehow 'better' than statistical models. In reality there is a *lot* of overlap between them, and the situation should dictate the model selection. However, since we assume the reader will inevitably be interested in machine learning, we think it's best to include it, at the very least so we can understand the similarities and differences when compared to statistical models.

The discussion below gives differences we believe are *typically* true. However, in topics with as much overlap as statistics and machine learning, virtual no difference is definite.

### Probabilistic vs Deterministic Outputs

A statistical model is built on probability distributions and can typically generate random data that accurately reflects its underlying statistical properties. A machine learning model *may* be built on probability distributions, and further, *may* be able to generate random data that accurately reflects its underlying statistical properties.

### Likelihood vs Loss Function

Although these topics are effectively left for the second half of the primer, we'll mention the following as it is an important difference: 

-   Statistical model parameters are optimized via a likelihood function based in probability theory
-   Machine learning model parameters are optimized using an objective/**loss function**

The confusing thing is that machine learning may have loss functions that use likelihood - but the key is they *do not have to*.

### Volume of Data and Number of Parameters

There are other common differences that are even more blurry - like that machine learning is focused on larger datasets and often have more flexible (i.e. more parameters) methods. This tends to also mean overfitting is a larger concern in machine learning. You would also typically not define a causal model as a foundation for a machine learning model, as they tend to use predefined algorithms.

## Neural Nets

There are many machine learning models we could focus on, however, we'll choose Neural Nets, culminating in Bayesian Neural Nets...

## Single Neuron

It's helpful to focus on a single neuron before buildings up the rest of a neural net. First it will show an artificial neuron is not complicated. And second, discussing a single neuron allows for simple notation. When we need to keep track of many neurons, the notation required is a bit more complicated.

A neuron has a number of connections feeding into it, this is very much like the different inputs into a linear regression equation that we would label as $x_1$, $x_2$ etc. Using familiar **regression notation** it would look like this:

$$
z = \beta_1x_1 + \beta_2x_2 + … + \beta_nx_n + \beta_0
$$

However, the neural net folks prefer something more akin to the following **neural network notation**:

$$
z = w_1a_1 + w_2a_2 + … + w_na_n + b
$$

Where we call $w$ a weight, $a$ an activation (since it's usually from the previous neuron), and $b$ the bias.

### Activation Function

There's one more step in that some **activation function** is applied to $z$, which is quite similar in form to generalized linear regression, although the purpose is only to introduce a convenient form of non-linearity. 

$$
a = f(z)
$$

Where $f(z)$ is some activation function, which could be the sigmoid function we've already seen, among other options. Although not preferred in most large neural networks, the 0 to 1 range of the sigmoid function has a nice interpretation as the physical activation of the neuron.

As a reminder here is what the sigmoid activation looks like:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

## Neural Net Structure

Although there are many variations, a basic neural net consists of layers of artificial neurons that are activated based the outputs of the previous layer of neurons. An example will make this much more obvious.

To explain that further it helps immensely to have a diagram that points out the layers, the neurons, and what connections are being used to feed information into the next neuron.

```{mermaid}
graph LR
  %% Define classes with desired styles
  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;
  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;
  classDef hiddenLayer2 fill:#d8c2f9,stroke:#6a00d6;
  classDef outputLayer fill:#c2f9c9,stroke:#00d640;

  subgraph Input Layer
    N1(["$$N^{(0)}_{1}$$"])
    class N1 inputLayer;
  end

  subgraph Hidden Layer 1
    N2(["$$N^{(1)}_{1}$$"])
    N3(["$$N^{(1)}_{2}$$"])
    N4(["$$N^{(1)}_{3}$$"])
    class N2,N3,N4 hiddenLayer1;
  end

  subgraph Hidden Layer 2
    N5(["$$N^{(2)}_{1}$$"])
    N6(["$$N^{(2)}_{2}$$"])
    N7(["$$N^{(2)}_{3}$$"])
    class N5,N6,N7 hiddenLayer2;
  end

  subgraph Output Layer
    N8(["$$N^{(3)}_{1}$$"])
    class N8 outputLayer;
  end

  N1 -->|"$$a^{(0)}_{1,1}$$"| N2
  N1 -->|"$$a^{(0)}_{1,2}$$"| N3
  N1 -->|"$$a^{(0)}_{1,3}$$"| N4
  
  N2 -->|"$$a^{(1)}_{1,1}$$"| N5
  N2 -->|"$$a^{(1)}_{1,2}$$"| N6
  N2 -->|"$$a^{(1)}_{1,3}$$"| N7
  
  N3 -->|"$$a^{(1)}_{2,1}$$"| N5
  N3 -->|"$$a^{(1)}_{2,2}$$"| N6
  N3 -->|"$$a^{(1)}_{2,3}$$"| N7
  
  N4 -->|"$$a^{(1)}_{3,1}$$"| N5
  N4 -->|"$$a^{(1)}_{3,2}$$"| N6
  N4 -->|"$$a^{(1)}_{3,3}$$"| N7
  
  N5 -->|"$$a^{(2)}_{1,1}$$"| N8
  N6 -->|"$$a^{(2)}_{2,1}$$"| N8
  N7 -->|"$$a^{(2)}_{3,1}$$"| N8
```

The number of input neurons match the number of variables (or dimensions) of the data we want to input. We are going to input one variable (the size/carats of a diamond) so we only need one input neuron.

We want the input and output functions to match the scale of our data (which can be greater than one), while we want the hidden layers to mimic the activation of a neuron (0% to 100%, or 0 to 1). Thus the inputs and outputs of our neural network will be based on linear regression, while the neurons in the hidden layers will be based on logistic regression.

### Notation

The building blocks of a neural net are nothing scary, however there are a lot of pieces, and therefore a lot to keep track of. This demands some notation to keep everything straight, which is probably the scariest bit. (We should mention training a neural net is another tough part, but we will not worry about that here).

Since the working of modern large neural nets is intricately tied to matrix multiplication, the standard descriptions involve matrix notation. We are not interested in large neural nets so we will alter the standard notation to remove the matrix notation for the rest of us mere mortals.

We have the following natural heirarchy of information:

-   Layers
    -   Neurons
        -   Weights
        -   Bias
        -   Activations

#### Neurons

We can use the following notation to refer to a specific neuron $N$:

$N^{(l)}_{i}$

-   $l$ (superscript), the layer index/number
-   $i$ (subscript),the neuron index/number within layer $l$

#### Weights

We can use the following notation to refer to a specific weight $w$:

$w^{(l)}_{i,j}$

-   $l$ (superscript), the layer index/number
-   $i$ (subscript), the neuron index/number within layer $l$
-   $j$ (subscript), the weight index/number within neuron $i$

#### Biases

We can use the following notation to refer to a specific bias $b$:

$b^{(l)}_{i,j}$

-   $l$ (superscript), the layer index/number
-   $i$ (subscript), the neuron index/number within layer $l$
-   $j$ (subscript), the bias index/number within neuron $i$

#### Activations

We can use the following notation to refer to a specific activation $a$:

$a^{(l)}_{i,j}$

-   $l$ (superscript), the layer index/number
-   $i$ (subscript), the neuron index/number within layer $l$
-   $j$ (subscript), the activation index/number within neuron $i$


### The Input Layer

The first neuron, $N^{(0)}_{1}$, takes one input and creates three unique outputs based on the following calculation, where $w$ is a weight/coefficient, $x$ is the input value (i.e. carats), and $b$ is a bias/intercept:

$$
\begin{aligned}
a^{(0)}_{1,1} &= w^{(0)}_{1,1} \cdot x \;+\; b^{(0)}_{1,1}, \\
a^{(0)}_{1,2} &= w^{(0)}_{1,2} \cdot x \;+\; b^{(0)}_{1,2}, \\
a^{(0)}_{1,3} &= w^{(0)}_{1,3} \cdot x \;+\; b^{(0)}_{1,3}.
\end{aligned}
$$

Shown on a subset of the graph:

```{mermaid}
graph LR
  %% Define classes with desired styles
  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;
  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;

  subgraph Input Layer
    N1(["$$N^{(0)}_{1}$$"])
    class N1 inputLayer;
  end

  subgraph Hidden Layer 1
    N2(["$$N^{(1)}_{1}$$"])
    N3(["$$N^{(1)}_{2}$$"])
    N4(["$$N^{(1)}_{3}$$"])
    class N2,N3,N4 hiddenLayer1;
  end

  N1 -->|"$$a^{(0)}_{1,1} = w^{(0)}_{1,1} \cdot x \;+\; b^{(0)}_{1,1}$$"| N2
  N1 -->|"$$a^{(0)}_{1,2} = w^{(0)}_{1,2} \cdot x \;+\; b^{(0)}_{1,2}$$"| N3
  N1 -->|"$$a^{(0)}_{1,3} = w^{(0)}_{1,3} \cdot x \;+\; b^{(0)}_{1,3}$$"| N4
```

#### Hidden Layer 1

At Hidden Layer 1, each neuron is again receiving one input (an activation this time), and again creating three unique outputs. Since we are now within the hidden layers, the math is logistic regression instead of linear regression. We examine the calculation occuring at neuron $N^{(1)}_{1}$. Here $a^{(0)}_{1,1}$ is the activation value received from neuron $N^{(0)}_{1}$.

$$
\begin{aligned}
a^{(1)}_{1,1} &= \frac{1}{1 + e^{-(w^{(1)}_{1,1} \cdot a^{(0)}_{1,1} + b^{(1)}_{1,1})}}, \\
a^{(1)}_{1,2} &= \frac{1}{1 + e^{-(w^{(1)}_{1,2} \cdot a^{(0)}_{1,1} + b^{(1)}_{1,2})}}, \\
a^{(1)}_{1,3} &= \frac{1}{1 + e^{-(w^{(1)}_{1,3} \cdot a^{(0)}_{1,1} + b^{(1)}_{1,3})}}.
\end{aligned}
$$

Shown on a subset of the graph:

```{mermaid}
graph LR
  %% Define classes with desired styles
  classDef hiddenLayer fill:#c2ebf9,stroke:#0096d6;   %% Blue
  classDef outputLayer fill:#d8c2f9,stroke:#6a00d6;   %% Purple

  subgraph Hidden Layer 1
    N2(["$$N^{(1)}_{1}$$"])
    class N2 hiddenLayer;
  end

  subgraph Hidden Layer 2
    O1(["$$N^{(2)}_{1}$$"])
    O2(["$$N^{(2)}_{2}$$"])
    O3(["$$N^{(2)}_{3}$$"])
    class O1,O2,O3 outputLayer;
  end

  N2 -->|"$$a^{(1)}_{1,1} = \frac{1}{1 + e^{-(w^{(1)}_{1,1} \cdot a^{(0)}_{1,1}  + b^{(1)}_{1,1})}}$$"| O1
  N2 -->|"$$a^{(1)}_{1,2} =\frac{1}{1 + e^{-(w^{(1)}_{1,2} \cdot a^{(0)}_{1,1}  + b^{(1)}_{1,2}}}$$"| O2
  N2 -->|"$$a^{(1)}_{1,3} =\frac{1}{1 + e^{-(w^{(1)}_{1,3} \cdot a^{(0)}_{1,1}  + b^{(1)}_{1,3})}}$$"| O3
```

#### Hidden Layer 2

At Hidden Layer 2 each neuron takes the values from the first hidden layer of neurons and performs another logistic regression to determine its activation. If we examine neuron $N^{(2)}_{1}$, it is taking in data from activation $a^{(1)}_{1,1}$, $a^{(1)}_{2,1}$, and $a^{(1)}_{3,1}$, and then creating a single output, $a^{(2)}_{1,1}$.

$$
a^{(2)}_{1,1} = \frac{1}{1 + e^{-(w^{(2)}_{1,1} \cdot a^{(1)}_{1,1} + w^{(2)}_{1,2} \cdot a^{(1)}_{2,1} + w^{(2)}_{1,3} \cdot a^{(1)}_{3,1} + b^{(2)}_{1,1})}}
$$

Shown on a subset of the graph:

```{mermaid}
graph LR
  %% Define classes with desired styles
  classDef hiddenLayer fill:#d8c2f9,stroke:#6a00d6;
  classDef outputLayer fill:#c2f9c9,stroke:#00d640;

  subgraph Hidden Layer 2 Neuron
    N5(["$$N^{(2)}_{1}$$"])
    class N5 hiddenLayer;
  end

  subgraph Output
    O1(["$$N^{(3)}_{1}$$"])
    class O1 outputLayer;
  end

  N5 -->|"$$\frac{1}{1 + e^{-(w^{(2)}_{1,1} \cdot a^{(1)}_{1,1} + w^{(2)}_{1,2} \cdot a^{(1)}_{2,1} + w^{(2)}_{1,3} \cdot a^{(1)}_{3,1} + b^{(2)}_{1,1})}}$$"| O1
```

#### The Output Layer

The output neuron $N^{(3)}_{1,1}$ receives the activations $a^{(2)}_{1,1}$, $a^{(2)}_{2,1}$, and $a^{(2)}_{3,1}$ and uses linear regression to obtain the output value of the neural net.

$$
y = w^{(3)}_{1,1} \cdot a^{(2)}_{1,1} + w^{(3)}_{1,2} \cdot a^{(2)}_{2,1} + w^{(3)}_{1,3} \cdot a^{(2)}_{3,1} + b^{(3)}_{1,1}
$$

#### Values of Weights and Biases

While the structure of a neural net influences the types of problems it is most appropriate for, the knowledge of the neural net is held in the weights and biases. Determining the weights of a neural network is generally accomplished by training on data via backpropogation, for which numerous resources are available elsewhere. Here, we will jump past the details of that step so that we can proceed with a reasonably trained neural net.

## Draft Apps

### Shiny Mermaid

```{shinylive-python}
#| standalone: true
#| viewerHeight: 800

from shiny import App, ui, render

app_ui = ui.page_fluid(
    # Include Mermaid JS in the head
    ui.tags.head(
        ui.tags.script(src="https://cdn.jsdelivr.net/npm/mermaid@11.4.1/dist/mermaid.min.js")
    ),
    ui.h1("Neural Network Diagram with Neutral Theme and Zoomed Out View"),
    # UI output for the Mermaid diagram
    ui.output_ui("mermaid_diagram"),
    # Initialize Mermaid with the 'neutral' theme and re-run rendering after dynamic content loads
    ui.tags.script("""
        mermaid.initialize({ startOnLoad: true, theme: 'neutral' });
        $(document).on('shiny:value', function(event) {
            if(event.name === 'mermaid_diagram') {
                setTimeout(function() {
                    mermaid.init();
                }, 100);
            }
        });
    """)
)

def server(input, output, session):
    @output
    @render.ui
    def mermaid_diagram():
        neural_network_mermaid = '''
graph LR
  %% Define classes with desired styles
  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;
  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;
  classDef hiddenLayer2 fill:#d8c2f9,stroke:#6a00d6;
  classDef outputLayer fill:#c2f9c9,stroke:#00d640;

  subgraph Input Layer
    N1(["$$N^{(0)}_{1}$$"])
    class N1 inputLayer;
  end

  subgraph Hidden Layer 1
    N2(["$$N^{(1)}_{1}$$"])
    N3(["$$N^{(1)}_{2}$$"])
    N4(["$$N^{(1)}_{3}$$"])
    class N2,N3,N4 hiddenLayer1;
  end

  subgraph Hidden Layer 2
    N5(["$$N^{(2)}_{1}$$"])
    N6(["$$N^{(2)}_{2}$$"])
    N7(["$$N^{(2)}_{3}$$"])
    class N5,N6,N7 hiddenLayer2;
  end

  subgraph Output Layer
    N8(["$$N^{(3)}_{1}$$"])
    class N8 outputLayer;
  end

  N1 -->|"$$a^{(0)}_{1,1}$$"| N2
  N1 -->|"$$a^{(0)}_{1,2}$$"| N3
  N1 -->|"$$a^{(0)}_{1,3}$$"| N4
  
  N2 -->|"$$a^{(1)}_{1,1}$$"| N5
  N2 -->|"$$a^{(1)}_{1,2}$$"| N6
  N2 -->|"$$a^{(1)}_{1,3}$$"| N7
  
  N3 -->|"$$a^{(1)}_{2,1}$$"| N5
  N3 -->|"$$a^{(1)}_{2,2}$$"| N6
  N3 -->|"$$a^{(1)}_{2,3}$$"| N7
  
  N4 -->|"$$a^{(1)}_{3,1}$$"| N5
  N4 -->|"$$a^{(1)}_{3,2}$$"| N6
  N4 -->|"$$a^{(1)}_{3,3}$$"| N7
  
  N5 -->|"$$a^{(2)}_{1,1}$$"| N8
  N6 -->|"$$a^{(2)}_{2,1}$$"| N8
  N7 -->|"$$a^{(2)}_{3,1}$$"| N8
        '''
        # Wrap the Mermaid diagram in a div with a CSS scale transform
        return ui.div(
            ui.tags.div(
                ui.tags.pre(
                    ui.tags.code(neural_network_mermaid, {"class": "mermaid"})
                ),
                style="transform: scale(1.0); transform-origin: top left;"
            )
        )

app = App(app_ui, server)


```

### Mermaid Rendered in Shiny with Controls

```{shinylive-python}
#| standalone: true
#| viewerHeight: 800

from shiny import App, ui, render, reactive

app_ui = ui.page_fluid(
    ui.tags.head(
        ui.tags.script(src="https://cdn.jsdelivr.net/npm/mermaid@11.4.1/dist/mermaid.min.js")
    ),
    ui.layout_sidebar(
        ui.sidebar(
            ui.h4("Node Values"),
            # Input Layer
            ui.input_numeric("N1", "N₁⁽⁰⁾", value=0.0, step=0.1),
            
            # Hidden Layer 1
            ui.input_numeric("N2", "N₁⁽¹⁾", value=0.0, step=0.1),
            ui.input_numeric("N3", "N₂⁽¹⁾", value=0.0, step=0.1),
            ui.input_numeric("N4", "N₃⁽¹⁾", value=0.0, step=0.1),
            
            # Hidden Layer 2
            ui.input_numeric("N5", "N₁⁽²⁾", value=0.0, step=0.1),
            ui.input_numeric("N6", "N₂⁽²⁾", value=0.0, step=0.1),
            ui.input_numeric("N7", "N₃⁽²⁾", value=0.0, step=0.1),
            
            # Output Layer
            ui.input_numeric("N8", "N₁⁽³⁾", value=0.0, step=0.1),
            width=200
        ),
        ui.h1("Neural Network Diagram with Node Values"),
        ui.output_ui("mermaid_diagram"),
    ),
    ui.tags.script("""
        mermaid.initialize({ startOnLoad: true, theme: 'neutral' });
        $(document).on('shiny:value', function(event) {
            if(event.name === 'mermaid_diagram') {
                setTimeout(function() {
                    mermaid.init();
                }, 1);
            }
        });
    """)
)

def server(input, output, session):
    @output
    @render.ui
    def mermaid_diagram():
        neural_network_mermaid = f'''
graph LR
  classDef inputLayer fill:#f9ebc2,stroke:#d6a000;
  classDef hiddenLayer1 fill:#c2ebf9,stroke:#0096d6;
  classDef hiddenLayer2 fill:#d8c2f9,stroke:#6a00d6;
  classDef outputLayer fill:#c2f9c9,stroke:#00d640;

  subgraph Input Layer
    N1(["N₁⁽⁰⁾<br>{input.N1():.2f}"])
    class N1 inputLayer;
  end

  subgraph Hidden Layer 1
    N2(["N₁⁽¹⁾<br>{input.N2():.2f}"])
    N3(["N₂⁽¹⁾<br>{input.N3():.2f}"])
    N4(["N₃⁽¹⁾<br>{input.N4():.2f}"])
    class N2,N3,N4 hiddenLayer1;
  end

  subgraph Hidden Layer 2
    N5(["N₁⁽²⁾<br>{input.N5():.2f}"])
    N6(["N₂⁽²⁾<br>{input.N6():.2f}"])
    N7(["N₃⁽²⁾<br>{input.N7():.2f}"])
    class N5,N6,N7 hiddenLayer2;
  end

  subgraph Output Layer
    N8(["N₁⁽³⁾<br>{input.N8():.2f}"])
    class N8 outputLayer;
  end

  N1 --> N2
  N1 --> N3
  N1 --> N4
  
  N2 --> N5
  N2 --> N6
  N2 --> N7
  
  N3 --> N5
  N3 --> N6
  N3 --> N7
  
  N4 --> N5
  N4 --> N6
  N4 --> N7
  
  N5 --> N8
  N6 --> N8
  N7 --> N8
        '''
        return ui.div(
            ui.tags.div(
                ui.tags.pre(
                    ui.tags.code(neural_network_mermaid, {"class": "mermaid"})
                ),
                style="transform: scale(1.0); transform-origin: top left;"
            )
        )

app = App(app_ui, server)
```

### Bayesian Attempt

```{shinylive-python}
#| standalone: true
#| viewerHeight: 900

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from io import StringIO

# --- Diamond Dataset ---
data_str = """ID,carat,cut,color,clarity,depth,table,price,x,y,z
51657,0.3,Ideal,G,VS2,62.3,58.0,545,4.26,4.28,2.66
34838,0.3,Premium,G,VVS2,60.8,58.0,878,4.38,4.34,2.65
9718,0.3,Ideal,H,VVS2,62.1,54.0,590,4.32,4.35,2.69
46635,0.3,Very Good,E,SI1,62.7,60.0,526,4.24,4.28,2.67
31852,0.3,Premium,G,VS1,62.2,59.0,776,4.28,4.24,2.65
40942,0.27,Ideal,H,VS1,62.3,54.0,500,4.16,4.19,2.6
49960,0.3,Good,H,SI1,63.7,56.0,540,4.22,4.2,2.68
30300,0.3,Very Good,D,SI2,61.0,61.0,447,4.25,4.31,2.61
15051,0.3,Ideal,F,VS2,61.4,57.0,605,4.34,4.36,2.67
32272,0.3,Very Good,G,VVS1,62.9,57.0,789,4.26,4.3,2.69
16695,0.3,Very Good,H,SI1,62.6,58.0,421,4.22,4.28,2.66
32358,0.3,Good,G,VVS1,63.1,56.0,789,4.25,4.28,2.69
3393,0.27,Very Good,E,VVS2,59.4,64.0,567,4.16,4.19,2.48
16027,0.3,Premium,I,VS1,60.5,60.0,608,4.33,4.3,2.61
5721,0.25,Very Good,E,VVS2,60.9,59.0,575,4.03,4.11,2.48
34695,0.3,Ideal,F,IF,61.7,56.0,873,4.31,4.35,2.67
28794,0.27,Very Good,F,VVS2,61.3,57.0,682,4.14,4.18,2.54
32496,0.3,Good,F,IF,58.8,61.0,796,4.35,4.39,2.57
16359,0.3,Good,D,VS2,64.1,57.0,608,4.25,4.21,2.71
31973,0.3,Very Good,I,VS2,60.5,55.0,453,4.34,4.37,2.63
51312,0.31,Ideal,G,VS2,59.1,57.0,544,4.45,4.48,2.64
27844,0.31,Very Good,G,VS2,63.2,58.0,651,4.3,4.28,2.71
37309,0.31,Ideal,F,IF,62.2,56.0,979,4.31,4.34,2.69
16685,0.31,Ideal,H,SI2,61.1,56.0,421,4.4,4.42,2.69
35803,0.31,Premium,F,IF,61.9,58.0,914,4.36,4.39,2.71
30256,0.31,Very Good,E,VVS1,60.4,61.0,725,4.34,4.4,2.64
36008,0.31,Ideal,F,IF,61.2,56.0,921,4.37,4.42,2.69
30803,0.31,Good,F,VVS1,63.6,61.0,742,4.21,4.25,2.69
32676,0.31,Premium,G,VS1,62.4,59.0,802,4.34,4.32,2.7
35593,0.31,Ideal,H,VVS1,62.2,54.0,907,4.39,4.36,2.72
20386,0.31,Premium,G,VS1,59.5,59.0,625,4.4,4.47,2.64
34570,0.31,Ideal,G,IF,61.0,55.0,871,4.39,4.42,2.69
33609,0.31,Ideal,D,SI2,62.0,56.0,462,4.33,4.35,2.69
32609,0.31,Premium,H,VVS2,61.4,59.0,802,4.38,4.35,2.68
32723,0.31,Ideal,F,VS2,62.7,57.0,802,4.34,4.3,2.71
44998,0.31,Premium,I,SI1,62.3,59.0,523,4.32,4.29,2.68
38803,0.31,Very Good,G,VVS1,63.1,56.0,1046,4.35,4.33,2.74
43285,0.31,Very Good,D,SI1,60.4,60.0,507,4.4,4.44,2.67
33131,0.31,Very Good,E,VVS2,60.8,55.0,816,4.38,4.43,2.68
35157,0.31,Very Good,G,IF,61.6,54.0,891,4.4,4.43,2.72
37580,0.32,Premium,D,VVS2,61.5,60.0,990,4.41,4.37,2.7
33506,0.32,Premium,G,VS1,62.5,60.0,828,4.35,4.29,2.7
26341,0.32,Ideal,H,VVS2,61.7,56.0,645,4.37,4.42,2.71
33033,0.32,Ideal,G,VVS1,61.4,57.0,814,4.39,4.41,2.7
36290,0.32,Ideal,G,SI1,61.3,57.0,477,4.37,4.4,2.69
36284,0.32,Ideal,D,SI2,62.4,54.0,477,4.38,4.4,2.74
13404,0.32,Very Good,F,VS2,61.2,58.0,602,4.38,4.41,2.69
30954,0.32,Ideal,I,VS2,62.5,55.0,449,4.38,4.39,2.74
29634,0.32,Ideal,J,VS1,62.0,54.7,442,4.39,4.42,2.73
30129,0.32,Ideal,G,VS2,61.8,57.0,720,4.4,4.37,2.71
46963,0.32,Good,F,SI1,61.6,60.1,528,4.38,4.4,2.71
32783,0.32,Ideal,D,VVS2,61.2,56.0,803,4.39,4.43,2.7
20012,0.32,Good,G,SI2,63.4,55.0,421,4.32,4.35,2.75
34133,0.32,Ideal,F,VVS1,60.4,57.0,854,4.41,4.43,2.67
27865,0.32,Ideal,G,SI1,61.4,56.0,653,4.44,4.42,2.72
29989,0.32,Ideal,F,VS1,61.0,54.0,716,4.42,4.44,2.7
30145,0.32,Premium,G,VS2,62.8,58.0,720,4.35,4.31,2.72
31320,0.32,Ideal,D,VS2,62.6,55.0,758,4.37,4.39,2.74
35896,0.32,Ideal,G,IF,61.7,54.0,918,4.42,4.46,2.74
50304,0.32,Very Good,G,VS2,62.3,55.0,544,4.38,4.41,2.73
32501,0.33,Premium,G,VS1,61.6,57.0,797,4.51,4.42,2.75
29919,0.33,Ideal,H,VVS1,61.8,55.0,713,4.42,4.44,2.74
37434,0.33,Good,G,IF,57.9,60.0,984,4.55,4.57,2.64
42419,0.33,Ideal,E,VVS1,61.9,57.0,1312,4.43,4.46,2.75
30338,0.34,Premium,F,SI1,59.4,62.0,727,4.59,4.54,2.71
23380,0.33,Very Good,G,SI1,63.2,57.0,631,4.44,4.39,2.79
18704,0.35,Very Good,I,VVS2,61.3,56.0,620,4.52,4.54,2.78
31350,0.34,Ideal,E,VS2,61.8,54.0,760,4.49,4.5,2.78
34543,0.35,Ideal,H,IF,61.5,57.0,868,4.55,4.58,2.8
13389,0.35,Premium,D,SI1,61.5,58.0,601,4.53,4.55,2.79
36970,0.34,Ideal,D,VS1,60.7,57.0,961,4.55,4.51,2.75
37025,0.33,Ideal,G,VVS2,62.5,54.0,965,4.45,4.41,2.77
30831,0.33,Premium,I,VVS2,61.5,58.0,743,4.45,4.43,2.73
36287,0.34,Very Good,E,SI2,61.7,61.0,477,4.47,4.51,2.77
34161,0.33,Premium,G,VS1,60.5,58.0,854,4.49,4.43,2.7
30719,0.35,Fair,E,VVS2,66.2,61.0,738,4.4,4.36,2.9
33204,0.35,Ideal,G,VVS2,61.8,55.0,820,4.53,4.56,2.81
26014,0.35,Premium,D,SI1,60.9,58.0,644,4.52,4.55,2.76
27052,0.33,Ideal,I,VVS1,62.2,54.0,646,4.43,4.45,2.76
34181,0.33,Ideal,G,VS1,62.1,56.0,854,4.42,4.4,2.74
28218,0.4,Premium,D,SI2,62.1,60.0,666,4.69,4.75,2.93
39564,0.4,Premium,G,VS1,62.2,55.0,1080,4.83,4.69,2.96
33662,0.36,Ideal,E,VS1,61.4,54.0,835,4.59,4.63,2.83
36552,0.4,Ideal,E,SI1,60.5,57.0,945,4.81,4.77,2.9
37369,0.4,Very Good,F,VS1,60.4,61.0,982,4.74,4.77,2.87
41873,0.38,Ideal,D,VVS2,61.5,56.0,1257,4.66,4.64,2.86
35767,0.4,Premium,E,VS2,60.7,60.0,912,4.7,4.75,2.87
27792,0.37,Premium,G,VS2,61.3,60.0,649,4.6,4.63,2.83
41652,0.4,Ideal,E,VVS2,62.1,56.0,1238,4.73,4.7,2.93
37757,0.38,Premium,D,VS2,61.6,59.0,998,4.66,4.62,2.86
36266,0.37,Ideal,H,IF,61.7,53.0,936,4.66,4.68,2.88
37328,0.4,Premium,G,VVS2,61.3,59.0,980,4.78,4.74,2.92
38250,0.36,Ideal,D,VS1,62.8,55.0,1018,4.55,4.52,2.85
35397,0.38,Good,F,VS2,62.4,54.3,899,4.6,4.65,2.89
31021,0.37,Premium,I,VS1,61.4,59.0,749,4.61,4.55,2.81
30667,0.4,Very Good,I,VS1,63.0,56.0,737,4.68,4.72,2.96
39618,0.37,Very Good,H,SI1,62.6,63.0,491,4.6,4.5,2.85
30669,0.4,Premium,F,SI1,62.5,59.0,737,4.67,4.71,2.93
17728,0.39,Ideal,E,SI2,61.0,55.0,614,4.74,4.77,2.9
35328,0.38,Ideal,H,VVS2,62.1,54.0,898,4.62,4.66,2.88
33367,0.41,Ideal,G,VS2,61.4,55.0,827,4.75,4.8,2.93
39486,0.41,Ideal,E,VS1,62.1,55.0,1079,4.75,4.78,2.96
31789,0.42,Ideal,E,SI1,61.3,57.0,773,4.79,4.81,2.94
33930,0.41,Good,G,VVS1,63.6,56.0,844,4.72,4.74,3.01
41724,0.41,Ideal,H,IF,61.8,55.0,1243,4.79,4.76,2.95
42168,0.41,Premium,D,VS1,59.3,58.0,1286,4.87,4.85,2.88
30052,0.41,Premium,G,SI1,59.1,58.0,719,4.83,4.88,2.87
41467,0.41,Premium,G,VVS1,61.0,61.0,1230,4.75,4.72,2.89
35509,0.41,Premium,E,SI1,62.8,58.0,904,4.77,4.72,2.98
24390,0.41,Very Good,E,SI2,63.0,57.0,638,4.7,4.73,2.97
35351,0.42,Ideal,H,SI1,62.4,57.0,898,4.79,4.76,2.98
37077,0.41,Premium,F,SI1,62.6,55.0,969,4.78,4.74,2.98
36978,0.42,Premium,G,VVS2,61.6,60.0,963,4.8,4.85,2.97
28454,0.41,Ideal,G,SI1,62.2,56.0,671,4.75,4.77,2.96
43252,0.42,Premium,G,IF,60.2,59.0,1400,4.8,4.87,2.91
41015,0.41,Very Good,F,VVS1,62.7,59.0,1186,4.75,4.78,2.99
37665,0.42,Premium,E,SI1,61.6,59.0,992,4.85,4.83,2.98
40213,0.41,Ideal,D,SI1,61.8,56.0,1122,4.78,4.73,2.94
37909,0.41,Ideal,F,VS1,60.8,56.0,1007,4.76,4.79,2.92
39436,0.41,Ideal,D,VS2,62.2,54.0,1076,4.81,4.77,2.98
46182,0.5,Ideal,I,VVS1,61.6,56.0,1747,5.1,5.13,3.15
38815,0.45,Premium,F,SI1,61.1,58.0,1046,4.97,4.95,3.03
41423,0.46,Ideal,H,VVS1,62.3,54.0,1227,4.96,4.99,3.1
50341,0.5,Ideal,D,VS2,61.1,57.0,2243,5.11,5.13,3.13
43455,0.5,Premium,G,VS2,61.5,57.0,1415,5.12,5.09,3.14
35239,0.43,Very Good,E,SI1,63.4,56.0,894,4.82,4.8,3.05
41838,0.44,Ideal,F,VVS2,60.9,55.0,1253,4.96,4.92,3.01
37303,0.5,Premium,G,SI2,60.7,57.0,978,5.15,5.07,3.1
37391,0.5,Ideal,I,SI1,62.0,55.0,982,5.08,5.11,3.16
38196,0.5,Very Good,D,SI2,63.1,56.0,1015,5.05,4.96,3.16
33009,0.43,Premium,F,SI2,58.3,62.0,813,4.97,4.91,2.88
43403,0.46,Ideal,G,VVS1,62.0,54.0,1412,4.97,5.0,3.09
44797,0.5,Very Good,E,VS2,61.5,56.0,1624,5.07,5.11,3.13
32446,0.43,Very Good,H,VS2,61.9,55.0,792,4.8,4.95,3.02
39507,0.5,Ideal,F,SI2,61.7,55.0,1080,5.13,5.15,3.17
42348,0.46,Ideal,H,SI1,61.2,56.0,1299,4.97,5.0,3.05
49157,0.5,Very Good,G,VVS1,63.3,56.0,2070,5.1,5.07,3.22
39697,0.48,Good,G,VS2,65.4,59.0,1088,4.79,4.88,3.16
47045,0.5,Premium,D,VS2,59.7,57.0,1819,5.13,5.08,3.05
38353,0.47,Very Good,F,SI1,61.1,61.0,1021,4.97,5.01,3.05
49694,0.51,Very Good,E,VVS2,62.8,57.0,2146,5.06,5.1,3.19
39316,0.53,Very Good,G,SI2,60.8,58.0,1070,5.19,5.21,3.16
44608,0.53,Premium,E,SI1,61.9,56.0,1607,5.22,5.19,3.22
47613,0.53,Ideal,G,VVS2,60.4,55.0,1881,5.26,5.3,3.19
44575,0.53,Ideal,E,VS2,62.5,57.0,1607,5.16,5.18,3.23
49934,0.51,Premium,E,VVS2,62.1,57.0,2185,5.18,5.15,3.21
41199,0.51,Very Good,D,SI2,60.3,57.0,1204,5.15,5.17,3.11
47601,0.52,Ideal,G,VVS2,60.8,57.0,1878,5.2,5.17,3.15
48545,0.52,Ideal,I,IF,60.2,56.0,1988,5.23,5.27,3.16
41422,0.52,Very Good,F,SI1,62.3,55.0,1227,5.14,5.17,3.21
48904,0.51,Very Good,F,VVS2,62.0,56.0,2041,5.1,5.15,3.17
43201,0.53,Good,G,VS2,63.4,58.0,1395,5.13,5.16,3.26
46534,0.51,Ideal,G,VS1,62.5,57.0,1781,5.14,5.07,3.19
43116,0.52,Very Good,H,VS2,63.5,58.0,1385,5.12,5.11,3.25
36885,0.51,Good,I,SI1,63.1,56.0,959,5.06,5.14,3.22
44284,0.51,Ideal,G,VS1,62.5,57.0,1577,5.08,5.1,3.18
37127,0.52,Ideal,D,I1,61.1,57.0,971,5.18,5.2,3.17
48116,0.52,Ideal,G,VVS1,61.9,54.4,1936,5.15,5.18,3.2
44258,0.51,Ideal,H,VVS2,61.0,57.0,1574,5.22,5.18,3.17
46475,0.51,Ideal,H,VVS1,61.4,55.0,1776,5.13,5.16,3.16
46460,0.54,Ideal,F,VS1,61.1,57.0,1774,5.28,5.3,3.23
50067,0.54,Ideal,F,VS1,61.5,55.0,2202,5.26,5.27,3.24
43563,0.58,Fair,G,VS2,65.0,56.0,1430,5.23,5.17,3.38
47010,0.56,Ideal,E,VS2,60.9,56.0,1819,5.32,5.35,3.25
41886,0.54,Ideal,I,VS2,61.1,55.0,1259,5.27,5.31,3.23
42007,0.59,Ideal,F,SI2,61.8,55.0,1265,5.41,5.44,3.35
48843,0.55,Ideal,E,VS2,62.5,56.0,2030,5.26,5.23,3.28
52201,0.54,Ideal,E,VVS2,61.9,54.5,2479,5.22,5.25,3.23
49498,0.56,Ideal,H,VVS2,61.8,56.0,2118,5.28,5.33,3.28
52348,0.55,Ideal,E,VVS2,61.4,56.0,2499,5.28,5.31,3.25
50508,0.54,Ideal,G,IF,62.3,56.0,2271,5.19,5.21,3.24
46004,0.54,Ideal,D,VS2,61.2,56.0,1725,5.24,5.28,3.22
46440,0.54,Ideal,F,VS1,60.9,57.0,1772,5.21,5.26,3.19
45822,0.56,Good,F,VS1,63.2,61.0,1712,5.2,5.28,3.3
46373,0.58,Ideal,G,VS2,61.9,55.0,1761,5.33,5.36,3.31
41799,0.6,Very Good,E,SI2,63.2,60.0,1250,5.32,5.28,3.35
45126,0.59,Very Good,E,SI1,62.9,58.0,1652,5.31,5.34,3.35
43185,0.54,Very Good,G,SI1,63.2,58.0,1392,5.15,5.16,3.26
45719,0.56,Ideal,E,SI1,62.7,57.0,1698,5.27,5.23,3.29
42200,0.56,Premium,G,SI1,61.1,61.0,1287,5.31,5.29,3.24
3262,0.7,Ideal,F,VS1,60.3,57.0,3359,5.74,5.79,3.47
51331,0.7,Very Good,F,VS2,62.3,56.0,2362,5.66,5.71,3.54
50892,0.7,Premium,G,VS2,60.8,58.0,2317,5.75,5.8,3.51
46073,0.63,Premium,F,SI1,59.1,57.0,1736,5.64,5.6,3.32
53792,0.7,Very Good,E,SI1,62.1,60.0,2730,5.62,5.66,3.5
1543,0.7,Very Good,D,VS1,63.4,59.0,3001,5.58,5.55,3.53
2516,0.7,Ideal,E,VS2,60.5,59.0,3201,5.72,5.75,3.47
52766,0.7,Very Good,G,VS2,58.7,53.0,2563,5.83,5.86,3.43
52504,0.7,Good,D,SI1,58.0,60.0,2525,5.79,5.93,3.4
52161,0.7,Premium,D,SI1,60.8,58.0,2473,5.79,5.66,3.48
44158,0.7,Fair,F,SI2,66.4,56.0,1564,5.51,5.42,3.63
46845,0.64,Premium,E,SI1,61.3,58.0,1811,5.57,5.53,3.4
47260,0.7,Premium,J,VS2,61.2,60.0,1843,5.7,5.73,3.5
2424,0.63,Ideal,E,VVS1,61.1,58.0,3181,5.49,5.54,3.37
48887,0.7,Very Good,F,SI2,59.6,61.0,2039,5.8,5.88,3.48
51599,0.7,Good,I,VVS2,63.3,55.0,2394,5.61,5.67,3.57
46198,0.7,Fair,I,SI1,65.2,58.0,1749,5.6,5.56,3.64
49877,0.7,Premium,H,SI1,60.9,62.0,2176,5.72,5.67,3.47
52012,0.7,Good,D,SI1,59.9,63.0,2444,5.74,5.81,3.46
2986,0.7,Ideal,G,VS1,60.8,56.0,3300,5.73,5.8,3.51
277,0.71,Very Good,E,VS2,60.7,56.0,2795,5.81,5.82,3.53
809,0.71,Premium,D,SI1,59.7,59.0,2863,5.82,5.8,3.47
52887,0.72,Premium,H,VS2,60.7,59.0,2583,5.84,5.8,3.53
946,0.72,Very Good,G,VVS2,62.5,58.0,2889,5.68,5.72,3.56
51695,0.71,Very Good,I,VVS2,59.5,60.0,2400,5.82,5.87,3.48
48158,0.72,Very Good,H,SI2,63.5,58.0,1942,5.65,5.68,3.6
51672,0.72,Ideal,E,SI2,61.9,55.0,2398,5.76,5.78,3.57
3806,0.72,Ideal,E,VS1,62.5,57.0,3465,5.73,5.76,3.59
51150,0.71,Premium,F,SI2,62.0,59.0,2343,5.68,5.65,3.51
694,0.71,Premium,F,VS2,62.6,58.0,2853,5.67,5.7,3.56
50848,0.72,Premium,H,SI1,62.2,57.0,2311,5.75,5.72,3.57
45878,0.71,Premium,G,SI2,59.9,59.0,1717,5.79,5.82,3.48
49717,0.72,Premium,I,SI1,61.5,59.0,2148,5.73,5.78,3.54
2140,0.72,Ideal,H,VVS1,61.4,56.0,3124,5.79,5.77,3.55
1181,0.71,Ideal,G,VS1,62.7,57.0,2930,5.69,5.73,3.58
50722,0.71,Premium,I,VS2,62.1,59.0,2294,5.7,5.73,3.55
53191,0.71,Premium,F,SI1,62.7,57.0,2633,5.68,5.65,3.55
48876,0.71,Very Good,F,SI2,63.3,56.0,2036,5.68,5.73,3.61
3635,0.71,Ideal,G,VS1,60.7,57.0,3431,5.76,5.8,3.51
51843,0.71,Very Good,E,SI2,62.2,58.0,2423,5.65,5.7,3.53
53670,0.74,Very Good,H,VS1,61.9,59.1,2709,5.74,5.77,3.56
7260,0.9,Ideal,F,SI2,61.5,56.0,4198,6.24,6.18,3.82
7909,0.9,Ideal,G,SI2,60.7,57.0,4314,6.19,6.33,3.8
8568,0.9,Premium,F,SI1,61.4,55.0,4435,6.18,6.16,3.79
1110,0.8,Very Good,F,SI1,63.5,55.0,2914,5.86,5.89,3.73
53096,0.75,Ideal,I,VS1,63.0,57.0,2613,5.8,5.82,3.66
1207,0.76,Premium,E,SI1,58.3,62.0,2937,6.12,5.95,3.52
580,0.78,Ideal,I,VS2,61.8,55.0,2834,5.92,5.95,3.67
47891,0.74,Very Good,J,SI1,62.2,59.0,1913,5.74,5.81,3.59
1486,0.77,Premium,E,SI1,61.7,58.0,2988,5.86,5.9,3.63
53472,0.76,Ideal,E,SI2,61.5,55.0,2680,5.88,5.93,3.63
4245,0.84,Good,E,SI1,61.9,61.0,3577,6.03,6.05,3.74
4671,0.76,Ideal,G,VVS1,62.0,54.7,3671,5.83,5.87,3.62
1813,0.78,Very Good,E,SI1,60.9,57.0,3055,5.93,5.97,3.62
682,0.75,Ideal,J,SI1,61.5,56.0,2850,5.83,5.87,3.6
113,0.9,Premium,I,VS2,63.0,58.0,2761,6.16,6.12,3.87
3221,0.9,Very Good,G,SI2,63.5,57.0,3350,6.09,6.13,3.88
9439,0.9,Very Good,H,VVS2,63.7,57.0,4592,6.09,6.02,3.86
53398,0.83,Ideal,H,SI2,61.1,59.0,2666,6.05,6.1,3.71
4108,0.74,Ideal,G,VVS1,62.1,54.0,3537,5.8,5.83,3.61
4215,0.91,Very Good,H,VS2,63.1,56.0,3567,6.2,6.13,3.89
9572,1.0,Premium,D,SI2,62.2,61.0,4626,6.36,6.3,3.94
8097,0.95,Premium,D,SI2,60.1,61.0,4341,6.37,6.35,3.82
14644,1.0,Premium,H,VVS2,61.4,59.0,5914,6.49,6.45,3.97
12007,1.0,Good,G,VS2,63.8,59.0,5148,6.26,6.34,4.02
3802,1.0,Very Good,J,SI1,61.9,62.0,3465,6.33,6.36,3.93
6503,0.97,Fair,F,SI1,56.4,66.0,4063,6.59,6.54,3.7
9575,1.0,Premium,D,SI2,59.4,60.0,4626,6.56,6.48,3.87
4748,0.92,Premium,F,SI1,62.6,59.0,3684,6.23,6.19,3.89
10565,1.0,Premium,G,SI1,60.8,58.0,4816,6.48,6.45,3.93
9806,0.91,Very Good,E,SI2,63.2,56.0,4668,6.08,6.14,3.86
13270,1.0,Good,G,VS2,56.6,61.0,5484,6.65,6.61,3.75
18435,1.0,Good,D,VS1,57.8,61.0,7500,6.62,6.56,3.81
3591,0.91,Premium,G,SI2,61.3,60.0,3423,6.17,6.2,3.79
5447,1.0,Fair,H,SI1,55.2,64.0,3830,6.69,6.64,3.68
15947,1.0,Premium,G,VS1,62.4,60.0,6377,6.39,6.37,3.98
10800,1.0,Good,H,VS2,63.7,59.0,4861,6.3,6.26,4.0
5849,1.0,Premium,H,SI2,61.3,58.0,3920,6.45,6.41,3.94
8315,0.91,Very Good,D,SI1,63.5,56.0,4389,6.13,6.18,3.91
4151,0.91,Premium,F,SI2,61.0,51.0,3546,6.24,6.21,3.8
9426,1.01,Very Good,D,SI2,62.8,59.0,4588,6.34,6.44,4.01
10581,1.01,Very Good,D,SI1,59.1,61.0,4821,6.46,6.5,3.83
15174,1.01,Very Good,H,VVS2,63.3,57.0,6097,6.39,6.35,4.03
5937,1.01,Very Good,F,SI2,60.8,63.0,3945,6.32,6.38,3.86
9236,1.01,Good,H,SI1,63.3,58.0,4559,6.37,6.4,4.04
15117,1.01,Premium,D,SI1,61.8,58.0,6075,6.42,6.37,3.95
7700,1.01,Fair,F,SI1,67.2,60.0,4276,6.06,6.0,4.05
9013,1.01,Premium,H,SI1,61.3,58.0,4513,6.47,6.39,3.94
15740,1.01,Ideal,G,VS2,60.6,58.0,6295,6.44,6.5,3.92
11337,1.01,Good,F,SI1,63.7,57.0,4989,6.4,6.35,4.06
15199,1.01,Very Good,G,VS2,61.9,56.0,6105,6.34,6.42,3.95
10942,1.01,Very Good,F,SI1,59.7,61.0,4899,6.49,6.55,3.89
4744,1.01,Very Good,G,SI2,62.0,58.0,3682,6.41,6.46,3.99
18733,1.01,Very Good,D,VS2,62.7,57.0,7652,6.36,6.39,4.0
15525,1.01,Very Good,E,VS2,63.0,60.0,6221,6.32,6.35,3.99
16288,1.01,Very Good,E,VS2,63.3,60.0,6516,6.33,6.3,4.0
11015,1.01,Very Good,G,SI1,60.6,57.0,4916,6.49,6.52,3.94
16798,1.01,Premium,E,VS2,60.4,57.0,6697,6.49,6.45,3.91
11293,1.01,Ideal,H,SI1,62.3,55.0,4977,6.43,6.37,3.99
13505,1.01,Ideal,D,SI1,61.2,57.0,5543,6.47,6.44,3.95
13562,1.02,Very Good,E,SI1,59.2,56.0,5553,6.57,6.63,3.91
9083,1.03,Premium,E,SI2,61.0,60.0,4522,6.53,6.46,3.96
9159,1.02,Very Good,E,SI2,63.3,58.0,4540,6.31,6.4,4.02
10316,1.03,Very Good,G,SI1,63.2,58.0,4764,6.43,6.38,4.05
12600,1.02,Very Good,F,SI1,60.9,57.0,5287,6.52,6.56,3.98
15398,1.02,Very Good,G,VS2,63.4,59.0,6169,6.32,6.3,4.0
8405,1.03,Ideal,I,SI1,63.3,57.0,4401,6.37,6.46,4.06
17889,1.04,Ideal,D,VS2,61.9,55.0,7220,6.5,6.52,4.03
7153,1.04,Very Good,F,SI2,62.3,58.0,4181,6.44,6.5,4.03
16983,1.03,Premium,F,VS1,61.7,56.0,6783,6.49,6.47,4.0
11198,1.02,Premium,H,VS2,60.0,58.0,4958,6.56,6.5,3.92
5865,1.03,Ideal,J,SI1,62.6,57.0,3922,6.45,6.43,4.03
15016,1.02,Very Good,D,SI1,62.8,56.0,6047,6.39,6.44,4.03
7502,1.04,Premium,E,SI2,61.6,59.0,4240,6.57,6.55,4.04
14328,1.03,Ideal,D,SI1,61.2,55.0,5804,6.51,6.57,4.0
8632,1.02,Premium,G,SI1,62.6,59.0,4449,6.43,6.38,4.01
7041,1.02,Ideal,F,SI2,62.1,56.0,4162,6.41,6.44,3.99
21809,1.03,Ideal,F,VVS1,61.3,54.0,9881,6.56,6.62,4.04
48885,1.04,Fair,I,I1,67.3,56.0,2037,6.34,6.23,4.22
16635,1.02,Premium,F,VS2,62.4,59.0,6652,6.4,6.45,4.01
15538,1.09,Ideal,I,VS1,61.8,55.0,6225,6.59,6.62,4.08
18682,1.11,Ideal,G,VS1,61.5,58.0,7639,6.7,6.66,4.11
7580,1.06,Very Good,I,SI1,62.8,56.0,4255,6.47,6.52,4.08
8646,1.06,Premium,F,SI2,62.4,58.0,4452,6.54,6.5,4.07
20512,1.11,Ideal,G,VVS2,63.1,57.0,8843,6.55,6.6,4.15
13460,1.13,Very Good,G,SI1,63.1,58.0,5526,6.65,6.59,4.18
11822,1.07,Ideal,I,SI1,61.7,56.0,5093,6.59,6.57,4.06
19907,1.09,Premium,G,VVS2,59.5,61.0,8454,6.74,6.7,4.0
16948,1.08,Ideal,G,VS2,60.3,59.0,6769,6.62,6.64,4.0
15439,1.05,Premium,G,VS2,61.8,58.0,6181,6.59,6.52,4.05
17304,1.09,Ideal,G,VS1,62.4,57.0,6934,6.55,6.63,4.11
14807,1.11,Ideal,E,SI2,60.6,56.0,5962,6.76,6.78,4.1
21425,1.07,Ideal,G,IF,61.5,57.0,9532,6.59,6.54,4.04
4661,1.13,Ideal,H,I1,61.1,56.0,3669,6.77,6.71,4.12
16344,1.1,Ideal,G,VS1,61.3,54.0,6535,6.69,6.65,4.09
11847,1.05,Ideal,I,VS1,61.5,55.0,5101,6.56,6.61,4.05
16867,1.07,Premium,G,VS1,62.0,58.0,6730,6.59,6.53,4.07
21535,1.12,Ideal,F,VVS2,61.4,57.0,9634,6.69,6.66,4.1
8220,1.09,Very Good,J,VS2,62.3,59.0,4372,6.56,6.63,4.11
18833,1.12,Ideal,G,VS1,61.6,55.0,7716,6.69,6.72,4.13
13956,1.16,Very Good,G,SI1,60.7,59.0,5678,6.74,6.87,4.13
20531,1.23,Premium,F,VS2,59.6,58.0,8855,6.94,7.02,4.16
12498,1.15,Very Good,E,SI2,60.0,59.0,5257,6.78,6.82,4.08
14003,1.2,Premium,I,VS2,62.6,58.0,5699,6.77,6.72,4.22
22973,1.2,Premium,F,VVS2,62.2,58.0,11021,6.83,6.78,4.23
8795,1.21,Premium,F,SI2,61.8,59.0,4472,6.82,6.77,4.2
18812,1.24,Ideal,H,VS2,60.1,59.0,7701,6.99,7.03,4.21
26565,1.2,Ideal,E,VVS1,61.8,56.0,16256,6.78,6.87,4.22
20122,1.24,Ideal,G,VS1,61.9,54.0,8584,6.89,6.92,4.27
12313,1.24,Ideal,I,SI2,61.9,57.0,5221,6.87,6.92,4.27
15155,1.21,Premium,F,SI2,59.0,60.0,6092,6.99,6.94,4.11
18869,1.22,Ideal,H,VS1,60.4,57.0,7738,6.86,6.89,4.15
16067,1.2,Premium,H,VS2,62.5,58.0,6416,6.77,6.73,4.23
10468,1.21,Very Good,I,SI2,62.1,59.0,4791,6.8,6.86,4.24
12328,1.2,Very Good,J,VS1,62.9,60.0,5226,6.64,6.69,4.19
7885,1.21,Premium,F,SI2,62.4,60.0,4310,6.77,6.73,4.21
23561,1.21,Ideal,G,VVS1,61.5,56.0,11572,6.83,6.89,4.22
20700,1.22,Very Good,G,VVS2,61.9,58.0,8975,6.84,6.85,4.24
20006,1.2,Ideal,G,VS1,62.4,57.0,8545,6.78,6.8,4.24
15584,1.2,Premium,F,SI1,62.4,58.0,6250,6.81,6.75,4.23
24545,1.51,Premium,G,VS1,62.4,60.0,12831,7.3,7.34,4.57
26041,1.5,Premium,D,VS2,61.8,60.0,15240,7.37,7.3,4.53
25000,1.5,Very Good,G,VS2,61.1,60.0,13528,7.4,7.3,4.49
6157,1.25,Fair,H,SI2,64.4,58.0,3990,6.82,6.71,4.36
10957,1.25,Ideal,H,SI2,61.6,54.0,4900,6.94,6.88,4.25
14113,1.4,Premium,G,SI2,60.6,58.0,5723,7.26,7.22,4.39
15653,1.26,Ideal,F,SI2,62.7,58.0,6277,6.91,6.87,4.32
12682,1.26,Ideal,J,VS2,63.2,57.0,5306,6.86,6.81,4.32
21426,1.5,Very Good,I,VS2,63.3,55.0,9533,7.3,7.26,4.61
22405,1.5,Good,G,SI1,64.2,58.0,10428,7.14,7.2,4.6
20409,1.5,Premium,F,SI1,62.1,60.0,8770,7.32,7.27,4.53
19944,1.5,Premium,H,SI2,62.3,60.0,8490,7.22,7.3,4.52
16950,1.5,Very Good,H,SI2,63.3,57.0,6770,7.27,7.21,4.59
19527,1.5,Good,I,SI1,62.9,60.0,8161,7.12,7.16,4.49
19250,1.33,Premium,H,VS2,60.7,59.0,7982,7.08,7.13,4.31
15127,1.32,Very Good,J,VS2,62.1,57.0,6079,7.01,7.04,4.36
24098,1.5,Very Good,E,SI1,59.3,60.0,12247,7.4,7.5,4.42
16218,1.33,Very Good,H,SI2,62.5,58.0,6482,7.04,6.97,4.38
20898,1.51,Premium,I,VS2,63.0,60.0,9116,7.3,7.25,4.58
21870,1.25,Ideal,D,VS2,62.6,56.0,9933,6.84,6.87,4.29
25222,1.7,Ideal,H,VS1,62.4,55.0,13823,7.61,7.69,4.77
24230,1.62,Good,H,VS2,61.5,60.8,12429,7.48,7.53,4.62
22614,1.52,Good,F,SI1,63.6,54.0,10664,7.33,7.22,4.63
22933,1.52,Ideal,I,VVS1,61.9,56.0,10968,7.34,7.37,4.55
19386,1.55,Ideal,I,SI2,60.7,60.0,8056,7.49,7.46,4.54
20220,1.54,Premium,J,VVS2,61.1,59.0,8652,7.45,7.4,4.54
24512,1.53,Ideal,E,SI1,62.3,54.2,12791,7.35,7.38,4.59
21122,1.54,Very Good,J,VS1,63.5,57.0,9285,7.27,7.37,4.65
23411,1.67,Premium,I,VS1,61.1,58.0,11400,7.69,7.6,4.67
19348,1.56,Good,I,SI2,58.5,61.0,8048,7.58,7.63,4.45
19758,1.56,Premium,J,VS1,61.1,59.0,8324,7.49,7.52,4.58
25204,1.52,Very Good,D,VS2,62.4,58.0,13799,7.23,7.28,4.53
27338,1.7,Ideal,F,VS2,62.3,56.0,17892,7.61,7.65,4.75
27530,1.7,Ideal,G,VVS1,61.0,56.0,18279,7.62,7.67,4.66
25164,1.7,Premium,F,VS2,62.5,61.0,13737,7.54,7.45,4.69
24018,1.7,Ideal,D,SI1,60.0,54.0,12190,7.76,7.71,4.64
15979,1.7,Ideal,H,I1,61.3,55.0,6397,7.7,7.63,4.7
25184,1.52,Ideal,G,VS2,62.1,56.0,13768,7.39,7.34,4.57
20248,1.55,Ideal,H,SI2,62.1,57.0,8678,7.39,7.43,4.6
17928,1.53,Ideal,G,SI2,61.7,57.0,7240,7.44,7.41,4.58
24211,2.14,Ideal,H,SI2,61.9,57.0,12400,8.34,8.28,5.14
24747,1.71,Premium,I,VS1,60.7,60.0,13097,7.74,7.71,4.69
22986,2.0,Good,J,SI2,61.5,61.0,11036,7.97,8.06,4.93
27421,2.32,Fair,H,SI1,62.0,62.0,18026,8.47,8.31,5.2
26081,2.0,Very Good,H,SI2,59.7,61.0,15312,8.15,8.2,4.88
21099,1.73,Premium,J,SI1,60.7,58.0,9271,7.78,7.73,4.71
24148,2.3,Ideal,J,SI1,62.3,57.0,12316,8.41,8.34,5.22
25882,2.06,Premium,I,SI2,60.1,58.0,14982,8.32,8.26,4.98
25883,2.01,Ideal,H,SI2,62.5,53.9,14998,8.04,8.07,5.04
26611,2.05,Premium,G,SI2,60.1,59.0,16357,8.2,8.3,4.96
26458,2.02,Premium,H,SI2,59.9,55.0,15996,8.28,8.17,4.93
20983,1.71,Premium,H,SI1,58.1,59.0,9193,7.88,7.81,4.56
22389,2.02,Ideal,I,SI2,62.2,57.0,10412,8.06,7.99,4.99
27090,2.15,Premium,H,SI2,62.8,58.0,17221,8.22,8.17,5.15
26063,1.77,Premium,E,VS2,61.6,58.0,15278,7.78,7.71,4.77
26617,2.28,Premium,J,VS2,62.4,58.0,16369,8.45,8.35,5.24
21815,1.75,Ideal,J,VS2,62.1,56.0,9890,7.74,7.69,4.79
24887,2.06,Premium,G,SI1,59.3,61.0,13317,8.44,8.36,4.98
26079,2.04,Ideal,I,SI1,60.0,60.0,15308,8.3,8.26,4.97
24966,2.02,Premium,H,SI1,63.0,60.0,13453,7.85,7.79,4.93
"""



# --- Activation Function (ReLU) and Its Derivative ---
def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x > 0).astype(float)

# --- Neural Network Class with SGLD and Customizable Noise & LR Decay ---
# Fixed architecture: 1-4-4-1 using ReLU.
class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):
        self.W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)
        self.b1 = np.zeros((1, hidden_size1))
        self.W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)
        self.b2 = np.zeros((1, hidden_size2))
        self.W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)
        self.b3 = np.zeros((1, output_size))
    
    def forward(self, X):
        self.Z1 = np.dot(X, self.W1) + self.b1
        self.A1 = relu(self.Z1)
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = relu(self.Z2)
        self.Z3 = np.dot(self.A2, self.W3) + self.b3
        return self.Z3
    
    def compute_loss(self, y_pred, y_true):
        return np.mean((y_pred - y_true)**2)
    
    def backward(self, X, y_true, y_pred, lr, noise_multiplier, noise_exponent):
        m = y_true.shape[0]
        dZ3 = (2.0 / m) * (y_pred - y_true)
        dW3 = np.dot(self.A2.T, dZ3)
        db3 = np.sum(dZ3, axis=0, keepdims=True)
        
        dA2 = np.dot(dZ3, self.W3.T)
        dZ2 = dA2 * relu_deriv(self.Z2)
        dW2 = np.dot(self.A1.T, dZ2)
        db2 = np.sum(dZ2, axis=0, keepdims=True)
        
        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * relu_deriv(self.Z1)
        dW1 = np.dot(X.T, dZ1)
        db1 = np.sum(dZ1, axis=0, keepdims=True)
        
        # Customizable SGLD update:
        # Compute noise standard deviation based on the user-specified exponent and multiplier.
        noise_std = noise_multiplier * (lr ** noise_exponent)
        self.W3 = self.W3 - (lr/2)*dW3 + np.random.normal(0, noise_std, self.W3.shape)
        self.b3 = self.b3 - (lr/2)*db3 + np.random.normal(0, noise_std, self.b3.shape)
        self.W2 = self.W2 - (lr/2)*dW2 + np.random.normal(0, noise_std, self.W2.shape)
        self.b2 = self.b2 - (lr/2)*db2 + np.random.normal(0, noise_std, self.b2.shape)
        self.W1 = self.W1 - (lr/2)*dW1 + np.random.normal(0, noise_std, self.W1.shape)
        self.b1 = self.b1 - (lr/2)*db1 + np.random.normal(0, noise_std, self.b1.shape)
    
    def train(self, X, y, epochs, learning_rate, burn_in, sample_interval, noise_multiplier, noise_exponent, lr_decay_factor):
        burn_in_losses = []
        sampling_losses = []
        samples = []
        
        lr = learning_rate
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = self.compute_loss(y_pred, y)
            if epoch < burn_in:
                burn_in_losses.append(loss)
            else:
                sampling_losses.append(loss)
            self.backward(X, y, y_pred, lr, noise_multiplier, noise_exponent)
            # At the end of burn-in, decay the learning rate.
            if epoch == burn_in:
                lr = learning_rate * lr_decay_factor
            # After burn-in, record weight samples at intervals.
            if epoch >= burn_in and (epoch - burn_in) % sample_interval == 0:
                sample = {
                    "W1": self.W1.copy(),
                    "b1": self.b1.copy(),
                    "W2": self.W2.copy(),
                    "b2": self.b2.copy(),
                    "W3": self.W3.copy(),
                    "b3": self.b3.copy()
                }
                samples.append(sample)
        return burn_in_losses, sampling_losses, samples

# --- Helper: Forward Pass with a Given Weight Sample ---
def forward_with_sample(X, sample):
    Z1 = np.dot(X, sample["W1"]) + sample["b1"]
    A1 = relu(Z1)
    Z2 = np.dot(A1, sample["W2"]) + sample["b2"]
    A2 = relu(Z2)
    Z3 = np.dot(A2, sample["W3"]) + sample["b3"]
    return Z3

# --- Shiny UI ---
app_ui = ui.page_fluid(
    ui.h2("Bayesian Neural Network with SGLD - Custom Noise and LR Decay"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("epochs", "Training Epochs", min=100, max=3000, value=1000, step=100),
            ui.input_slider("learning_rate", "Learning Rate", min=0.0001, max=0.01, value=0.01, step=0.0001),
            ui.input_slider("burn_in", "Burn-in Epochs", min=0, max=1000, value=500, step=50),
            ui.input_slider("sample_interval", "Sample Interval", min=1, max=50, value=10, step=1),
            ui.input_slider("noise_exponent", "Noise Exponent", min=0.1, max=1.0, value=0.5, step=0.05),
            ui.input_slider("noise_multiplier", "Noise Multiplier", min=0.1, max=10.0, value=1.0, step=0.1),
            ui.input_slider("lr_decay", "Learning Rate Decay Factor", min=0.1, max=1.0, value=0.1, step=0.05)
        ),
        ui.navset_tab(
            ui.nav_panel("Burn-in Loss",
                ui.output_plot("burnInLossPlot", height="400px")
            ),
            ui.nav_panel("Sampling Loss",
                ui.output_plot("samplingLossPlot", height="400px")
            ),
            ui.nav_panel("Predictions",
                ui.output_plot("predictionPlot", height="400px")
            )
        )
    )
)

# --- Shiny Server ---
def server(input, output, session):
    
    @reactive.Calc
    def train_model():
        # Read and preprocess data.
        df = pd.read_csv(StringIO(data_str))
        X = df["carat"].values.reshape(-1, 1)
        y = df["price"].values.reshape(-1, 1)
        X_mean, X_std = X.mean(), X.std()
        X_norm = (X - X_mean) / X_std
        y_mean, y_std = y.mean(), y.std()
        y_norm = (y - y_mean) / y_std
        
        # Get hyperparameters from the UI.
        epochs = input.epochs()
        learning_rate = input.learning_rate()
        burn_in = input.burn_in()
        sample_interval = input.sample_interval()
        noise_exponent = input.noise_exponent()
        noise_multiplier = input.noise_multiplier()
        lr_decay = input.lr_decay()
        
        # Create and train the network using SGLD.
        nn = NeuralNetwork(input_size=1, hidden_size1=4, hidden_size2=4, output_size=1)
        burn_in_losses, sampling_losses, samples = nn.train(
            X_norm, y_norm, epochs, learning_rate, burn_in, sample_interval,
            noise_multiplier, noise_exponent, lr_decay
        )
        
        # Compute predictions from the collected samples.
        preds = []
        for sample in samples:
            pred_norm = forward_with_sample(X_norm, sample)
            preds.append(pred_norm)
        preds = np.array(preds)
        pred_mean_norm = preds.mean(axis=0)
        pred_std_norm = preds.std(axis=0)
        pred_mean = pred_mean_norm * y_std + y_mean
        pred_std = pred_std_norm * y_std
        
        return {"burn_in_losses": burn_in_losses,
                "sampling_losses": sampling_losses,
                "X": X, "y": y,
                "pred_mean": pred_mean, "pred_std": pred_std}
    
    @output
    @render.plot
    def burnInLossPlot():
        result = train_model()
        losses = result["burn_in_losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="green")
        ax.set_title("Burn-in Phase Loss")
        ax.set_xlabel("Epoch (burn-in)")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def samplingLossPlot():
        result = train_model()
        losses = result["sampling_losses"]
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(losses, color="purple")
        ax.set_title("Sampling Phase Loss (Post Burn-in)")
        ax.set_xlabel("Epoch (sampling)")
        ax.set_ylabel("MSE Loss")
        return fig
    
    @output
    @render.plot
    def predictionPlot():
        result = train_model()
        X = result["X"].flatten()
        y = result["y"].flatten()
        pred_mean = result["pred_mean"].flatten()
        pred_std = result["pred_std"].flatten()
        order = np.argsort(X, axis=0)
        X_sorted = X[order]
        pred_mean_sorted = pred_mean[order]
        pred_std_sorted = pred_std[order]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(X, y, color="blue", label="Actual Price")
        ax.plot(X_sorted, pred_mean_sorted, color="red", label="Predicted Mean")
        ax.fill_between(X_sorted,
                        pred_mean_sorted - 2*pred_std_sorted,
                        pred_mean_sorted + 2*pred_std_sorted,
                        color="red", alpha=0.3, label="Uncertainty (±2σ)")
        ax.set_title("Actual vs. Predicted Diamond Price (SGLD)")
        ax.set_xlabel("Carat")
        ax.set_ylabel("Price")
        ax.legend()
        return fig

app = App(app_ui, server)


```

