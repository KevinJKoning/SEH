---
title: Machine Learning
format: html
filters:
  - shinylive
---

I assume most engineers, a priori, consider *machine learning* models as somehow 'better' than statistical models. In reality there is a *lot* of overlap between them, and the situation should dictate the model selection. However, since we assume the reader will inevitably be interested in machine learning, we think it's best to include it, at the very least so we can understand the similarities and differences when compared to statistical models.

Due to the amount of blur/overlap, this may not be a universally agreed distinction, but we define an important difference this way:

* Statistical models are built around probability theory
* Machine learning models are built around an objective/loss function

The confusing thing is that a lot of methods known as being machine learning have objective/loss functions that are purely probabilistic - but the key is they *do not have to be*.

There are other common differences that are also blurry - like that machine learning is focused on larger datasets and often have more flexible (i.e. more parameters) methods. This tends to also mean overfitting is a larger concern in machine learning. You would also typically not define a causal model as a foundation for a machine learning model...

There is a real danger and opportunity in including machine learning... The danger is that the themes of the primer are just muddied and confused. The opportunity is that there is a comparison point to make the themese even clearer, and the reader will appreciate the inclusion of the topic.

**It's a little weird including machine learning in P(D|M) consider the point is that you can't do that with many machine learning models**

Include primarily Bayesian neural networks and decision trees. Show how only one has the statistical interpretation.

**Statistical models are always probabilistic and generative, machine learning models, if they are probabilistic and generative also work for P(D|M) but not all are**

## Loss Function

A loss function, denoted as $L$, is typically computed as the difference between the true target values $y$ and the predicted values $\hat{y}$ The generic form of a loss function can be expressed as:

$$
L(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} \ell(y_i, \hat{y}_i)
$$

#### Variables:
- $y_i$: The true value for the \(i\)-th data point.
- $\hat{y}_i$: The predicted value for the \(i\)-th data point.
- $\ell(y_i, \hat{y}_i)$: The individual error for the \(i\)-th data point, defined by a specific loss function (e.g., squared error, cross-entropy).
- $N$: The total number of data points.

#### Explanation:
- The function $\ell(y_i, \hat{y}_i)$ depends on the task (regression or classification). 
  - Example for regression: $\ell(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$ (Mean Squared Error).
  - Example for classification: $\ell(y_i, \hat{y}_i) = -[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$ (Binary Cross-Entropy).
- The summation aggregates the error across all data points.
- Dividing by $N$ ensures the loss represents the average error.

## Likelihood Function

The loss function is in contrast to using likelihood functions, which are based in probability theory. We use likelihood to find the best statistical model, specifically by finding the Maximum Likelihood Estimate (MLE).

It's a bit awkward to mention Likelihood here, because we have been trying to wait until the second half of the primer in order to tackle it head on. However, you probably have an expectation that Machine Learning will be covered, at least to some degree, and it makes sense to do that while introducing all the other types of models. I'll continue to make it awkward and just state that yes this is the difference, but to understand Likelihood we'll have to wait to the second half...

## Machine Learning Models

The following are some of the most common machine learning models. *Supervised learning* means that there is a result or 'y' variable to learn from. *Unsupervised learning* does not have the result or 'y' variable, which is a little confusing - instead of trying to predict they are generally attempting some kind of categorization/clustering or simplification.

You may note that the first two on the list below, linear regression and logistic regression, would also firmly sit on most lists of common statistical models.

#### Supervised Learning
1. **Linear Regression**: Predicts continuous values.
2. **Logistic Regression**: For binary or multi-class classification.
3. **Decision Trees**: Simple, interpretable models for classification and regression.
4. **Random Forest**: Ensemble of decision trees for better accuracy.
5. **Gradient Boosting (e.g., XGBoost, LightGBM)**: Powerful ensemble method for structured data.
6. **Support Vector Machines (SVM)**: For classification and regression.
7. **Neural Networks**: Flexible models, basis of deep learning.

#### Unsupervised Learning
1. **k-Means Clustering**: Groups similar data points.
2. **Principal Component Analysis (PCA)**: Reduces dimensionality while preserving variance.


## Bayesian Neural Net

```{mermaid}
graph LR
  subgraph Input Layer
    A1(["In 1"]) 
    A2(["In 2"]) 
    A3(["In 3"]) 
  end

  subgraph Hidden Layer 1
    B1(["H1 1"])
    B2(["H1 2"])
    B3(["H1 3"])
  end

  subgraph Hidden Layer 2
    C1(["H2 1"])
    C2(["H2 2"])
  end

  subgraph Output Layer
    D1(["Out"])
  end

  A1 -->|~Prob. Conn.~| B1
  A1 -->|~Prob. Conn.~| B2
  A2 -->|~Prob. Conn.~| B2
  A2 -->|~Prob. Conn.~| B3
  A3 -->|~Prob. Conn.~| B1
  A3 -->|~Prob. Conn.~| B3
  
  B1 -->|~Prob. Conn.~| C1
  B1 -->|~Prob. Conn.~| C2
  B2 -->|~Prob. Conn.~| C1
  B3 -->|~Prob. Conn.~| C2
  
  C1 -->|~Prob. Conn.~| D1
  C2 -->|~Prob. Conn.~| D1
```

## Summary
There are plently of resources for this topic... My suggestion is to start with 'Statistical Learning'....
