---
title: Without Priors
format: html
filters:
  - shinylive
---

## Preview

We finally flip to $P(D|M)$ the probability of a [model]{style="text-decoration: underline;"} given the [data]{style="text-decoration: underline;"}.

Like before, we begin with a simple dice model as we consider it to be an intuitive subject. We then work forward in generally the same order as the first half of the primer. We work with probability distributions and how to find the most likely parameters given the observed data. We pause to examine how likely we are to calculate one set of parameters when the true process has another set of parameters. As usual we use computation to avoid misunderstood analytical solutions.

We then switch to the likelihood of models that can ingest data. We start with standard linear regression and show that it has a very efficient analytical solution. We also consider how we updated our linear regression model to better model heteroscedasticity (a word I cannot spell or pronounce), and how that breaks the analytical solution. We start to explore non-analytical solutions, but that will end when we begin to find priors very useful. That will be the subject of the next and last chapter.

## Data Generation Reversed

In part one we wanted to understand the probability of the data based on a fixed model of a data generating process. In part two we want to take the data and find the most likely model of the data generating process. It's reasonable to think of this as the 'reverse' of our previous approach in part one.

The simple app below lets you select a model parameter, the number of dice to roll, such that you can see if your selection makes it match the data better or worse. See if you can find a parameter value that does a particularly good job of matching the data.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

# --- Precompute the "permanent" histogram for 7 dice, 10,000 rolls ---
FIXED_NUM_DICE = 7
FIXED_NUM_ROLLS = 10000

fixed_sums = [np.random.randint(1, 7, FIXED_NUM_DICE).sum() for _ in range(FIXED_NUM_ROLLS)]
fixed_unique_vals, fixed_counts = np.unique(fixed_sums, return_counts=True)

app_ui = ui.page_fluid(
    ui.h2("Dice Rolling Demo"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "num_dice",
                "Number of Dice (1–10)",
                min=1,
                max=10,
                value=2,
                step=1
            ),
        ),
        ui.output_plot("dicePlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def user_sums():
        # Always roll the user-selected dice 10,000 times
        N_ROLLS = 10000
        n_dice = input.num_dice()
        rolls = [np.random.randint(1, 7, n_dice).sum() for _ in range(N_ROLLS)]
        return rolls

    @output
    @render.plot
    def dicePlot():
        # Get the user’s histogram
        sums = user_sums()
        user_unique_vals, user_counts = np.unique(sums, return_counts=True)
        
        # Determine the union of x-values (totals) so both histograms can share the same axis
        all_x = np.arange(
            min(fixed_unique_vals[0], user_unique_vals[0]),
            max(fixed_unique_vals[-1], user_unique_vals[-1]) + 1
        )
        
        # Convert the unique/value arrays to dictionaries for easy indexing
        fixed_map = dict(zip(fixed_unique_vals, fixed_counts))
        user_map = dict(zip(user_unique_vals, user_counts))
        
        # Pull out frequency or 0 if total not present in the distribution
        fixed_freqs = [fixed_map.get(x, 0) for x in all_x]
        user_freqs  = [user_map.get(x, 0) for x in all_x]
        
        # Plot
        fig, ax = plt.subplots()
        
        # Bar chart for the fixed 7-dice histogram
        ax.bar(all_x, fixed_freqs, color="lightblue", alpha=0.6, label="Fixed Dice")
        
        # Overlay user histogram as points
        ax.scatter(all_x, user_freqs, color="red", marker="o", label="User Selected Dice")
        
        ax.set_title("Update the Input Parameter to Match Observations")
        ax.set_xlabel("Dice Total")
        ax.set_ylabel("Frequency")
        ax.legend()
        
        # Make x-axis tick at every possible total
        plt.xticks(all_x, rotation=90)
        
        return fig

app = App(app_ui, server)
```

I'm guessing you succeeded. We want to be able to do that automatically, and with many more parameters, such that if we have data but aren't certain about the model generating it, we can work in 'reverse' to find a likely model of the real world data generating process.

## Likelihood

In the previous dice example, we were just eyeballing the right model parameters to maximize the probability of observing our data under the model. We’d like a more consistent and mathematical way to achieve the same intent, which is called a **likelihood** function. In generic form, it looks like this:

$$
\mathcal{L}(\theta \mid \mathbf{y}) = \prod_{i=1}^{N} P(y_i \mid \theta)
$$ 

1. $\mathcal{L}(\theta \mid \mathbf{y})$: The likelihood of the model parameter[s] ($\theta$) given the observed data ($\mathbf{y}$).

2. $P(y_i \mid \theta)$: The probability of a data point ($y_i$) given the model parameter[s] ($\theta$). This is the individual probability density for continuous functions or probability for discrete functions. 

3. $\prod_{i=1}^{N}$: This is the product operation, which multiplies the individual probabilities $P(y_i \mid \theta)$ across all $N$ data points in the dataset. This assumes that the data points are independent. In practice this is typically replaced by the addition of log probabilities, as previously described.

In practice, calculating $P(y_i \mid \theta)$ generally requires making an informed assumption about which named probability distribution best represents your data. We've tried to make the point that reality does not consist of only the *named* probability distributions, however, we use them because they allow for efficient probability calculations, and we can generally choose a named probability distribution that fits the problem as well as practically needed.

In the dice app above, the likelihood of the number of dice parameter being a value of 1, 2, or 3 based on the data shown is approximately zero ($\mathcal{L}(\theta \in {1,2,3} \mid \mathbf{y}) \approx 0$). In the next section we'll make the calculations more precisely.

## Maximum Likelihood Estimation with Discrete Data

It is common to consider the best model to be the one that maximizes the likelihood of the observed data. This is called Maximum Likelihood Estimation (MLE). The difficulty in finding the maximum likelihood estimate varies substantially depending on the problem. For simpler problems there is often a derived analytical solution. We will touch on these briefly, but we hold to one of our general themes, which is that we prefer computational solutions that can handle real-world problems.

We introduced the binomial distribution in our chapter on discrete probability distributions. There we worked a problem in which we could find the exact probability of the data given a model, P(D|M), where the model was the binomial distribution with a given value of the p parameter. However, we now want to work in the other direction, P(M|D), in which we find the value of the p parameter with the maximum likelihood.

Although the app below does not automatically give you the maximum likelihood estimate, it will hopefully make it easy for you to approximate the value. Pick several p values you think are reasonable and for each select 'Add to plot'.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render
from scipy.stats import binom

app_ui = ui.page_fluid(
    ui.h3("In four batches of 1,000 products, we have 992, 995, 989, and 998 pass, what is the most likely pass probability?"),
    
    # Input row
    ui.row(
        ui.column(
            4,
            ui.input_numeric(
                "pInput", "Enter probability (p):",
                value=0.993, min=0, max=1, step=0.001
            ),
        ),
        ui.column(
            4,
            ui.br(),
            ui.br(),
            ui.input_action_button("addBtn", "Add to Plot"),
            ui.input_action_button("resetBtn", "Reset", class_="btn-danger ms-2"),
        ),
        ui.column(
            4,
            ui.h4("MLE Estimate of p:"),
            ui.output_text("mleOutput"),
        ),
    ),
    
    ui.br(),
    
    # Plots
    ui.row(
        ui.column(
            6,
            ui.output_plot("batchPlot"),
        ),
        ui.column(
            6,
            ui.output_plot("probPlot"),
        ),
    ),
)

def server(input, output, session):
    # Data for the four batches
    batch_data = [(992, 1000), (995, 1000), (989, 1000), (998, 1000)]  # (successes, trials)
    
    # Store the user-selected probabilities and their log-likelihoods
    stored_probs = reactive.Value([])
    stored_lls = reactive.Value([])
    
    # Calculate MLE estimate
    total_success = sum(x[0] for x in batch_data)
    total_trials = sum(x[1] for x in batch_data)
    mle_p = total_success / total_trials
    
    @output
    @render.text
    def mleOutput():
        return f"p = {mle_p:.6f}"
    
    # Reset function
    @reactive.Effect
    @reactive.event(input.resetBtn)
    def _():
        stored_probs.set([])
        stored_lls.set([])
    
    # Add new probability when button is clicked
    @reactive.Effect
    @reactive.event(input.addBtn)
    def _():
        p = input.pInput()
        if p not in stored_probs.get():
            probs = list(stored_probs.get())
            probs.append(p)
            stored_probs.set(probs)
            
            # Calculate cumulative log likelihood for this p
            total_ll = 0
            batch_lls = []
            for success, trials in batch_data:
                ll = binom.logpmf(success, trials, p)
                total_ll += ll
                batch_lls.append(total_ll)
            
            lls = list(stored_lls.get())
            lls.append((p, batch_lls))
            stored_lls.set(lls)
    
    @output
    @render.plot
    def batchPlot():
        fig, ax = plt.subplots(figsize=(8, 6))
        
        for p, batch_lls in stored_lls.get():
            ax.plot(range(1, 5), batch_lls, 'o-', label=f'p={p:.3f}')
        
        ax.set_xlabel('Batch Number')
        ax.set_ylabel('Cumulative Log Probability')
        ax.set_title('Cumulative Log Probability by Batch')
        ax.grid(True)
        
        # Set x-axis ticks to integers
        ax.set_xticks(range(1, 5))
        
        if stored_lls.get():
            ax.legend()
        
        return fig
   
    @output
    @render.plot
    def probPlot():
        fig, ax = plt.subplots(figsize=(8, 6))
        
        if stored_lls.get():
            # Sort the data by probability before plotting
            sorted_data = sorted(stored_lls.get(), key=lambda x: x[0])
            probs = [x[0] for x in sorted_data]
            final_lls = [x[1][-1] for x in sorted_data]
            
            ax.plot(probs, final_lls, 'o-')
            ax.axvline(mle_p, color='red', linestyle='--', 
                      label=f'MLE (p={mle_p:.3f})')
        
        ax.set_xlabel('Probability (p)')
        ax.set_ylabel('Total Log Probability')
        ax.set_title('Total Log Probability vs. p')
        ax.grid(True)
        if stored_lls.get():
            ax.legend()
        
        return fig

app = App(app_ui, server)
```

#### A Subtly Good Strategy

Playing around with the app you should be able to see that the chart on the right, the 'Total Log Probability vs p.' has an obvious peak. What we want are parameter values that are the most probable, i.e. the ones with the largest total log probability. Here we can fairly easily pick random values, and maybe use some hints from the slope of the graph, to find a good estimate of p. That strategy to find the most probable model is actually not a bad one - a state of the art method is effectively a very sophisticated version of that same strategy.

#### Analytical Solution

For the Binomial distribution there's actually a simple analytical solution, and while it may seem simple in hindsight, the proper derivation is maybe more complex than you'd expect... The basic idea is that the derivative of the 'Total Log Probability vs p.' chart will equal zero at the maximum probability. After a brief drum roll we can reveal that the most likely estimate of p is ....... just the average success rate. More precisely, the successes divided by the total n, e.g. (992 + 995 + 989 + 998)/(1,000 + 1,000 + 1,000 + 1,000).

Although we mostly avoid them, analytical solutions can be very useful. For one they are typically very fast to compute. And secondly, they can be a nice verification that approximate/numerical methods converge to reasonable solutions, for at least some set of circumstances.

## Maximum Likelihood Estimate with Continuous Data

We move onto continuous data and continuous probability distributions, but with the same goal of finding the model parameters that maximize the likelihood of the observed data. The methods remain the same. The only difference we should keep in mind is that when we calculate $P(y_i \mid \theta)$, with a continuous probability distribution, it is a probability density, and not a true probability. However, since probability densities can still be used to compare relative probability, and we are only trying to find the model parameter values that are the most likely relative to all other possibilities, this nuance has little practical impact.

### Normal Distribution with MLE Estimate

The following is a very similar app to what we saw earlier that just calculated with likelihood of the data. However, this time we also have ability to find the most likely model to explain the data, i.e the Maximimum Likelihood Estimate *MLE* ...

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plots
    ui.row(
        ui.column(6, ui.output_plot("normalPlot", height="400px")),
        ui.column(6, ui.output_plot("cumLogProbPlot", height="400px"))
    ),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

    # Plot the cumulative log probability
    @output
    @render.plot
    def cumLogProbPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        # Sort data points for plotting
        sorted_indices = np.argsort(y)
        sorted_y = y[sorted_indices]
        
        # Calculate log probabilities
        log_probs = -0.5 * np.log(2 * math.pi * var) - 0.5 * ((sorted_y - mu) / sigma)**2
        cum_log_probs = np.cumsum(log_probs)

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(sorted_y, cum_log_probs, 'b-', marker='o')
        ax.set_title("Cumulative Log Probability")
        ax.set_xlabel("Data Points")
        ax.set_ylabel("Cumulative Log Probability")
        ax.grid(True)

        return fig

app = App(app_ui, server)
```


Older version:

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        # MLE for variance uses 1/n factor
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```




## Segway

The following is a nice segway to the machine learning section, but should be at the very end of the chapter.

Maximum Likelihood Estimation is not the only method to find optimal model parameters, although it is the standard for statistical models.

::: callout-note
## Loss Functions

Likelihood is a specific form of a **loss function**. Likelihood is rooted in probability, but a loss function does not need to be. Loss functions in machine learning are what likelihood functions are in statistics. In generic form they look like this:

$$
\mathcal{L}(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^{N} \ell(y_i, \hat{y}_i)
$$

1.  $\mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})$: This represents the overall loss function, which measures the difference between the true values $\mathbf{y}$ and the predicted values $\hat{\mathbf{y}}$. It aggregates the individual losses across all data points.

2.  $\ell(y_i, \hat{y}_i)$: This is the individual loss for a single data point $i$. It can take different forms depending on the type of problem. 

3.  $\frac{1}{N} \sum_{i=1}^{N}$: This is the averaging operation, which sums up the individual losses $\ell(y_i, \hat{y}_i)$ across all $N$ data points in the dataset and divides by $N$ to compute the average loss. This helps ensure that the loss function is independent of the dataset size.
:::

We introduce loss functions here but will apply them next chapter??

## Methods

Before we dive too deep into the details, we are going to take a moment to reflect on what we're attempting to do and common techniques to do it...

When we were just generating data, it was obvious what variables went into the model, and what data came out of the model. However, we should do a better job of defining that now. Part of the issue here is that many names are used for many applications. We will call inputs predictor/feature variables, and we will call the outputs the outcome/target variables...

## Note in 3.2 we will go into the likelihood calculation itself


## Traditional Linear Regression

Single-variable linear regression aims to model the relationship between a single independent variable \( x \) and a dependent variable \( y \) using a linear equation:

\[
y = mx + b
\]

where:
- \( m \) (slope) represents the rate of change of \( y \) with respect to \( x \),
- \( b \) (intercept) is the value of \( y \) when \( x = 0 \).

## Least Squares Method

The goal is to minimize the sum of squared residuals:

\[
J(m, b) = \sum_{i=1}^{n} (y_i - (mx_i + b))^2
\]

where \( (x_i, y_i) \) are the given data points.

### Solving for \( m \) and \( b \)

To find \( m \) and \( b \) analytically, we take the partial derivatives of \( J(m, b) \) and set them to zero:

#### Step 1: Compute the slope \( m \)

\[
m = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\]

where \( \bar{x} \) and \( \bar{y} \) are the means of \( x \) and \( y \):

\[
\bar{x} = \frac{1}{n} \sum x_i, \quad \bar{y} = \frac{1}{n} \sum y_i
\]

#### Step 2: Compute the intercept \( b \)

\[
b = \bar{y} - m \bar{x}
\]

### Final Model

Thus, the best-fitting line is:

\[
y = mx + b
\]

where \( m \) and \( b \) are computed using the formulas above.

This method provides an exact solution without requiring iterative optimization, making it efficient for small datasets.


### Notes

TODO: We should maintain approximately the same rate of callouts?