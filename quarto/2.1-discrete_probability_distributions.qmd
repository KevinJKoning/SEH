---
title: Discrete Probability Distributions
format: html
filters:
  - shinylive
---

## Preview

In the first half of the primer we focus on $P(D|M)$, the probability of the [data]{style="text-decoration: underline;"} given a [model]{style="text-decoration: underline;"} of a data generating process. This first chapter considers only processes with discrete (in contrast with continuous) outputs. We use dice as our first example as it is hopefully an intuitive subject. We then introduce *discrete probability distributions* as models of other idealized processes. For the first half of the primer we will not question our models, we will consider them as set/frozen and just allow them to generate data for us, assuming that they represent the data generating process of interest.

Once we have have some discussion of models under our belt, we will change our focus to the probability of data. For any model we can use the relative frequency of an event to approximate the events probability. We will show how the accuracy of this probability estimate increases with the number of samples. Where an analytical solution of $P(D|M)$ is available, we will also compare exact probabilities to those estimated from relative frequency.

We then shift to the probability of multiple events based on the laws of probability for independent events. We show how we can use computation to calculate the relative probability of a specific series of events by comparing it to a multitude of randomly generated series of events. We use the example of finding the probability that a die is weighted (unfair) after an observed series of rolls. This is our more intuitive approach to hypothesis testing.

## Models of Discrete Data Generating Processes

### A Dice Total Model

We'd prefer not to spend too much time on toy examples, however, there are a lot of benefits to starting with something that is intuitive and simple. Subsequently we'll use rolling dice as our first example of a data generating process. It is also convenient that the mathematical model we'll use is a good approximation of the real data generating process, so long as you're OK with ignoring all the physical bouncing of the dice and are content with just the result after a roll.

A dice model is built into the app displayed beneath this paragraph. It will simulate rolling the number of dice you specify, as if you threw them out of a cup all at once, and total the value on those dice from the cup, which is considered one roll. Additionally, it will repeat rolling that cup of dice the number of times you specify, and summarize the results on a histogram. Play around with the two inputs/parameters of the dice rolling app below, the number of dice and the number of rolls.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Dice Rolling App"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("numDice", "Number of Dice", min=1, max=10, value=2, step=1),
            ui.input_slider("numRolls", "Number of Rolls", min=1, max=10000, value=100, step=1),
        ),
        ui.output_plot("dicePlot", height="400px"),
    ),
)

def server(input, output, session):
    # Define a reactive calculation that depends on numDice and numRolls
    @reactive.Calc
    def dice_sums():
        return [
            np.random.randint(1, 7, input.numDice()).sum()
            for _ in range(input.numRolls())
        ]

    @output
    @render.plot
    def dicePlot():
        current_sums = dice_sums()
        fig, ax1 = plt.subplots()

        # Calculate frequencies and relative frequencies
        unique_sums, counts = np.unique(current_sums, return_counts=True)
        relative_freq = counts / len(current_sums)

        # Create the frequency bars with darker blue
        bars = ax1.bar([str(s) for s in unique_sums], counts, color="steelblue")  
        ax1.set_xlabel("Dice Total")
        ax1.set_ylabel("Frequency", color="steelblue")
        ax1.tick_params(axis='y', labelcolor="steelblue")

        # Create secondary y-axis for relative frequency
        ax2 = ax1.twinx()
        markers = ax2.plot([str(s) for s in unique_sums], relative_freq, 
                          color="r", marker='_', linestyle='None', label="Relative Frequency")
        ax2.set_ylabel("Relative Frequency", color="red")
        ax2.tick_params(axis='y', labelcolor="red")
        
        # Set y-axis limits to start at 0 for relative frequency
        ax2.set_ylim(bottom=0)

        # Set title
        plt.title("Frequency and Relative Frequency of Dice Totals", fontsize=14)
        
        # Add legend
        lines = [bars.patches[0], markers[0]]
        labels = ["Frequency", "Relative Frequency"]
        ax1.legend(lines, labels, loc='upper left')

        # Adjust layout to prevent label cutoff
        plt.tight_layout()
        
        # Rotate x-axis labels after tight_layout
        ax1.tick_params(axis='x', rotation=90)

        return fig

app = App(app_ui, server)
```

Hopefully you've noted how a larger number of rolls seems to give us smoother and more consistent results. We will revisit this point more precisely in a later section.

### Discrete Probability Distributions as Models of Data Generating Processes

In the last section, we used dice rolling as our data generating process, however there are other discrete processes we may be interested in, such as testing 1,000 products that have a 0.999 probability of success each. As you can imagine, when you change the data generating process, the relative frequency (distribution) of the outcomes change. There are a number of important discrete processes that can be described with [mostly] analytical mathematical models, and subsequently as statisticians have discovered/created them, they have **named** these special *discrete probability distributions*.

::: callout-note
If you are familiar with discrete probability distributions, you may have expected them to be introduced as a way to find the exact probability of an event, instead of using them to generate data. We will eventually use this feature, but our goal is to more generally introduce models of data generating processes and their associated probability distributions. Only a small number have properties that allow for nice analytical solutions to the probabilities of their data, and we'd prefer not to constrain ourselves to thinking in only an analytical (as opposed to computational) framework.
:::

Initially we want to think about discrete probability distributions in the same way we thought about our dice model, that it is a model that will generate random events from a data generating process of interest. Below are a couple examples to illustrate the point.

#### Binomial Distribution

The binomial distribution is a model that represents the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (commonly referred to as “success” and “failure”). The probability of success can range between 0 and 1. In the app below you can recreate something like the product testing scenario recently mentioned. Feel free to play around with the settings/parameters to get a feel for how the probability distribution behaves.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import math

app_ui = ui.page_fluid(
    ui.h2("Binomial Distribution Simulation with Binned Histogram"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "numTrials", 
                "Number of Trials (n)", 
                min=1, 
                max=10000, 
                value=100, 
                step=1
            ),
            ui.input_slider(
                "probSuccess", 
                "Probability of Success (p)", 
                min=0.001, 
                max=0.999, 
                value=0.5, 
                step=0.001
            ),
        ),
        ui.output_plot("binomPlot", height="400px"),
    ),
)

def server(input, output, session):
    FIXED_NUM_SIMULATIONS = 10000

    @reactive.Calc
    def binomial_samples():
        n = input.numTrials()
        p = input.probSuccess()
        size = FIXED_NUM_SIMULATIONS
        return np.random.binomial(n, p, size)
    
    def determine_bin_width(data, max_bins=30):
        data_min = data.min()
        data_max = data.max()
        data_range = data_max - data_min + 1
        for bin_width in range(1, data_range + 1):
            num_bins = math.ceil(data_range / bin_width)
            if num_bins <= max_bins:
                return bin_width
        return 1
    
    @output
    @render.plot
    def binomPlot():
        samples = binomial_samples()
        bin_width = determine_bin_width(samples, max_bins=30)
        
        data_min = samples.min()
        data_max = samples.max()
        bins = np.arange(data_min, data_max + bin_width, bin_width)
        
        counts, bin_edges = np.histogram(samples, bins=bins)
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        # Calculate relative frequencies
        relative_freq = counts / len(samples)
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Plot absolute frequencies
        bars = ax1.bar(bin_centers, counts, width=bin_width*0.9, 
                      color="steelblue", alpha=0.6, edgecolor="black", align='center')
        
        ax1.set_xlabel("Number of Successes")
        ax1.set_ylabel("Frequency", color="steelblue")
        ax1.tick_params(axis='y', labelcolor="steelblue")
        
        # Create secondary y-axis for relative frequency
        ax2 = ax1.twinx()
        markers = ax2.plot(bin_centers, relative_freq, 
                          color="red", marker='_', linestyle='None', 
                          markersize=10, markeredgewidth=2, label="Relative Frequency")
        ax2.set_ylabel("Relative Frequency", color="red")
        ax2.tick_params(axis='y', labelcolor="red")
        ax2.set_ylim(bottom=0)
        
        # Set title
        plt.title("Frequency and Relative Frequency of Successes", fontsize=14)
        
        # Add legend
        lines = [bars.patches[0], markers[0]]
        labels = ["Frequency", "Relative Frequency"]
        ax1.legend(lines, labels, loc='upper left')
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([int(x) for x in bin_centers], rotation=90)
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```


#### Poisson Distribution

The Poisson distribution is a model that describes the number of events occurring in a fixed interval of time or space, assuming that the events occur independently and at a constant average rate. For example, it might represent the number of phone calls received by a call center in an hour or the number of cars passing through a toll booth in a minute. The Poisson Distribution is similar to the Binomial distribution, except there are not a fixed number of trials, so there is not an upper limit to the number of events returned by a sample. However, values much larger than the average rate become incredibly unlikely.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
import math

app_ui = ui.page_fluid(
    ui.h2("Poisson Distribution Simulation"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "rate_param", 
                "Rate Parameter (λ)", 
                min=0.01, 
                max=10.0, 
                value=1.0, 
                step=0.01
            ),
            ui.input_slider(
                "num_trials",
                "Times to Repeat Sample",
                min=100,
                max=10000,
                value=10000,
                step=100
            )
        ),
        ui.output_plot("poissonPlot", height="400px"),
    )
)

def server(input, output, session):
    @reactive.Calc
    def num_simulations():
        return input.num_trials()

    @reactive.Calc
    def poisson_samples():
        lam = input.rate_param()
        n_trials = num_simulations()
        return np.random.poisson(lam, n_trials)
    
    def determine_bin_width(data, max_bins=30):
        data_min = data.min()
        data_max = data.max()
        data_range = data_max - data_min + 1
        for bin_width in range(1, data_range + 1):
            num_bins = math.ceil(data_range / bin_width)
            if num_bins <= max_bins:
                return bin_width
        return 1

    @output
    @render.plot
    def poissonPlot():
        samples = poisson_samples()
        lam = input.rate_param()
        
        bin_width = determine_bin_width(samples, max_bins=30)
        
        data_min = samples.min()
        data_max = samples.max()
        bins = np.arange(data_min, data_max + bin_width, bin_width)
        
        counts, bin_edges = np.histogram(samples, bins=bins)
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        # Calculate relative frequencies
        relative_freq = counts / len(samples)
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Plot absolute frequencies
        bars = ax1.bar(bin_centers, counts, width=bin_width*0.9, 
                      color="steelblue", alpha=0.6, edgecolor="black", align='center')
        
        ax1.set_xlabel("Number of Events")
        ax1.set_ylabel("Frequency", color="steelblue")
        ax1.tick_params(axis='y', labelcolor="steelblue")
        
        # Create secondary y-axis for relative frequency
        ax2 = ax1.twinx()
        markers = ax2.plot(bin_centers, relative_freq, 
                          color="red", marker='_', linestyle='None', 
                          markersize=10, markeredgewidth=2, label="Relative Frequency")
        ax2.set_ylabel("Relative Frequency", color="red")
        ax2.tick_params(axis='y', labelcolor="red")
        ax2.set_ylim(bottom=0)
        
        # Set title
        plt.title(f"Frequency and Relative Frequency of Poisson Events", 
                 fontsize=14)
        
        # Add legend
        lines = [bars.patches[0], markers[0]]
        labels = ["Frequency", "Relative Frequency"]
        ax1.legend(lines, labels, loc='upper right')
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([int(x) for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([int(x) for x in bin_centers], rotation=90)
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```


#### Summary

We keep this section brief as there are plenty of easily accessible references for discrete probability distributions. Hopefully the point was made though - that each discrete probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcomes of a process.

## Probability of Data

Having given some background on models, we can now focus on the probability of the data given the model, $P(D|M)$.

### Dice Totals Probability of Data

#### Relative Frequency

We want to find the probability of a particular dice total (the data) given a dice model. To estimate the probability $P(E)$ of an event $E$, we can use the relative frequency approach. This involves counting the number of occurrences of the event E and dividing it by the total number of trials. For example, if we observed a total of twelve occur in 40 out of 5,000 dice rolls, the probability estimate is:

$$
P(E) \approx \frac{\text{Number of times event } E \text{ occurs}}{\text{Total number of trials}} = \frac{40}{5000} = 0.008
$$

#### Law of Large Numbers

The accuracy of this estimate depends on the total number of trials as governed by the Law of Large Numbers. The standard error, which gives a measure of uncertainty in the estimate of a mean value, is proportional to one over the square root of the number of samples:

$$
P_{\text{error}} \propto \frac{1}{\sqrt{N_{\text{total}}}}
$$

Which indicates there are diminishing returns to just making the sample size larger. Now I know you're smart, and you're saying to yourself, I can figure out the *exact* probability of rolling a certain dice total. Of course you can for this example - but you probably can't for more realistic examples, and we want to learn techniques that work well for real problems. In general, if you are concerned with the quality of an estimate with this approach, just rerun the model and see if the outcome changes meaningfully - if it does, increase the number of times we run the model until the output is stable enough for your application. If that's still not enough, dig into exact/analytic methods.

#### Revised Dice App

Here's another version of the Dice Total App that you saw earlier - except it now has additional functionality to calculate the approximate and exact probability of a certain dice total based on your inputs. It also shows the relative frequency of the dice sums, i.e. a true probability distribution where the sum of the probabilities equals one.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 560

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

# --- Utility function to compute exact distribution of sums for n dice ---
def dice_sum_distribution(n_dice):
    """
    Return a list 'dist' where dist[s] = probability of sum s for n_dice dice.
    Indices go from 0 up to 6*n_dice. Only sums in range [n_dice..6*n_dice]
    have nonzero probabilities.
    """
    # ways[s] = number of ways to get sum s
    ways = [0] * (6*n_dice + 1)
    ways[0] = 1  # base case

    for _ in range(n_dice):
        new_ways = [0] * (6*n_dice + 1)
        for sum_val, count in enumerate(ways):
            if count > 0:
                for face in range(1, 7):
                    new_ways[sum_val + face] += count
        ways = new_ways

    total_outcomes = 6 ** n_dice
    dist = [count / total_outcomes for count in ways]
    return dist

# -------------------------- UI Definition ---------------------------
app_ui = ui.page_fluid(
    ui.h2("Dice Rolling App with Probability Mass Function"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("numDice", "Number of Dice", min=1, max=10, value=2, step=1),
            ui.input_slider("numRolls", "Number of Rolls", min=1, max=10000, value=100, step=1),
            ui.input_select(
                "selectedTotal", 
                "Select Dice Total", 
                choices=[""],   # initially empty, will be updated dynamically
                multiple=False,
            ),
            ui.output_text("approxProbability"),
            ui.output_text("exactProbability")
        ),
        ui.output_plot("dicePlot", height="400px"),
    )
)

# -------------------------- Server Definition -------------------------
def server(input, output, session):
    # Reactive: Generate random sums based on numDice and numRolls
    @reactive.Calc
    def dice_sums():
        return [
            np.random.randint(1, 7, input.numDice()).sum()
            for _ in range(input.numRolls())
        ]

    # Reactive: Exact distribution of sums for the current number of dice
    @reactive.Calc
    def exact_distribution():
        return dice_sum_distribution(input.numDice())

    # Dynamically update the choices in the 'selectedTotal' select input
    @reactive.Effect
    def _():
        current_sums = dice_sums()
        unique_sums = sorted(np.unique(current_sums))
        ui.update_select(
            "selectedTotal",
            choices=[str(s) for s in unique_sums],
            selected=str(unique_sums[0]) if len(unique_sums) > 0 else ""
        )

    # Plot the relative frequency of dice totals
    @output
    @render.plot
    def dicePlot():
        current_sums = dice_sums()
        
        fig, ax = plt.subplots()
        
        # Get theoretical distribution first to set x-axis limits
        dist = exact_distribution()
        theoretical_sums = range(input.numDice(), 6*input.numDice()+1)
        theoretical_probs = [dist[i] for i in theoretical_sums]
        
        # Create dictionary to store empirical probabilities for all possible sums
        empirical_dict = {i: 0 for i in theoretical_sums}
        unique_sums, counts = np.unique(current_sums, return_counts=True)
        for sum_val, count in zip(unique_sums, counts):
            if sum_val in empirical_dict:
                empirical_dict[sum_val] = count / len(current_sums)
        
        # Plot empirical distribution
        ax.bar([str(s) for s in theoretical_sums], 
               [empirical_dict[s] for s in theoretical_sums],
               color="steelblue", label="Empirical")
        
        # Plot theoretical distribution
        ax.plot([str(s) for s in theoretical_sums], theoretical_probs, 
                'r', marker='_', linestyle='', label="Theoretical")

        ax.set_title("Probability Mass Function of Dice Totals")
        ax.set_xlabel("Dice Total")
        ax.set_ylabel("Probability")
        ax.legend()
        plt.xticks(rotation=90)

        return fig

    # Approximate probability of the selected dice total
    @output
    @render.text
    def approxProbability():
        if not input.selectedTotal():
            return "Select a dice total to see probabilities."

        current_sums = dice_sums()
        selected_total = int(input.selectedTotal())

        count = sum(1 for x in current_sums if x == selected_total)
        if len(current_sums) == 0:
            prob = 0
        else:
            prob = count / len(current_sums)

        return f"Approx. Probability of {selected_total}: {prob:.4f}"

    # Exact probability of the selected dice total
    @output
    @render.text
    def exactProbability():
        if not input.selectedTotal():
            return ""

        selected_total = int(input.selectedTotal())
        dist = exact_distribution()

        # If the selected total is out of range, probability is 0
        if selected_total < 0 or selected_total >= len(dist):
            prob = 0
        else:
            prob = dist[selected_total]

        return f"Exact Probability of {selected_total}: {prob:.4f}"

app = App(app_ui, server)
```

Hopefully you can demonstrate to yourself that with enough samples, the approximate probability calculation is awfully close to the exact probability. However, there is an exception. The tails (the slim far ends) are not as accurate. Properly calculating probability in these tail sections happens to be trivial for dice where an exact solution is available, but for a real problem, accurate tail probabilities are incredibly difficult.

::: callout-note
A few readers may already know an essay called the 'The Bitter Lesson' by Rich Sutton. If you haven't read it, it is worth a read, and easily found on the internet. I believe it has a strong engineering corollary, in that our education is too obsessed with analytical solutions instead of computational ones. To temper that statement slightly, I can also assure you that solving problems through raw computation has its limits as there's no way to compute through a bad algorithm. But we're professionals in a hurry, we choose computation whenever it's plausible.
:::

### Exact Probabilities from a Discrete Probability Distribution

Here we provide an example of the more 'typical' use of a named discrete probability distribution, to compute the probability of an event. Let's use the properties of the binomial probability distribution to determine the probability of having less than 999 products pass a quality test when we have a true 0.999 pass rate and we inspect 1,000 products.

We'll solve this problem by computing the probability of having 999 or 1,000 products pass, and then just inverting the probability. Note that $P(X = 1000)$ is simplified below based on the *multiplication rule of independent events* to equal 0.999\^1,000. Also, if you're unfamiliar with the n over k in parenthesis, look up 'n choose k'.

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
$$

$$
P(X = 999) + P(X = 1,000) = \binom{1000}{999} (0.999)^{999} (0.001)^1 + (0.999)^{1,000}
$$

The result is 0.736, so the chance of having a real pass rate of 0.999 and having less than 999 pass is 1 - 0.736 = 0.264. It's fairly plausible to have 0.999 pass rate and have a couple of products fail. However, if you had four or more fail, you should have a very hard time convincing management that it was just 'bad luck'. The more formal way to discuss this is *hypothesis testing*.

::: callout-note
Much of statistics is dedicated to hypothesis testing. There's no reason for this primer to repeat such a repeated topic. The problem though, is that in the real world, folks get it wrong. I have a simple hypothesis for this, and that's because non-expert practitioners do not understand their statistical models. And there's good reasons they don't, they are often analytical mathematical 'magic' with buried assumptions. We take a different approach.
:::

### Probability of Multiple Events

So far we have only considered $P(D|M)$ where the data is a single event. However, it is more common to have multiple events (i.e. multiple data points), and we'd like to know the probability of a specific dataset vs other datasets we may have sampled. To calculate probabilities we'll use the *multiplication rule of independent events*. In practice it may be very difficult to prove perfectly independent events - however be sure there is not a reason for strong correlation, such as almost all *time series*.

#### Numerical Stability

We should also mention for many small probabilities, multiplication risks numerical instability. However there is one very simple and clever workaround to this, which is to add log probabilities instead:

$$
P(A \cap B \cap C \cap \dots) = P(A) \cdot P(B) \cdot P(C) \cdot \dots = 10^{(\log_{10}(P(A)) + \log_{10}(P(B)) + \log_{10}(P(C)) + \dots))}
$$

#### Example

Let's work through a couple example calculations before extending it to a scenario that approximates more serious problems we'll tackle later. What is the probability of rolling three dice out of a cup, where the total is 11, and then rolling the dice again, where the total is 7? We use the app above to get the probabilities of each roll, where 11 = 0.1250 and 7 = 0.0694.

$$
P(Roll 1 \cap Roll 2) = 0.1250 \cdot 0.0694 = 0.008675 = 10^{(\log_{10}(0.1250) + \log_{10}(0.0694))}
$$

If we stay in log probabilities, as that is generally more convenient:

$$
\log_{10}(P(Roll 1 \cap Roll 2)) = \log_{10}(0.1250) + \log_{10}(0.0694) = -0.9031 + -1.1586 = -2.0617
$$

### Calculating the Relative Probability of Multiple Events

With the background from the last section we can now simulate the probability of multiple events. However, often it is more useful to think about the relative probability of a long series of events rather than the absolute probability, since the absolute probability will be tiny. I think an example will help. Again, to have better intuition about the problem, we'll use dice, although it can obviously be extended to other discrete probability distributions.

We'll extend the recent example of rolling three dice out of a cup. We'll let most of our simulations use fair dice, and we'll show the log probability of each exact series of events as we perform rolls out of the cup. Note that for three dice, the values can range from 3 to 18, and values near the middle, such as 10, will be more common than those at the end, such as 3 or 18. The log probability of more rare events will be more negative. If we have a series of very unlikely events, the values will become negative more quickly.

We're also going to include one series of rolls with a cup of three weighted dice, which will be the dotted blue line. The values and probabilities will be 1=0.10, 2=0.10, 3=0.15, 4=0.15, 5=0.2, 6=0.3. If we expect the outcomes of fair dice, we'll see that the weighted dice will usually trend towards being a fairly improbable series of rolls. We can refer to the percentile of their log-probability to see how unusual they are.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 650

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

# --- Utility Functions ---

def roll_three_dice_fair():
    """Simulate rolling three fair dice and return their sum."""
    return np.sum(np.random.randint(1, 7, size=3))

def roll_three_dice_weighted():
    """Simulate rolling three weighted dice and return their sum."""
    weights = [0.10, 0.10, 0.15, 0.15, 0.2, 0.30]
    # Define possible outcomes
    outcomes = [1, 2, 3, 4, 5, 6]
    # Roll three weighted dice
    return np.sum(np.random.choice(outcomes, size=3, p=weights))

def get_probability_of_sum(dice_sum):
    """Calculate probability of getting a specific sum with three fair dice."""
    possibilities = 0
    total_outcomes = 216  # 6^3 possible outcomes
    
    for i in range(1, 7):
        for j in range(1, 7):
            for k in range(1, 7):
                if i + j + k == dice_sum:
                    possibilities += 1
                        
    return possibilities / total_outcomes

def get_probability_of_sum_weighted(dice_sum):
    # We want to pretend it has the same probability and see how much of an outlier weighted dice are
    return get_probability_of_sum(dice_sum)

def simulate_rolls_fair(num_rolls=10):
    """Simulate a series of rolls with fair dice and calculate cumulative log probability."""
    rolls = [roll_three_dice_fair() for _ in range(num_rolls)]
    probabilities = [get_probability_of_sum(roll) for roll in rolls]
    
    # To handle log(0), replace zero probabilities with a very small number
    probabilities = [p if p > 0 else 1e-10 for p in probabilities]
    log_probs = np.log10(probabilities)
    cumulative_log_probs = np.cumsum(log_probs)
    
    return rolls, cumulative_log_probs

def simulate_rolls_weighted(num_rolls=10):
    """Simulate a series of rolls with weighted dice and calculate cumulative log probability."""
    rolls = [roll_three_dice_weighted() for _ in range(num_rolls)]
    probabilities = [get_probability_of_sum_weighted(roll) for roll in rolls]
    
    # To handle log(0), replace zero probabilities with a very small number
    probabilities = [p if p > 0 else 1e-10 for p in probabilities]
    log_probs = np.log10(probabilities)
    cumulative_log_probs = np.cumsum(log_probs)
    
    return rolls, cumulative_log_probs

# -------------------------- UI Definition ---------------------------
app_ui = ui.page_fluid(
    ui.h2("Three-Dice Roll Simulations with Cumulative Log Probability"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_select(
                "selectedSim", 
                "Select Simulation to Highlight", 
                choices=[],   # initially empty, will be updated dynamically
                multiple=False,
            ),
            ui.output_text("selectedDetails"),
            ui.hr(),
            # Removed the Number of Simulations slider
            # Set Number of Simulations to 100 for fair and 5 for weighted
            # Retain Number of Rolls slider
            ui.input_slider("numRolls", "Number of Rolls per Simulation", min=5, max=100, value=10, step=1),
            ui.input_action_button("runSim", "Run Simulations")
        ),
        ui.output_plot("probPlot", height="500px"),
    )
)

# -------------------------- Server Definition -------------------------
def server(input, output, session):
    # Reactive value to store simulations as a list of tuples: (label, cum_log_probs)
    simulations = reactive.Value([])
    
    # Reactive: Perform simulations when 'Run Simulations' button is clicked
    @reactive.Effect
    def _run_simulations():
        input.runSim()  # Depend on the runSim button
        num_rolls = input.numRolls()
        new_simulations = []
        
        # Run 99 fair simulations (99 + 1 unfair = 100)
        num_sim_fair = 99
        for i in range(num_sim_fair):
            _, cum_probs = simulate_rolls_fair(num_rolls)
            label = f"Simulation {i+1}"
            new_simulations.append((label, cum_probs))
        
        # Run 1 weighted simulations (was 5)
        num_sim_weighted = 1
        for i in range(num_sim_weighted):
            _, cum_probs = simulate_rolls_weighted(num_rolls)
            label = f"Weighted Simulation {i+1}"
            new_simulations.append((label, cum_probs))
        
        simulations.set(new_simulations)
        
        # Update the select input choices
        sim_choices = [sim[0] for sim in new_simulations]
        ui.update_select(
            "selectedSim",
            choices=sim_choices,
            selected=sim_choices[0] if sim_choices else ""
        )
    
    # Initialize simulations on app start
    @reactive.Effect
    def _initialize():
        input.runSim()  # Trigger initial simulation run
    
    # Plot the simulations, highlighting the selected one
    @output
    @render.plot
    def probPlot():
        sims = simulations()
        num_sim = len(sims)
        if num_sim == 0:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "No simulations to display.\nClick 'Run Simulations' to start.", 
                    horizontalalignment='center', verticalalignment='center', fontsize=12)
            ax.axis('off')
            return fig
        
        num_rolls = len(sims[0][1])
        x = np.arange(1, num_rolls + 1)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot all fair simulations in light gray
        for label, probs in sims:
            if label.startswith("Simulation "):
                ax.plot(x, probs, color='#D3D3D3', alpha=0.5)
        
        # Plot all weighted simulations in blue
        for label, probs in sims:
            if label.startswith("Weighted Simulation "):
                ax.plot(x, probs, color='blue', alpha=0.7, linestyle='--')
        
        # Highlight the selected simulation
        selected = input.selectedSim()
        if selected:
            try:
                # Find the simulation by label
                selected_sim = next((sim for sim in sims if sim[0] == selected), None)
                if selected_sim:
                    label, selected_probs = selected_sim
                    color = 'red' if label.startswith("Simulation ") else 'green'
                    linestyle = '-' if label.startswith("Simulation ") else '--'
                    ax.plot(x, selected_probs, color=color, linewidth=2.5, linestyle=linestyle, label=label)
            except (IndexError, ValueError):
                pass
        
        ax.set_xlabel('Number of Rolls')
        ax.set_ylabel('Cumulative Log10 Probability')
        ax.set_title('Cumulative Log Probability of Multiple Three-Dice Roll Simulations')
        ax.grid(True)
        if selected:
            ax.legend()
        
        plt.tight_layout()
        return fig
    
    # Display details of the selected simulation, including percentile
    @output
    @render.text
    def selectedDetails():
        sims = simulations()
        selected = input.selectedSim()
        if not selected:
            return "No simulation selected."
        try:
            # Find the simulation by label
            selected_sim = next((sim for sim in sims if sim[0] == selected), None)
            if not selected_sim:
                return "Selected simulation not found."
            label, cum_probs = selected_sim
            # Get the final cumulative log probability
            final_log_prob = cum_probs[-1]
            # Collect all final cumulative log probabilities
            all_final_log_probs = [sim[1][-1] for sim in sims]
            # Compute percentile
            percentile = (np.sum(np.array(all_final_log_probs) <= final_log_prob) / len(all_final_log_probs)) * 100
            return (
                f"{label}\n\n"
                f"Final Cumulative Log10 Probability: {final_log_prob:.4f}\n"
                f"Percentile: {percentile:.2f}th"
            )
        except (IndexError, ValueError):
            return "Invalid selection."

app = App(app_ui, server)
```

You may notice that it's easier to spot the weighted dice with a longer series of rolls. In just a couple of rolls they often don't seem that unusual, but most of the time over 100 roles they have a markedly different slope than the other roles. When we calculate the percentile of the weighted dice, we are approximating the p-value of a hypothesis test. The p-value essentially says, if you assume your model is correct, here is the probability of witnessing data as, or more, extreme than what you witnessed. If the p-value becomes small enough, you may suspect it's not bad luck, but that the data is being generated by a different model.

::: callout-note
Traditional hypothesis testing starts with a *null* model which we can think of as the status quo. If the data has less than a (traditionally) 0.05 probability of being generated by that *null* model, we assume that something other than the *null* model generated the data.
:::

## Mini Appendix on Probability Distributions

John K. Kruschke said it well:

> A probability distribution is simply a list of all possible outcomes and their corresponding probabilities.

We think probability distributions in statistics is a confused topic - not confusing - confused. The main concern is that a special case (the *named* probability distributions) is made so prominent a newcomer is easily confused it is the *only* case. Here let's build up some intuition for discrete probability distributions, in the general sense, with appropriate reference to the special case of the *named* probability distributions.

### Joint Discrete Probability Distributions

The first thing we normally note in a data generating process is what we consider to be the outcome, or in common mathematical parlance, the $y$ variable. For example, if we are asking whether a car breaks down over the course of a year, we want P(Breakdown).

| Breakdown | Probability |
|-----------|-------------|
| Yes       | 0.30        |
| No        | 0.70        |

This is an *empirical* discrete probability distribution as it is based on observations (or in this case made up observations). We could also track whether a car, over the course of a year, has experienced extreme heat, which we could call P(Extreme_Heat).

| Extreme_Heat | Probability |
|--------------|-------------|
| Yes          | 0.40        |
| No           | 0.60        |

Now, we could be interested in the combination of the two things, which would be the combination of all possible P(Breakdown) and P(Extreme_Heat) outcomes, which we'd call P(Breakdown, Extreme_Heat). Here's a possible version of that joint probability distribution:

| Breakdown | Extreme_Heat | Probability |
|-----------|--------------|-------------|
| Yes       | Yes          | 0.10        |
| Yes       | No           | 0.20        |
| No        | Yes          | 0.30        |
| No        | No           | 0.40        |

Now it's worth pointing out that while many of us may be interested in the breakdown outcome, one could also be interested in the extreme heat outcome - what we consider the $y$ can be a matter of perspective. Now it's obvious that more variables could be part of this joint probability distribution, although their usefulness ultimately depends on how correlated they are to the outcome of interest. Lets assume we want to predict breakdowns, so we add another variable that is probably useful. Now we have P(Brand, Breakdown, Extreme_Heat).

| Brand  | Breakdown | Extreme Heat | Probability |
|--------|-----------|--------------|-------------|
| BrandA | Yes       | Yes          | 0.05        |
| BrandA | Yes       | No           | 0.10        |
| BrandA | No        | Yes          | 0.15        |
| BrandA | No        | No           | 0.20        |
| BrandB | Yes       | Yes          | 0.10        |
| BrandB | Yes       | No           | 0.10        |
| BrandB | No        | Yes          | 0.10        |
| BrandB | No        | No           | 0.20        |

We could imagine all sorts of variables that may impact Breakdown. Please also note, that regardless of how many variables we have, the probability column always sums to 1. Also note that we chose only two possibilities for each column (e.g. BrandA, BrandB), but there could have been many more.

### Conditional Discrete Probability Distributions

A conditional probability distribution is one in which we are given the state of one or more variables. For example, let's examine P(Breakdown, Extreme_Heat \| Brand = BrandB), which in words is the joint probability distribution of Breakdown and Extreme_Heat given that Brand = BrandB.

| Brand  | Breakdown | Extreme Heat | Probability |
|--------|-----------|--------------|-------------|
| BrandB | Yes       | Yes          | 0.20        |
| BrandB | Yes       | No           | 0.20        |
| BrandB | No        | Yes          | 0.20        |
| BrandB | No        | No           | 0.40        |

Because BrandB is a constant, we could just write it this way:

| Breakdown | Extreme Heat | Probability |
|-----------|--------------|-------------|
| Yes       | Yes          | 0.20        |
| Yes       | No           | 0.20        |
| No        | Yes          | 0.20        |
| No        | No           | 0.40        |

Note how this is not the same probability distribution as P(Breakdown, Extreme_Heat).

When we work with a *named* probability distribution, like the binomial, we give the value of the distributions parameters so they can be plugged into the mathematical model of the data generating process - which is what distinguishes it from an empirical distribution. Here is a discrete probability distribution for a Binomial process with n=3 and p=0.2, i.e. P( k \| n=3, p=0.2).

| Successes (k) | n   | k   | Probability |
|---------------|-----|-----|-------------|
| 0             | 3   | 0.2 | 0.512       |
| 1             | 3   | 0.2 | 0.384       |
| 2             | 3   | 0.2 | 0.096       |
| 3             | 3   | 0.2 | 0.008       |

Because n and k are constant, we may prefer a table without them, although they are still there, implicitly.

| Successes (k) | Probability |
|---------------|-------------|
| 0             | 0.512       |
| 1             | 0.384       |
| 2             | 0.096       |
| 3             | 0.008       |

We can't examine the joint probability distribution of the Binomial distribution without giving a value for n and p because the number of possibilities for n and p are infinite, and subsequently the joint probability distribution is infinite. Note we can effectively do the same thing with the 'Breakdown' joint probability distribution if we just changed extreme_heat to temperature, and tried to observe breakdowns for every temperature. The *named* probability distributions are special in that we have a complete mathematical description of the process, so we can simply calculate the results of any combination, while in the empirical distribution of Breakdown, we would have to observe the results.

It is also possible to have a probability distribution from one of the *named* probability distributions in which we have a set of values for the parameters, instead of one value. For example, we can have a Binomial distribution using the set notation $\in$ which states that $n$ must be either 3 or 4 and $p$ must be either 0.2 or 0.3.

$$
P(k, \quad n \in \{3, 4\}, \quad p \in \{0.2, 0.3\})
$$

We should also explicity define how each combination in the set contributes to the total probability, but for this simple example we will assume they contribute equally. This is like the 'Breakdown' table which was limited to the following set of data:

$$
P(Breakdown, \quad ExtremeHeat \in \{Yes, No\}, \quad Brand \in \{BrandA, BrandB\})
$$

At this point hopefully the empirical and named probability distributions 'feel' the same, except we know the mathematical models behind the named probability distributions, so we do not have to guess their distributions through observation.

### Marginal Discrete Probability Distributions

A marginal probability distribution is essentially a way to work from the end of the section *Joint Discrete Probability Distributions* backwards to the beginning - which is a way to eliminate variables from the joint probability distribution and make them simpler. Let's say we want to eliminate the Extreme_Heat variable:

| Breakdown | Extreme_Heat | Probability |
|-----------|--------------|-------------|
| Yes       | Yes          | 0.10        |
| Yes       | No           | 0.20        |
| No        | Yes          | 0.30        |
| No        | No           | 0.40        |

We simply add up all the possible values of Extreme_Heat within a given Breakdown, e.g. for Breakdown = Yes, 0.10 + 0.20, and for Breakdown = No, 0.30+ 0.40. Then we arrive at the original table:

| Breakdown | Probability |
|-----------|-------------|
| Yes       | 0.30        |
| No        | 0.70        |

The name 'marginal' comes from the fact that, in the times of paper tables, a statistician could do this addition and write the result in the margin.


## Dashboard App

TESTING OF DASHBOARD APP:

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import pandas as pd

app_ui = ui.page_fluid(
    ui.h2("Simulated Breakdown Data"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "numEvents",
                "Number of Simulated Outcomes",
                min=1,
                max=100000,
                value=10000,
                step=1000,
            ),
            ui.input_checkbox_group(
                "covariates",
                "Select Covariates to Group By",
                choices=["Mileage", "Brand"],
                inline=True,
                selected=["Mileage", "Brand"],
            ),
            ui.input_select(
                "filter_covariate",
                "Conditional Filter: Choose Covariate",
                choices=["None", "Mileage", "Brand"],
                selected="None",
            ),
            # Dynamic UI element for filter value input:
            ui.output_ui("filter_value_ui"),
        ),
        ui.output_table("results_table")
    ),
)

def server(input, output, session):
    # Set the seed for reproducibility.
    np.random.seed(42)
    
    @reactive.Calc
    def simulated_data():
        n = input.numEvents()
        miles_driven = np.random.lognormal(mean=np.log(10000), sigma=0.5, size=n)
        brands = np.random.choice(
            ["Brand_A", "Brand_B", "Brand_C"],
            size=n,
            p=[0.4, 0.4, 0.2],
        )
        beta_miles = 0.0000000001
        brand_effects = {"Brand_A": 0.001, "Brand_B": 0.0005, "Brand_C": -0.0002}
        lambda_vals = np.exp(beta_miles * miles_driven + np.vectorize(brand_effects.get)(brands))
        breakdowns = np.random.poisson(lambda_vals)
        data = pd.DataFrame({
            "Miles_Driven": miles_driven,
            "Brand": brands,
            "Breakdowns": breakdowns,
        })
        data["Miles_Binned"] = (data["Miles_Driven"] // 10000) * 10000
        return data

    @reactive.Calc
    def grouped_table():
        data = simulated_data()
        covariates = input.covariates()
        group_cols = []
        if "Mileage" in covariates:
            group_cols.append("Miles_Binned")
        if "Brand" in covariates:
            group_cols.append("Brand")
        group_cols.append("Breakdowns")
        
        grouped = data.groupby(group_cols).size().reset_index(name="Frequency")
        
        filter_cov = input.filter_covariate()
        if filter_cov != "None":
            if filter_cov == "Mileage":
                filter_val = input.filter_value()
                if filter_val is not None:
                    grouped = grouped[grouped["Miles_Binned"] == float(filter_val)]
            elif filter_cov == "Brand":
                filter_val = input.filter_value()
                if filter_val is not None:
                    grouped = grouped[grouped["Brand"] == filter_val]
        
        total_freq = grouped["Frequency"].sum()
        if total_freq > 0:
            grouped["Probability"] = grouped["Frequency"] / total_freq
        else:
            grouped["Probability"] = 0
        return grouped

    @output
    @render.table
    def results_table():
        return grouped_table()

    @output
    @render.ui
    def filter_value_ui():
        filter_cov = input.filter_covariate()
        if filter_cov == "Mileage":
            data = simulated_data()
            unique_bins = sorted(data["Miles_Binned"].unique())
            return ui.input_select(
                "filter_value",
                "Select Mileage Bin",
                choices=[str(int(x)) for x in unique_bins],
            )
        elif filter_cov == "Brand":
            return ui.input_select(
                "filter_value",
                "Select Brand",
                choices=["Brand_A", "Brand_B", "Brand_C"],
            )
        else:
            return ui.div()

app = App(app_ui, server)

```

<!-- ```{shinylive-python}
from shiny import App, ui, render, reactive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.navset_tab(
        ui.nav("Table",
            ui.layout_sidebar(
                ui.sidebar(
                    ui.input_slider(
                        "numEvents",
                        "Number of Simulated Outcomes",
                        min=1,
                        max=100000,
                        value=10000,
                        step=1000,
                    ),
                    ui.input_checkbox_group(
                        "covariates",
                        "Select Covariates to Group By",
                        choices=["Mileage", "Brand"],
                        inline=True,
                        selected=["Mileage", "Brand"],
                    ),
                    ui.input_select(
                        "filter_covariate",
                        "Conditional Filter: Choose Covariate",
                        choices=["None", "Mileage", "Brand"],
                        selected="None",
                    ),
                    ui.output_ui("filter_value_ui"),
                ),
                ui.output_table("results_table")
            )
        ),
        ui.nav("Histogram",
            ui.layout_sidebar(
                ui.sidebar(
                    ui.input_select(
                        "hist_var",
                        "Select Variable for Histogram",
                        choices=["Miles_Driven", "Miles_Binned", "Breakdowns", "Brand"],
                        selected="Miles_Driven"
                    )
                ),
                ui.output_plot("hist_plot")
            )
        ),
        ui.nav("XY Plot",
            ui.layout_sidebar(
                ui.sidebar(
                    ui.input_select(
                        "xy_var",
                        "Select Variable for X-Axis",
                        choices=["Miles_Driven", "Miles_Binned", "Brand"],
                        selected="Miles_Binned"
                    )
                ),
                ui.output_plot("xy_plot")
            )
        )
    ),
    title="Simulated Breakdown Data"
)

def server(input, output, session):
    # Set seed for reproducibility.
    np.random.seed(42)
    
    @reactive.Calc
    def simulated_data():
        n = input.numEvents()
        miles_driven = np.random.lognormal(mean=np.log(10000), sigma=0.5, size=n)
        brands = np.random.choice(
            ["Brand_A", "Brand_B", "Brand_C"],
            size=n,
            p=[0.4, 0.4, 0.2],
        )
        beta_miles = 0.000001
        brand_effects = {"Brand_A": 0.1, "Brand_B": 0.05, "Brand_C": -0.02}
        lambda_vals = np.exp(beta_miles * miles_driven + np.vectorize(brand_effects.get)(brands))
        breakdowns = np.random.poisson(lambda_vals)
        data = pd.DataFrame({
            "Miles_Driven": miles_driven,
            "Brand": brands,
            "Breakdowns": breakdowns,
        })
        # Bin miles into intervals of 10,000.
        data["Miles_Binned"] = (data["Miles_Driven"] // 10000) * 10000
        return data

    @reactive.Calc
    def grouped_table():
        data = simulated_data()
        covariates = input.covariates()
        group_cols = []
        if "Mileage" in covariates:
            group_cols.append("Miles_Binned")
        if "Brand" in covariates:
            group_cols.append("Brand")
        # Always include Breakdowns.
        group_cols.append("Breakdowns")
        
        grouped = data.groupby(group_cols).size().reset_index(name="Frequency")
        
        # Apply filtering if requested.
        filter_cov = input.filter_covariate()
        if filter_cov != "None":
            if filter_cov == "Mileage":
                filter_val = input.filter_value()
                if filter_val is not None:
                    grouped = grouped[grouped["Miles_Binned"] == float(filter_val)]
            elif filter_cov == "Brand":
                filter_val = input.filter_value()
                if filter_val is not None:
                    grouped = grouped[grouped["Brand"] == filter_val]
        
        total_freq = grouped["Frequency"].sum()
        if total_freq > 0:
            grouped["Probability"] = grouped["Frequency"] / total_freq
        else:
            grouped["Probability"] = 0
        return grouped

    @reactive.Calc
    def aggregated_for_xy():
        data = simulated_data()
        selected_var = input.xy_var()
        if selected_var == "Miles_Driven":
            # Use binned values for Miles_Driven.
            data = data.copy()
            data["Miles_Driven_Binned"] = data["Miles_Binned"]
            agg = data.groupby("Miles_Driven_Binned").agg(
                Total_Breakdowns=("Breakdowns", "sum"),
                Count=("Breakdowns", "count")
            ).reset_index().rename(columns={"Miles_Driven_Binned": selected_var})
        elif selected_var == "Miles_Binned":
            agg = data.groupby("Miles_Binned").agg(
                Total_Breakdowns=("Breakdowns", "sum"),
                Count=("Breakdowns", "count")
            ).reset_index().rename(columns={"Miles_Binned": selected_var})
        elif selected_var == "Brand":
            agg = data.groupby("Brand").agg(
                Total_Breakdowns=("Breakdowns", "sum"),
                Count=("Breakdowns", "count")
            ).reset_index().rename(columns={"Brand": selected_var})
        else:
            agg = data.groupby(selected_var).agg(
                Total_Breakdowns=("Breakdowns", "sum"),
                Count=("Breakdowns", "count")
            ).reset_index()
        return agg

    @output
    @render.table
    def results_table():
        return grouped_table()

    @output
    @render.ui
    def filter_value_ui():
        filter_cov = input.filter_covariate()
        if filter_cov == "Mileage":
            data = simulated_data()
            unique_bins = sorted(data["Miles_Binned"].unique())
            return ui.input_select(
                "filter_value",
                "Select Mileage Bin",
                choices=[str(int(x)) for x in unique_bins],
            )
        elif filter_cov == "Brand":
            return ui.input_select(
                "filter_value",
                "Select Brand",
                choices=["Brand_A", "Brand_B", "Brand_C"],
            )
        else:
            return ui.div()

    @output
    @render.plot
    def hist_plot():
        var = input.hist_var()
        data = simulated_data()
        fig, ax = plt.subplots()
        if var in ["Miles_Driven", "Miles_Binned", "Breakdowns"]:
            ax.hist(data[var], bins=20, color="skyblue", edgecolor="black")
            ax.set_xlabel(var)
            ax.set_ylabel("Frequency")
            ax.set_title(f"Histogram of {var}", fontsize=14)
        elif var == "Brand":
            counts = data[var].value_counts()
            ax.bar(counts.index, counts.values, color="skyblue", edgecolor="black")
            ax.set_xlabel(var)
            ax.set_ylabel("Count")
            ax.set_title(f"Bar Chart of {var}")
        return fig

    @output
    @render.plot
    def xy_plot():
        agg = aggregated_for_xy()
        var = input.xy_var()
        fig, ax = plt.subplots()
        if var == "Brand":
            ax.bar(agg[var], agg["Total_Breakdowns"], color="steelblue")
            ax.set_xlabel(var)
            ax.set_ylabel("Total Breakdowns")
            ax.set_title(f"{var} vs Total Breakdowns")
        else:
            ax.scatter(agg[var], agg["Total_Breakdowns"], color="steelblue")
            ax.set_xlabel(var)
            ax.set_ylabel("Total Breakdowns")
            ax.set_title(f"{var} vs Total Breakdowns")
        return fig

app = App(app_ui, server)
``` -->

