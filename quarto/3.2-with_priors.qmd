---
title: With Priors
format: html
filters:
  - shinylive
---

## Preview

We finally come to the last chapter, which is focused on finding the $\mathcal{L}(M \mid D)$ with prior values for the parameters.

In more statistical terms, this chapter utilizes a **Bayesian** perspective. We will introduce Bayes Theorem later...

## Why Use Priors?

Let's say we are flipping a coin that, upon basic examination, appears to be fair (produces 50% heads). However, let's say the first three flips are: Heads, Heads, and Heads. Now what should we estimate the true propability of heads to be after the three flips?

- In the Frequentist view (under which most of the primer has operated), the probability of heads with the maximum likelihood is simply P=1, i.e. we expect to always get heads.
- In the Bayesian view, the probability of heads is greater than 0.5 and less than 1, depending on how strongly you had a prior belief that the coin was fair.

Here's an app that let's you set the real probability of the coin and the strength of your prior belief. The more narrowly you set the prior distribution around 0.5, the more strongly you believe 0.5 is the correct probability.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.output_plot("posteriorPlot", height="500px"),
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd()
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig            
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Track posterior probabilities at p=0.5 for each flip count
        flip_counts = []
        posterior_values = []
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posteriors for each flip count
        for idx, data in enumerate(history):
            total_flips = data["total_flips"]
            total_heads = data["total_heads"]
            
            # Calculate posterior using Beta distribution (conjugate prior approximation)
            # We convert the normal prior to an approximate beta prior
            alpha_prior = 0.5 / prior_sd**2
            beta_prior = alpha_prior
            
            # Update with observed data
            alpha_posterior = alpha_prior + total_heads
            beta_posterior = beta_prior + (total_flips - total_heads)
            
            # Calculate and normalize posterior
            posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
            posterior = posterior / np.max(posterior)  # Normalize for better display
            
            # Plot with darker lines for more recent posteriors
            alpha = 0.3 + 0.7 * (idx + 1) / len(history)
            lw = 1 + idx * 0.5
            if idx == len(history) - 1:
                ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                        label=f'Posterior after {total_flips} flips ({total_heads} heads)')
            
            # Store values for the evolution plot
            flip_counts.append(total_flips)
            posterior_values.append(stats.beta.pdf(input.true_prob(), alpha_posterior, beta_posterior) / 
                                np.max(stats.beta.pdf(x, alpha_posterior, beta_posterior)))
        
        # Add true probability line
        ax.axvline(x=history[-1]["true_prob"], color='red', linestyle='--', 
                label=f'True probability: {history[-1]["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        # Add an inset plot showing the evolution of posterior at the true probability
        if len(flip_counts) > 1:
            # Use subplot method instead of add_axes
            inset_ax = fig.add_subplot(3, 3, 9)
            inset_ax.plot(flip_counts, posterior_values, 'ko-', ms=4)
            inset_ax.set_xlabel('Number of Flips', fontsize=9)
            inset_ax.set_ylabel('Posterior Density', fontsize=9)
            inset_ax.set_title('Evolution at True p', fontsize=10)
            inset_ax.grid(True, linestyle='--', alpha=0.7)
            inset_ax.tick_params(axis='both', labelsize=8)
        
        plt.tight_layout()
        return fig

app = App(app_ui, server)
```

If you knew the coin had just come from the bank, you may give a strong prior for a fair coin such that even after the three heads, the probability would be only slightly above 0.5. However, if you didn't know the origin of the coin, you may have a much weaker prior - but you also know it would be virtually impossible to create a coin that could only land heads - and therefore you would still give a prior that prevented P=1.

### Posterior Distribution

Hopefully you noticed that after flipping coins the chart in the app showed a 'posterior' distribution. This distribution shows what we believe about the probability a random flip shows heads after we've seen a number of flips. We may say the posterior is a revised belief after witnessing additional evidence/data.

### Probabilities about Probabilities

One of the very confusing thing about Bayesian statistics is it gets meta. We start to talk about probabilities of parameters that describe probabilities. Like treating addiction, the first step is to explicitly acknowledge what's happening:

1) We have a probability distribution that describes a parameter P. The probability distribution says if we sampled P randomly, here's the more and less likely values.
2) Let's assume we've sampled P and we have a value, like 0.578.
3) We then use the value for P, which was 0.578 to generate random data.
    - This will randomly generate 0's for tails and 1's for heads
    - It will do so at a rate that is approximately 0.578 heads
4) We could resample P, it will have a new value like 0.489.
    - This will randomly generate 0's for tails and 1's for heads
    - It will do so at a rate that is approximately 0.489 heads

We can have the model sample the value of parameter P based on either what we believe before we saw data (the prior) or after we saw data (the posterior). The distribution for P is different in the prior and posterior, but the process of generating data described above is the same.

Here's an updated coin flipping app to try to make this more obvious. We can now see some examples (four) of the value P may take when sampled, and then subsequently how flip data would be generated from the sampled P. There is a seperate tab for sampling from the prior, and another for sampling from the posterior.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.navset_tab(
            ui.nav_panel("Prior and Posterior", 
                 ui.output_plot("posteriorPlot", height="500px")
            ),
            ui.nav_panel("Prior P Histograms", 
                 ui.output_plot("fixedPHistograms", height="500px")
            ),
            ui.nav_panel("Posterior P Histograms", 
                 ui.output_plot("posteriorPHistograms", height="500px")
            )
        )
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd(),
            "raw_flips": flips
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig

    @output
    @render.plot
    def fixedPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
            
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        
        # Calculate prior parameters
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Sample P values from the prior distribution with truncation between 0 and 1
        prior_p_samples = []
        for _ in range(4):
            sample = np.random.normal(prior_mean, prior_sd)
            # Truncate to [0,1]
            sample = min(max(sample, 0.001), 0.999)
            prior_p_samples.append(sample)
            
        n = int(input.num_flips())
        
        for i, p in enumerate(prior_p_samples):
            # Generate data with prior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='skyblue', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Prior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Prior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig
    
    @output
    @render.plot
    def posteriorPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
        
        data = history[0]
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        prior_sd = data["prior_sd"]
        
        # Calculate posterior parameters
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Draw 4 samples from the posterior
        posterior_p_samples = np.random.beta(alpha_posterior, beta_posterior, 4)
        
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        n = int(input.num_flips())
        
        for i, p in enumerate(posterior_p_samples):
            # Generate data with posterior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='lightgreen', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Posterior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Posterior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig

app = App(app_ui, server)

```

Picking a larger sample size and choosing a true probability of heads that differs notably from 0.5 can help make the difference between the prior and posterior more obvious. In small sample sizes you are much more at the mercy of random chance, which will occassionally have some unexpected results.

## The Argument For and Against Priors

These are generally the 'Bayesian' arguments for priors:

1) If you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.
2) If you *do* have an overwhelming amount of data, the priors you set will have little difference in the final statistical model.

The argument against priors is that they can introduce arbitrary practitioner bias into an otherwise sound statistical model. This is certainly possible, but explanation and disclosure of any priors used should mitigate these concerns. Finally, with regards to 2) given above, this is generally true unless your priors precluded a valid part of the sample space. For instance, a uniform prior (constant probability between two points) can be a way to minimize practitioner bias, however, if a valid parameter value is outside of the bounds, it is completely inaccessible to the model.

## The Math of Bayesian Updating

The updating of probabilities/beliefs in Bayesian analysis is based on Bayes' Theorem:

$$
P(\theta | \text{data}) = \frac{P(\text{data} | \theta) P(\theta)}{P(\text{data})}
$$

Where:

- $P(\theta | \text{data})$  is the posterior
- $P(\text{data} | \theta)$  is the likelihood
- $P(\theta)$  is the prior
- $P(\text{data})$  is the marginal likelihood (normalizing constant)

Since $P(\text{data})$ in practical applications is just a normalizing constant, it can be much simpler conceptually to think about Bayes' Theorem this way:

$$
P(\theta | \text{data}) \propto P(\text{data} | \theta) P(\theta)
$$

Which states that posterior value of the model parameters ($P(\theta | \text{data})$) after seeing the data is proportional to the likelihood of the data based on the model parameters ($P(\text{data} | \theta)$) multiplied by the prior belief ($P(\theta)$) in the parameter values.

## ...

...

## Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori (MAP)

In machine learning, MAP (Maximum A Posteriori) estimation is a Bayesian approach to parameter estimation. It finds the most probable parameter values given the observed data and a prior distribution.

Mathematically, it maximizes the posterior probability:

$$
\theta_{MAP} = \arg\max_{\theta} P(\theta | X)
$$

Using Bayes’ theorem:

$$
P(\theta | X) = \frac{P(X | \theta) P(\theta)}{P(X)}
$$

Since  P(X)  is constant for optimization, MAP simplifies to:

$$
\theta_{MAP} = \arg\max_{\theta} P(X | \theta) P(\theta)
$$

This contrasts with Maximum Likelihood Estimation (MLE), which maximizes only  P(X | \theta)  (ignoring the prior  P(\theta) ). MAP is useful when incorporating prior knowledge into the learning process.

### Low Dimensional Search (Grid)

Just like we did in the previous chapter on $\mathcal{L}(M|D)$ without priors, we can find parameter values with grid search. However, we should acknowledge two major differences:

1) Previously we did grid search to find the parameter with the maximum likelihood. Now we do grid search to find the distribution of likelihoods for the parameter $(P(\text{data})$. The results will include a maximum likelihood value of the parameter, but also other lower likelihood values of the parameter.
2) We multiply the values found in the grid search by our prior belief in the parameter values ($P(\theta)$). If we do not have a strong prior belief, we can choose values that have little to no effect.

**NOTE TO SELF, NEED TO UNIFY THE LIKELIHOOD AND PROB NOTATION**

### High Dimensional Search (MCMC)

Like before, grid search does not scale well to many parameters/dimensions. To solve this previously we introduced Gradient Ascent, which uses slopes/gradients to guide us to the maximum value.

There's a major problem, however, with using Gradient Ascent in Bayesian analysis - we do not want to find just the most likely value of the parameter - we want to find the correct probability distribution for the parameter (the likelihood of any value the parameter could take).

The solution to this is to use random sampling methods, such that we search through the possible combinations of the parameters and see how likely they are. As the algorithm starts, we can think of it a little like grid search - we check some combination of parameters and calculate a likelihood for those parameters based on how well they fit the data. The problem is that in high dimensional space, the extreme majority of the parameter combinations will have virtually zero likelihood. (Picture a model with random parameter values generating data that isn't even close to what we observe).

To solve the problem of constantly testing parameters that have virtually zero likelihood, modern sampling algorithms know to head towards areas of higher likelihood. However, importantly they do not simply head towards the peak like they would in Gradient Ascent - instead they strategically wander around the edges and peaks of the high likelihood space, mapping out those regions like a cartographer mapping a mountain ridge.

### Parameter Find App

To get a feel for what it's like to search the parameter space for parameter combinations with a high likelihood, play around with the app below.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Parameter Estimation"),
    ui.row(
        ui.column(3, 
            ui.input_select("search_method", "Search Method:", 
                choices=["Grid Search", "Likelihood Search"], selected="Grid Search"),
            ui.input_select("num_samples", "Number of Samples:", 
                choices=["10", "100", "1000"], selected="100"),
            ui.input_action_button("generate", "Generate Samples", class_="btn-primary"),
            ui.p(""),
            ui.input_checkbox("show_samples", "Show sample points", value=True)
        ),
        ui.column(9,
            ui.output_plot("likelihoodPlot", height="500px")
        )
    )
)

def server(input, output, session):
    # Store the current samples
    samples = reactive.Value(None)
    
    @reactive.effect
    @reactive.event(input.generate)
    def handle_generate():
        try:
            n = int(input.num_samples())
            search_method = input.search_method()
            
            # True parameters
            true_x, true_y = 0.6, 0.4
            
            if search_method == "Grid Search":
                # Create evenly spaced grid
                grid_size = int(np.sqrt(n))
                x_grid = np.linspace(0, 1, grid_size)
                y_grid = np.linspace(0, 1, grid_size)
                X, Y = np.meshgrid(x_grid, y_grid)
                theta1 = X.flatten()[:n]  # Ensure exactly n samples
                theta2 = Y.flatten()[:n]
                
                # Add small noise to make it look more realistic
                theta1 += 0.01 * np.random.randn(len(theta1))
                theta2 += 0.01 * np.random.randn(len(theta2))
                
                # Ensure values stay within bounds
                theta1 = np.clip(theta1, 0, 1)
                theta2 = np.clip(theta2, 0, 1)
                
            else:  # Likelihood Search
                # First, generate random samples across the parameter space
                theta1_initial = np.random.uniform(0, 1, n*2)  # Generate extra samples
                theta2_initial = np.random.uniform(0, 1, n*2)
                
                # Calculate likelihood based on distance from true parameters
                # (simulate a likelihood function with a peak at true parameters)
                likelihoods = np.exp(-10 * ((theta1_initial - true_x)**2 + (theta2_initial - true_y)**2))
                
                # Sample from these points with probability proportional to likelihood
                selection_probs = likelihoods / np.sum(likelihoods)
                selected_indices = np.random.choice(len(theta1_initial), size=n, p=selection_probs, replace=False)
                
                theta1 = theta1_initial[selected_indices]
                theta2 = theta2_initial[selected_indices]
            
            samples.set((theta1, theta2))
        except Exception as e:
            print(f"Error generating samples: {e}")
            samples.set(None)

    @output
    @render.plot
    def likelihoodPlot():
        # Create basic plot
        fig, ax = plt.subplots(figsize=(8, 6))
        
        # Get current samples
        current_samples = samples.get()
        
        if current_samples is None:
            ax.text(0.5, 0.5, "Click 'Generate Samples' to create data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            return fig
        
        theta1, theta2 = current_samples
        n = len(theta1)
        
        try:
            # Create a 2D histogram
            H, xedges, yedges = np.histogram2d(
                theta1, theta2, 
                bins=20, 
                range=[[0, 1], [0, 1]]
            )
            
            # Use pcolormesh for better rendering
            pcm = ax.pcolormesh(xedges, yedges, H.T, cmap='viridis')
            
            # Add colorbar
            plt.colorbar(pcm, ax=ax, label='Count')
            
            # Plot the samples if requested
            if input.show_samples():
                ax.scatter(theta1, theta2, s=5, color='red', alpha=0.3, label=f'Samples (n={n})')
            
            # Mark true parameters
            ax.plot(0.6, 0.4, 'y*', markersize=15, markeredgecolor='black', label='True parameters')
            
            # Labels and layout
            ax.set_xlabel('Parameter θ₁')
            ax.set_ylabel('Parameter θ₂')
            
            search_method = input.search_method().split()[0]
            ax.set_title(f'Parameter Estimation using {search_method} ({n} samples)')
            
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.legend(loc='upper right')
            
        except Exception as e:
            print(f"Error plotting: {e}")
            ax.text(0.5, 0.5, f"Error creating plot: {str(e)}", 
                   ha='center', va='center', fontsize=12)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
        
        return fig

app = App(app_ui, server)
```


## Conclusion

As stated in the title, this is intended to be a 'primer' to introduce you to the topic. We hope you have some sense for the important foundations of statistics. We encourage you to learn by experimenting - creating your own data and examples will be the fastest path to understanding for the majority of people.

There are also many excellent texts in both statistics and machine learning, hopefully with some intuition on the basics, the rationale and math will be much easier to understand.


## Text

Text.


```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        # MLE for variance uses 1/n factor
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```


## Old Thoughts

For with priors, consider:
- Low Dimensional Search (Grid)
- High Dimensional Search (MCMC)


## Probability Distribution or Uncertainty?

While I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.

The second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don't have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.

In the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).

```{mermaid}
flowchart LR
    subgraph DGD[Data Generating Distribution]
        subgraph UD[Uncertainty Distributions]
            GammaMean[Gamma Distribution]
            GammaVar[Gamma Distribution]
        end
        Mean[Mean]
        Variance[Variance]
        GammaMean --> Mean
        GammaVar --> Variance
        Mean --> Normal
        Variance --> Normal
        Normal[Normal Distribution]
    end
    Normal --> Height[Height Observations]

```

