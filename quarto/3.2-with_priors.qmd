---
title: With Priors
format: html
filters:
  - shinylive
---

## Preview

We finally come to the last chapter, which is focused on finding the $\mathcal{L}(M \mid D)$ with prior values for the parameters.

In more statistical terms, this chapter utilizes a **Bayesian** perspective. We will introduce Bayes Theorem later...

## Why Use Priors?

Let's say we are flipping a coin that, upon basic examination, appears to be fair (produces 50% heads). However, let's say the first three flips are: Heads, Heads, and Heads. Now what should we estimate the true propability of heads to be after the three flips?

- In the Frequentist view, the probability of heads with the maximum likelihood is simply P=1, i.e. we expect to always get heads.
- In the Bayesian view, the probability of heads is greater than 0.5 and less than 1, depending on how strongly you had a prior belief that the coin was fair.

Here's an app that let's you set the real probability of the coin and the strength of your prior belief. The more narrowly you set the prior distribution around 0.5, the more strongly you believe 0.5 is the correct probability.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.output_plot("posteriorPlot", height="500px"),
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd()
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig            
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Track posterior probabilities at p=0.5 for each flip count
        flip_counts = []
        posterior_values = []
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posteriors for each flip count
        for idx, data in enumerate(history):
            total_flips = data["total_flips"]
            total_heads = data["total_heads"]
            
            # Calculate posterior using Beta distribution (conjugate prior approximation)
            # We convert the normal prior to an approximate beta prior
            alpha_prior = 0.5 / prior_sd**2
            beta_prior = alpha_prior
            
            # Update with observed data
            alpha_posterior = alpha_prior + total_heads
            beta_posterior = beta_prior + (total_flips - total_heads)
            
            # Calculate and normalize posterior
            posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
            posterior = posterior / np.max(posterior)  # Normalize for better display
            
            # Plot with darker lines for more recent posteriors
            alpha = 0.3 + 0.7 * (idx + 1) / len(history)
            lw = 1 + idx * 0.5
            if idx == len(history) - 1:
                ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                        label=f'Posterior after {total_flips} flips ({total_heads} heads)')
            
            # Store values for the evolution plot
            flip_counts.append(total_flips)
            posterior_values.append(stats.beta.pdf(input.true_prob(), alpha_posterior, beta_posterior) / 
                                np.max(stats.beta.pdf(x, alpha_posterior, beta_posterior)))
        
        # Add true probability line
        ax.axvline(x=history[-1]["true_prob"], color='red', linestyle='--', 
                label=f'True probability: {history[-1]["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        # Add an inset plot showing the evolution of posterior at the true probability
        if len(flip_counts) > 1:
            # Use subplot method instead of add_axes
            inset_ax = fig.add_subplot(3, 3, 9)
            inset_ax.plot(flip_counts, posterior_values, 'ko-', ms=4)
            inset_ax.set_xlabel('Number of Flips', fontsize=9)
            inset_ax.set_ylabel('Posterior Density', fontsize=9)
            inset_ax.set_title('Evolution at True p', fontsize=10)
            inset_ax.grid(True, linestyle='--', alpha=0.7)
            inset_ax.tick_params(axis='both', labelsize=8)
        
        plt.tight_layout()
        return fig

app = App(app_ui, server)
```

If you knew the coin had just come from the bank, you may give a strong prior for a fair coin such that even after the three heads, the probability would be only slightly above 0.5. However, if you didn't know the origin of the coin, you may have a much weaker prior - but you also know it would be virtually impossible to create a coin that could only land heads - and therefore you would still give a prior that prevented P=1.

## The Argument For and Against Priors

These are generally the 'Bayesian' arguments for priors:

1) If you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.
2) If you *do* have an overwhelming amount of data, the priors you set will have little difference in the final statistical model.

The argument against priors is that they can introduce arbitrary practitioner bias into an otherwise sound statistical model. This is certainly possible, but explanation and disclosure of any priors used should mitigate these concerns. Finally, with regards to 2) given above, this is generally true unless your priors precluded a valid part of the sample space. For instance, your statistical model included a uniform prior on human height with the lowest being the shortest human and the highest being the tallest human - but someone could still be born that is outside of those bounds.

## Basics of Bayesian Analysis

Note to self - prevention of non-sensical parameter values.

Bayesian statistical analysis is based on the principles of probability updating through Bayes’ Theorem. The key elements include:

1.	Prior Distribution – Represents prior beliefs about the parameters before observing data. It can be informative or non-informative.
2.	Likelihood Function – Describes the probability of the observed data given the parameters.
3.	Posterior Distribution – Obtained by combining the prior distribution and the likelihood using Bayes’ Theorem. It represents updated beliefs after observing the data.
4.	Bayes’ Theorem – The fundamental formula:

$$
P(\theta | \text{data}) = \frac{P(\text{data} | \theta) P(\theta)}{P(\text{data})}
$$

where:

- $P(\theta | \text{data})$  is the posterior
- $P(\text{data} | \theta)$  is the likelihood
- $P(\theta)$  is the prior
- $P(\text{data})$  is the marginal likelihood (normalizing constant)

5.	Evidence (Marginal Likelihood) – The total probability of the observed data across all parameter values. It is used in model comparison.
6.	Posterior Predictive Distribution – Used to make predictions by integrating over the posterior distribution.
7.	Computation Methods – Due to complex posterior distributions, computational techniques like Markov Chain Monte Carlo (MCMC) or Variational Inference are often used for approximation.


## Other Arguments For and Against Baysian Analysis?


## Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori (MAP)

In machine learning, MAP (Maximum A Posteriori) estimation is a Bayesian approach to parameter estimation. It finds the most probable parameter values given the observed data and a prior distribution.

Mathematically, it maximizes the posterior probability:

$$
\theta_{MAP} = \arg\max_{\theta} P(\theta | X)
$$

Using Bayes’ theorem:

$$
P(\theta | X) = \frac{P(X | \theta) P(\theta)}{P(X)}
$$

Since  P(X)  is constant for optimization, MAP simplifies to:

$$
\theta_{MAP} = \arg\max_{\theta} P(X | \theta) P(\theta)
$$

This contrasts with Maximum Likelihood Estimation (MLE), which maximizes only  P(X | \theta)  (ignoring the prior  P(\theta) ). MAP is useful when incorporating prior knowledge into the learning process.

### Low Dimensional Search (Grid)

### High Dimensional Search (MCMC)


## Text

Text.


```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Likelihood Calculation"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # When 'MLE' is clicked, update muInput and varInput to MLE estimates
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        y = data_vals()
        n = len(y)
        mle_mean = np.mean(y)
        # MLE for variance uses 1/n factor
        mle_var = np.sum((y - mle_mean)**2) / n
        session.send_input_message("muInput", {"value": mle_mean})
        session.send_input_message("varInput", {"value": mle_var})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```


## Old Thoughts

For with priors, consider:
- Low Dimensional Search (Grid)
- High Dimensional Search (MCMC)


## Probability Distribution or Uncertainty?

While I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.

The second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don't have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.

In the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).

```{mermaid}
flowchart LR
    subgraph DGD[Data Generating Distribution]
        subgraph UD[Uncertainty Distributions]
            GammaMean[Gamma Distribution]
            GammaVar[Gamma Distribution]
        end
        Mean[Mean]
        Variance[Variance]
        GammaMean --> Mean
        GammaVar --> Variance
        Mean --> Normal
        Variance --> Normal
        Normal[Normal Distribution]
    end
    Normal --> Height[Height Observations]

```

