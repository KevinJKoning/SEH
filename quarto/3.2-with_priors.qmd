---
title: With Priors
format: 
    html:
        mermaid:
            theme: neutral
filters:
  - shinylive
---

## Preview

We finally come to the last chapter, which is focused on finding the $\mathcal{L}(M \mid D)$ with **prior** values for the parameters.

In more statistical terms, this chapter utilizes a **Bayesian** perspective. Instead of starting off with Bayes Theorem, we show the usefulness of having prior values for our parameters through a coin flip example with limited data. This naturally leads to **posterior** estimates of the parameter values, which we obtain after we consider the data. Once we've gained some intuition for what Bayes Theorem does, we give the formal definition.

We call the posterior estimate of the parameter values the **posterior distribution**, since it is not a single value but a distribution, and it comes after (post) utilizing the data. We then discuss methods for finding the posterior distribution in low and high dimensions, including the similarities and differences to methods for finding the Maximum Likelihood Estimate (MLE).

## Why Use Priors?

Let's say we are flipping a coin that, upon basic examination, appears to be fair (produces 50% heads). However, let's say the first three flips are: Heads, Heads, and Heads. Now what should we estimate the true propability of heads to be after the three flips?

- In the Frequentist view (under which most of the primer has operated), the probability of heads with the maximum likelihood is simply P=1, i.e. we expect to always get heads.
- In the Bayesian view, the probability of heads is greater than 0.5 and less than 1, depending on how strongly you had a prior belief that the coin was fair.

Here's an app that let's you set the real probability of the coin and the strength of your prior belief. The more narrowly you set the prior distribution around 0.5, the more strongly you believe 0.5 is the correct probability.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.output_plot("posteriorPlot", height="500px"),
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd()
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig            
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Track posterior probabilities at p=0.5 for each flip count
        flip_counts = []
        posterior_values = []
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posteriors for each flip count
        for idx, data in enumerate(history):
            total_flips = data["total_flips"]
            total_heads = data["total_heads"]
            
            # Calculate posterior using Beta distribution (conjugate prior approximation)
            # We convert the normal prior to an approximate beta prior
            alpha_prior = 0.5 / prior_sd**2
            beta_prior = alpha_prior
            
            # Update with observed data
            alpha_posterior = alpha_prior + total_heads
            beta_posterior = beta_prior + (total_flips - total_heads)
            
            # Calculate and normalize posterior
            posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
            posterior = posterior / np.max(posterior)  # Normalize for better display
            
            # Plot with darker lines for more recent posteriors
            alpha = 0.3 + 0.7 * (idx + 1) / len(history)
            lw = 1 + idx * 0.5
            if idx == len(history) - 1:
                ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                        label=f'Posterior after {total_flips} flips ({total_heads} heads)')
            
            # Store values for the evolution plot
            flip_counts.append(total_flips)
            posterior_values.append(stats.beta.pdf(input.true_prob(), alpha_posterior, beta_posterior) / 
                                np.max(stats.beta.pdf(x, alpha_posterior, beta_posterior)))
        
        # Add true probability line
        ax.axvline(x=history[-1]["true_prob"], color='red', linestyle='--', 
                label=f'True probability: {history[-1]["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        # Add an inset plot showing the evolution of posterior at the true probability
        if len(flip_counts) > 1:
            # Use subplot method instead of add_axes
            inset_ax = fig.add_subplot(3, 3, 9)
            inset_ax.plot(flip_counts, posterior_values, 'ko-', ms=4)
            inset_ax.set_xlabel('Number of Flips', fontsize=9)
            inset_ax.set_ylabel('Posterior Density', fontsize=9)
            inset_ax.set_title('Evolution at True p', fontsize=10)
            inset_ax.grid(True, linestyle='--', alpha=0.7)
            inset_ax.tick_params(axis='both', labelsize=8)
        
        plt.tight_layout()
        return fig

app = App(app_ui, server)
```

If you knew the coin had just come from the bank, you may give a strong prior for a fair coin such that even after the three heads, the probability would be only slightly above 0.5. However, if you didn't know the origin of the coin, you may have a much weaker prior - but you also know it would be virtually impossible to create a coin that could only land heads - and therefore you would still give a prior that prevented $P=1$.

### Posterior Distribution

Hopefully you noticed that after flipping coins the chart in the app showed a 'posterior' distribution. This distribution shows what we believe about the probability a random flip shows heads after we've seen a number of flips. We may say the posterior is a revised belief after witnessing additional evidence/data.

### Probabilities about Probabilities

One of the very confusing thing about Bayesian statistics is it gets meta. We start to talk about probabilities of parameters that describe probabilities. Like treating addiction, the first step is to explicitly acknowledge what's happening:

1) We have a probability distribution that describes a parameter $P$. The probability distribution says if we sampled $P$ randomly, here's the more and less likely values.
2) Let's assume we've sampled $P$ and we have a value, like 0.578.
3) We then use the value for P, which was 0.578, to generate random data.
    - This will randomly generate 0's for tails and 1's for heads
    - It will do so at a rate that is approximately 0.578 heads
4) We could resample P, it will have a new value like 0.489.
    - This will randomly generate 0's for tails and 1's for heads
    - It will do so at a rate that is approximately 0.489 heads

Here's the process summarized as a diagram:

```{mermaid}
graph LR
    subgraph "Draw of Parameter P"
        subgraph "Uncertainty in P"
            mean[Mean]
            var[Variance]
            mean --> dist
            var --> dist
        end
        dist[Normal Distribution]
        dist --> |"generates"| P[Parameter P]
    end
    
    P --> |"generates"| flips[Coin Flip Outcomes]
    
    style P fill:#f9f,stroke:#333,stroke-width:2px
    style flips fill:#bbf,stroke:#333,stroke-width:2px
    style dist fill:#dfd,stroke:#333,stroke-width:2px
```

We can have the model sample the value of parameter $P$ based on either what we believe before we saw data (the prior) or after we saw data (the posterior). The distribution for $P$ is different in the prior and posterior, but the process of generating data described above is the same.

Here's an updated coin flipping app to try to make this more obvious. We can now see some examples (four) of the value $P$ may take when sampled, and then subsequently how flip data would be generated from the sampled P. There is a seperate tab for sampling from the prior, and another for sampling from the posterior.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

app_ui = ui.page_fluid(
    ui.h2("Bayesian Coin Flip Analysis"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.p("The prior is always centered at 0.5"),
            ui.input_slider("prior_sd", "Prior Standard Deviation", min=0.01, max=0.5, value=0.1, step=0.01),
            ui.p("Set the true probability of heads for the coin:"),
            ui.input_slider("true_prob", "True Probability of Heads", min=0.1, max=0.9, value=0.5, step=0.05),
            ui.p("Select the number of coin flips:"),
            ui.input_select("num_flips", "Number of Coin Flips", 
                          choices=["2", "5", "10", "50", "100", "1000", "10000"],
                          selected="10"),
            ui.input_action_button("flip", "Flip Coins"),
        ),
        ui.navset_tab(
            ui.nav_panel("Prior and Posterior", 
                 ui.output_plot("posteriorPlot", height="500px")
            ),
            ui.nav_panel("Prior P Histograms", 
                 ui.output_plot("fixedPHistograms", height="500px")
            ),
            ui.nav_panel("Posterior P Histograms", 
                 ui.output_plot("posteriorPHistograms", height="500px")
            )
        )
    ),
)

def server(input, output, session):
    flips_history = reactive.Value([])
    
    @reactive.effect
    @reactive.event(input.flip)
    def handle_flip():
        # Generate new coin flips
        n = int(input.num_flips())
        true_prob = input.true_prob()
        flips = np.random.binomial(1, true_prob, n)
        heads_count = np.sum(flips)
        
        # Reset history and add new flip result
        flips_history.set([{
            "num_flips": n,
            "total_flips": n,
            "heads_count": heads_count,
            "total_heads": heads_count,
            "true_prob": true_prob,
            "prior_sd": input.prior_sd(),
            "raw_flips": flips
        }])

    @output
    @render.plot
    def posteriorPlot():
        # Forces the plot to react to button clicks
        input.flip()
        
        # Get the current history
        history = flips_history.get()
        if not history:
            # Show prior only if no flips yet
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Plot the prior distribution
            x = np.linspace(0, 1, 1000)
            prior_mean = 0.5
            prior_sd = input.prior_sd()
            prior = stats.norm.pdf(x, prior_mean, prior_sd)
            prior = prior / np.max(prior)  # Normalize for better display
            
            ax.plot(x, prior, 'b-', lw=2, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1.1)
            ax.set_xlabel('Probability of Heads', fontsize=12)
            ax.set_ylabel('Density (normalized)', fontsize=12)
            ax.set_title('Bayesian Analysis of Coin Flips: Prior Distribution', fontsize=14)
            ax.axvline(x=input.true_prob(), color='red', linestyle='--', 
                       label=f'True probability: {input.true_prob()}')
            ax.legend(loc='upper left')
            
            plt.tight_layout()
            return fig
        
        # Create plot for posterior evolution
        plt.close('all')  # Close any existing figures to avoid memory leaks
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot values for different number of total flips
        x = np.linspace(0, 1, 1000)
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Plot the prior lightly
        prior = stats.norm.pdf(x, prior_mean, prior_sd)
        prior = prior / np.max(prior)  # Normalize
        ax.plot(x, prior, 'b-', alpha=0.3, lw=1, label=f'Prior (Normal with μ=0.5, σ={prior_sd})')
        
        # Plot posterior for current data
        data = history[0]  # We only have one entry now with the reset approach
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        
        # Calculate posterior using Beta distribution (conjugate prior approximation)
        # We convert the normal prior to an approximate beta prior
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        
        # Update with observed data
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Calculate and normalize posterior
        posterior = stats.beta.pdf(x, alpha_posterior, beta_posterior)
        posterior = posterior / np.max(posterior)  # Normalize for better display
        
        # Plot the posterior
        ax.plot(x, posterior, 'g-', alpha=1.0, lw=2.5, 
                label=f'Posterior after {total_flips} flips ({total_heads} heads)')
        
        # Add true probability line
        ax.axvline(x=data["true_prob"], color='red', linestyle='--', 
                   label=f'True probability: {data["true_prob"]}')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1.1)
        ax.set_xlabel('Probability of Heads', fontsize=12)
        ax.set_ylabel('Density (normalized)', fontsize=12)
        ax.set_title('Bayesian Analysis of Coin Flips: Posterior Distribution', fontsize=14)
        ax.legend(loc='upper left')
        
        plt.tight_layout()
        return fig

    @output
    @render.plot
    def fixedPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
            
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        
        # Calculate prior parameters
        prior_mean = 0.5
        prior_sd = history[0]["prior_sd"]
        
        # Sample P values from the prior distribution with truncation between 0 and 1
        prior_p_samples = []
        for _ in range(4):
            sample = np.random.normal(prior_mean, prior_sd)
            # Truncate to [0,1]
            sample = min(max(sample, 0.001), 0.999)
            prior_p_samples.append(sample)
            
        n = int(input.num_flips())
        
        for i, p in enumerate(prior_p_samples):
            # Generate data with prior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='skyblue', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Prior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Prior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig
    
    @output
    @render.plot
    def posteriorPHistograms():
        # Forces the plot to react to button clicks
        input.flip()
        
        history = flips_history.get()
        if not history:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "Please click 'Flip Coins' to generate data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
        
        data = history[0]
        total_flips = data["total_flips"]
        total_heads = data["total_heads"]
        prior_sd = data["prior_sd"]
        
        # Calculate posterior parameters
        alpha_prior = 0.5 / prior_sd**2
        beta_prior = alpha_prior
        alpha_posterior = alpha_prior + total_heads
        beta_posterior = beta_prior + (total_flips - total_heads)
        
        # Draw 4 samples from the posterior
        posterior_p_samples = np.random.beta(alpha_posterior, beta_posterior, 4)
        
        fig, axs = plt.subplots(2, 2, figsize=(10, 8))
        axs = axs.flatten()
        n = int(input.num_flips())
        
        for i, p in enumerate(posterior_p_samples):
            # Generate data with posterior P
            simulated_flips = np.random.binomial(1, p, n)
            
            # Plot histogram
            axs[i].hist(simulated_flips, bins=[-0.5, 0.5, 1.5], 
                        rwidth=0.8, color='lightgreen', edgecolor='black',
                        align='mid')
            axs[i].set_title(f'P ~ Posterior: {p:.3f}')
            axs[i].set_xticks([0, 1])
            axs[i].set_xticklabels(['Tails (0)', 'Heads (1)'])
            axs[i].set_ylabel('Frequency')
            
        fig.suptitle(f'Sampled Posterior P Values ({n} flips each)', fontsize=16)
        plt.tight_layout()
        return fig

app = App(app_ui, server)

```

Picking a larger sample size and choosing a true probability of heads that differs notably from 0.5 can help make the difference between the prior and posterior more obvious. In small sample sizes you are much more at the mercy of random chance, which will occassionally have some unexpected results.

## The Argument For and Against Priors

These are generally the 'Bayesian' arguments for priors:

1) If you do not have an overwhelming amount of data, your expert knowledge in how the system works is crucial to ensuring the statistical model predicts reasonable values.
2) If you *do* have an overwhelming amount of data, the priors you set will have little difference in the final statistical model.

The argument against priors is that they can introduce arbitrary practitioner bias into an otherwise sound statistical model. This is certainly possible, but explanation and disclosure of any priors used should mitigate these concerns. Finally, with regards to 2) given above, this is generally true unless your priors precluded a valid part of the sample space. For instance, a uniform prior (constant probability between two points) can be a way to minimize practitioner bias, however, if a valid parameter value is outside of the bounds, it is completely inaccessible to the model.

## The Math of Bayesian Updating

The updating of probabilities/beliefs in Bayesian analysis is based on Bayes' Theorem, where $\theta$ is the model parameter[s] and D is the observed data:

$$
P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
$$

Where:

- $P(\theta | D)$  is the posterior
- $P(D | \theta)$  is the likelihood
- $P(\theta)$  is the prior
- $P(D)$  is the marginal likelihood (normalizing constant)

Since $P(D)$ in practical applications is just a normalizing constant, it can be much simpler conceptually to think about Bayes' Theorem this way:

$$
P(\theta | D) \propto P(D | \theta) P(\theta)
$$

Which states that posterior value of the model parameters ($P(\theta | D)$) after seeing the data is proportional to the likelihood of the data based on the model parameters ($P(D | \theta)$) multiplied by the prior belief ($P(\theta)$) in the parameter values.

### Notation Notation Notation...

We've given Bayes' Theorem as you would normally see it, however, the notation of the result is slightly different than what would be consistent with our previous discussions. When we discussed the probability of model parameter values in the context of Maximum Likelihood Estimates, we called the result of finding the joint probability of the data given a model parameter[s] the **likelihood**:

$$
\mathcal{L}(\theta \mid D) = \prod_{i=1}^{N} P(d_i \mid \theta)
$$

So to be consistent, we may be tempted to write likelihood in the Bayesian context as:

$$
\mathcal{L}(\theta \mid D) \propto P(D | \theta) P(\theta)
$$

or more explicitly including the product:

$$
\mathcal{L}(\theta \mid D) \propto \prod_{i=1}^{N}  P(d_i | \theta) P(\theta)
$$

**However, only $P(D \mid \theta)$ is known as 'the likelihood' in Bayesian statistics**, I believe to be consistent with the right-hand side of the first equation above, not the second or third equation. But as Ralph Waldo Emerson said:

> A foolish consistency is the hobgoblin of little minds, adored by little statesmen and philosophers and divines.

## Methods for Estimating the Posterior Distribution

Estimating the posterior distribution of Bayesian model parameters can have similarities to Maximum Likelihood Estimates (MLE), but we will need to keep in mind a Bayesian posterior is a distribution, not a point estimate.

### Low Dimensional Search (Grid)

Just like we did in the previous chapter on $\mathcal{L}(M|D)$ without priors, we can find parameter values with grid search. However, we should acknowledge two major differences:

1) Previously we did grid search to find the parameter with the maximum likelihood. Now we do grid search to find the distribution of likelihoods for the parameter $(P(D)$. The results will include a maximum likelihood value of the parameter, but also other lower likelihood values of the parameter.
2) We multiply the values found in the grid search by our prior belief in the parameter values ($P(\theta)$). If we do not have a strong prior belief, we can choose values that have little to no effect.

**NOTE TO SELF, NEED TO UNIFY THE LIKELIHOOD AND PROB NOTATION**

### High Dimensional Search

Like before, grid search does not scale well to many parameters/dimensions. To solve this previously we introduced Gradient Ascent, which uses slopes/gradients to guide us to the maximum value.

There's a major problem, however, with using Gradient Ascent in Bayesian analysis - we do not want to find just the most likely value of the parameter - we want to find the correct probability distribution for the parameter (the likelihood of any value the parameter could take).

#### Markov Chain Monte Carlo (MCMC)

The solution to this is to use random sampling methods, such that we search through the possible combinations of the parameters and see how likely they are. As the algorithm starts, we can think of it a little like grid search - we check some combination of parameters and calculate a likelihood for those parameters based on how well they fit the data. The problem is that in high dimensional space, the extreme majority of the parameter combinations will have virtually zero likelihood. (Picture a model with random parameter values generating data that isn't even close to what we observe).

To solve the problem of constantly testing parameters that have virtually zero likelihood, modern sampling algorithms know to head towards areas of higher likelihood. However, importantly they do not simply head towards the peak like they would in Gradient Ascent - instead they strategically wander around the edges and peaks of the high likelihood space, mapping out those regions like a cartographer mapping a mountain ridge with points from a transit.

#### Variational Inference (VI)

Hamiltonian Monte Carlo based methods such as the No U-Turn Sampler (NUTS) are the gold standard, but there's a major disadvantage - they are slow. And to make that clearer - even on modern computers they are slow. One option to speed things up is to make the problem more like the gradient ascent described in the previous chapter. This may assume that the posterior distribution can be described by a multi-variate gaussian (a multi-dimensionsal normal distribution), which means if you can approximate its mean and variance you're done. This is a lot faster than describing the shape with thousands of individual points. There's a major drawback however, which is the same as we've tried to warn about elsewhere, that not everything is in the shape of the normal distribution.

### Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori (MAP)

In machine learning, MAP (Maximum A Posteriori) estimation is a Bayesian approach to parameter estimation. It finds the most probable parameter values given the observed data and a prior distribution.

Mathematically, it maximizes the posterior probability:

$$
\theta_{MAP} = \arg\max_{\theta} P(\theta | D)
$$

Using Bayes’ theorem:

$$
P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
$$

Since  P(D)  is constant for optimization, MAP simplifies to:

$$
\theta_{MAP} = \arg\max_{\theta} P(D | \theta) P(\theta)
$$

This contrasts with Maximum Likelihood Estimation (MLE), which maximizes only  P(D | \theta).

## Parameter Finding App

To get a feel for what it's like to search the parameter space for parameter combinations with a high likelihood, play around with the app below.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 750

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Parameter Estimation"),
    ui.row(
        ui.column(3, 
            ui.input_select("search_method", "Search Method:", 
                choices=["Grid Search", "Likelihood Search"], selected="Grid Search"),
            ui.input_select("num_samples", "Number of Samples:", 
                choices=["10", "100", "1000"], selected="100"),
            ui.input_action_button("generate", "Generate Samples", class_="btn-primary"),
            ui.p(""),
            ui.input_checkbox("show_samples", "Show sample points", value=True)
        ),
        ui.column(9,
            ui.output_plot("likelihoodPlot", height="500px")
        )
    )
)

def server(input, output, session):
    # Store the current samples
    samples = reactive.Value(None)
    
    @reactive.effect
    @reactive.event(input.generate)
    def handle_generate():
        try:
            n = int(input.num_samples())
            search_method = input.search_method()
            
            # True parameters
            true_x, true_y = 0.6, 0.4
            
            if search_method == "Grid Search":
                # Create evenly spaced grid
                grid_size = int(np.sqrt(n))
                x_grid = np.linspace(0, 1, grid_size)
                y_grid = np.linspace(0, 1, grid_size)
                X, Y = np.meshgrid(x_grid, y_grid)
                theta1 = X.flatten()[:n]  # Ensure exactly n samples
                theta2 = Y.flatten()[:n]
                
                # Add small noise to make it look more realistic
                theta1 += 0.01 * np.random.randn(len(theta1))
                theta2 += 0.01 * np.random.randn(len(theta2))
                
                # Ensure values stay within bounds
                theta1 = np.clip(theta1, 0, 1)
                theta2 = np.clip(theta2, 0, 1)
                
            else:  # Likelihood Search
                # First, generate random samples across the parameter space
                theta1_initial = np.random.uniform(0, 1, n*2)  # Generate extra samples
                theta2_initial = np.random.uniform(0, 1, n*2)
                
                # Calculate likelihood based on distance from true parameters
                # (simulate a likelihood function with a peak at true parameters)
                likelihoods = np.exp(-10 * ((theta1_initial - true_x)**2 + (theta2_initial - true_y)**2))
                
                # Sample from these points with probability proportional to likelihood
                selection_probs = likelihoods / np.sum(likelihoods)
                selected_indices = np.random.choice(len(theta1_initial), size=n, p=selection_probs, replace=False)
                
                theta1 = theta1_initial[selected_indices]
                theta2 = theta2_initial[selected_indices]
            
            samples.set((theta1, theta2))
        except Exception as e:
            print(f"Error generating samples: {e}")
            samples.set(None)

    @output
    @render.plot
    def likelihoodPlot():
        # Create basic plot
        fig, ax = plt.subplots(figsize=(8, 6))
        
        # Get current samples
        current_samples = samples.get()
        
        if current_samples is None:
            ax.text(0.5, 0.5, "Click 'Generate Samples' to create data", 
                   ha='center', va='center', fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            return fig
        
        theta1, theta2 = current_samples
        n = len(theta1)
        
        try:
            # Create a 2D histogram
            H, xedges, yedges = np.histogram2d(
                theta1, theta2, 
                bins=20, 
                range=[[0, 1], [0, 1]]
            )
            
            # Use pcolormesh for better rendering
            pcm = ax.pcolormesh(xedges, yedges, H.T, cmap='viridis')
            
            # Add colorbar
            plt.colorbar(pcm, ax=ax, label='Count')
            
            # Plot the samples if requested
            if input.show_samples():
                ax.scatter(theta1, theta2, s=5, color='red', alpha=0.3, label=f'Samples (n={n})')
            
            # Mark true parameters
            ax.plot(0.6, 0.4, 'y*', markersize=15, markeredgecolor='black', label='True parameters')
            
            # Labels and layout
            ax.set_xlabel('Parameter θ₁')
            ax.set_ylabel('Parameter θ₂')
            
            search_method = input.search_method().split()[0]
            ax.set_title(f'Parameter Estimation using {search_method} ({n} samples)')
            
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.legend(loc='upper right')
            
        except Exception as e:
            print(f"Error plotting: {e}")
            ax.text(0.5, 0.5, f"Error creating plot: {str(e)}", 
                   ha='center', va='center', fontsize=12)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
        
        return fig

app = App(app_ui, server)
```


## Conclusion

As stated in the title, this is intended to be a 'primer' to introduce you to the topic. We hope you have some sense for the important foundations of statistics. We encourage you to learn by experimenting - creating your own data and examples will be the fastest path to understanding for the majority of people.

There are also many excellent texts in both statistics and machine learning, hopefully with some intuition on the basics, the rationale and math will now be much easier to understand.