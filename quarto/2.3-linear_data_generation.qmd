---
title: Linear Data Generation
format: html
filters:
  - shinylive
---

## Intro 

At some point you have probably been exposed to linear regression... You may roll your eyes at being asked to warp back to your early education... But linear regression, and it's generalized cousins, are an incredibly useful set of techniques in themselves, and *truly* understanding them is an enormous boon for understanding the entire suite of machine learning methods.

We're also going to approach this in a way that is likely backwards from your previous introduction. Typically you'll be presented with some data and the task will be to find a best fit line through the data. Often this starts with a single explanatory variable, but can it can easily be extended to many more. The actual algorithm of the best fit line is usually hand waved as some sort of magic - tackling that magic, i.e. finding the data generating process that best fits the data, is saved for the second half of this primer. Here we will think about it in the forward direction, if we already have a linear regression model, what data does it generate? And how likely were we to see that data once generated?


```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Linear Regression Likelihood"),

    # Row 1: Sliders for alpha, beta, sigma^2, and n
    ui.row(
        ui.column(
            3,
            ui.input_slider(
                "alphaInput", "Intercept (α):",
                min=-10, max=10, value=0, step=0.1
            ),
        ),
        ui.column(
            3,
            ui.input_slider(
                "betaInput", "Slope (β):",
                min=-5, max=5, value=1, step=0.1
            ),
        ),
        ui.column(
            3,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=0.1, max=10, value=1, step=0.1
            ),
        ),
        ui.column(
            3,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=5, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Buttons, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("mleBtn", "MLE"),
        ),
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            4,
            ui.h4("Current Data (X, Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            4,
            ui.h4("Log-Likelihood:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("regressionPlot", height="400px"),
)

def server(input, output, session):
    # Reactive value to store X and Y
    data_vals = reactive.Value(None)

    # Function to generate linear-regression data
    def generate_data(n, alpha, beta, var):
        # For simplicity, let X be a random uniform(0, 10)
        X = np.random.uniform(0, 10, size=n)
        # Y = alpha + beta*X + noise
        Y = alpha + beta*X + np.random.normal(0, np.sqrt(var), size=n)
        return X, Y

    # Initialize data once
    data_vals.set(generate_data(10, 0, 1, 1))

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()
        data_vals.set(generate_data(n, alpha, beta, var))

    # Display the current data
    @output
    @render.text
    def dataText():
        X, Y = data_vals()
        # Show a few decimal places
        pairs_str = [
            f"({round(xi,1)}, {round(yi,1)})" for xi, yi in zip(X, Y)
        ]
        return ", ".join(pairs_str)

    # When 'MLE' is clicked, compute OLS estimates and update alpha, beta, var
    @reactive.Effect
    @reactive.event(input.mleBtn)
    def _():
        X, Y = data_vals()
        n = len(Y)

        # Compute MLE (which in classical linear regression is the OLS solution)
        X_mean = np.mean(X)
        Y_mean = np.mean(Y)

        # beta_hat = Cov(X,Y)/Var(X)
        beta_hat = np.sum((X - X_mean)*(Y - Y_mean)) / np.sum((X - X_mean)**2)

        # alpha_hat = mean(Y) - beta_hat*mean(X)
        alpha_hat = Y_mean - beta_hat*X_mean

        # var_hat = (1/n) * sum((y_i - alpha_hat - beta_hat*x_i)^2)
        residuals = Y - (alpha_hat + beta_hat*X)
        var_hat = np.sum(residuals**2) / n

        # Update the UI sliders
        session.send_input_message("alphaInput", {"value": alpha_hat})
        session.send_input_message("betaInput", {"value": beta_hat})
        session.send_input_message("varInput", {"value": var_hat})

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        X, Y = data_vals()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()
        n = len(Y)

        if var <= 0:
            return float("nan")

        # Compute sum of squared residuals
        residuals = Y - (alpha + beta*X)
        ssr = np.sum(residuals**2)

        # log-likelihood for linear regression
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * (ssr / var)
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the data and the regression line
    @output
    @render.plot
    def regressionPlot():
        X, Y = data_vals()
        alpha = input.alphaInput()
        beta = input.betaInput()
        var = input.varInput()

        fig, ax = plt.subplots(figsize=(6, 4))

        # Plot data points
        ax.scatter(X, Y, color="blue", alpha=0.7, label="Data")

        # Plot regression line from min(X) to max(X)
        x_min, x_max = np.min(X), np.max(X)
        x_vals = np.linspace(x_min, x_max, 100)
        y_vals = alpha + beta * x_vals
        ax.plot(x_vals, y_vals, color="red", label=f"Line (α={round(alpha,2)}, β={round(beta,2)})")

        ax.set_title("Linear Regression Fit")
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.legend()
        ax.grid(True)

        return fig

app = App(app_ui, server)
```


## Data Generation

The great thing about data generation with a model is that it is going to show you *what it knows* and not *what you think it knows*. Let's walk through an example with a common dataset made available with the R package ggplot2, the diamonds dataset.

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
file_path = "../datasets/diamonds.csv"
diamonds = pd.read_csv(file_path)
diamonds = diamonds[(diamonds['cut'] == 'Premium') & (diamonds['color'] == 'E')]


# Plot the relationship between carat and price
plt.figure(figsize=(10, 6))
sns.scatterplot(data=diamonds, x='carat', y='price', alpha=0.6)
plt.title('Relationship between Carat and Price of Diamonds')
plt.xlabel('Carat')
plt.ylabel('Price')
plt.grid(True)
plt.show()
```


```{python}
import statsmodels.api as sm

# Add a constant term to the predictor variable (for the intercept)
X = sm.add_constant(diamonds['carat'])
y = diamonds['price']

# Fit the linear regression model
model = sm.OLS(y, X).fit()

# Display the summary of the regression results
print(model.summary())

```



```{python}
import numpy as np

# Define model parameters based on regression results
intercept = -2564.4751
slope = 8503.5628
residual_std_dev = model.resid.std()  # Standard deviation of residuals

# Generate new data points for carat
X_new = np.linspace(0.2, 3, 1000)  # Example carat values

# Simulate random noise
random_noise = np.random.normal(loc=0, scale=residual_std_dev, size=len(X_new))

# Generate new price values with variability
Y_new = intercept + slope * X_new + random_noise

# Create a DataFrame to store the new data
new_data = pd.DataFrame({'carat': X_new, 'price': Y_new})

# Visualize the simulated data
plt.figure(figsize=(10, 6))
sns.scatterplot(x=new_data['carat'], y=new_data['price'], alpha=0.6)
plt.title('Simulated Data Based on Linear Regression')
plt.xlabel('Carat')
plt.ylabel('Price')
plt.grid(True)
plt.show()
```


##
OK, so we got all this fancy output, including a decently high R-squared value of 0.843. Since an R-squared value of 0 is a model that explains none of the variability in the result, and a value of 1 is that it explains all the variability, we conclude we've done a fairly good job at 0.843.



```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the dataset
file_path = "../datasets/diamonds.csv"
diamonds = pd.read_csv(file_path)
diamonds = diamonds[(diamonds['cut'] == 'Premium') & (diamonds['color'] == 'E')]

# Plot 1: Relationship between Carat and Price
intercept = -2564.4751
slope = 8503.5628
residual_std_dev = 3000  # Assumed value for demonstration; replace with actual std_dev

# Generate new data points for carat
X_new = np.linspace(0.2, 3, 1000)  # Example carat values

# Simulate random noise
random_noise = np.random.normal(loc=0, scale=residual_std_dev, size=len(X_new))

# Generate new price values with variability
Y_new = intercept + slope * X_new + random_noise

# Create a DataFrame to store the new data
new_data = pd.DataFrame({'carat': X_new, 'price': Y_new})

# Plot both side by side
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Original Data
axes[0].scatter(diamonds['carat'], diamonds['price'], alpha=0.6, color='blue')
axes[0].set_title('Original Data: Carat vs Price')
axes[0].set_xlabel('Carat')
axes[0].set_ylabel('Price')
axes[0].grid(True)

# Simulated Data
axes[1].scatter(new_data['carat'], new_data['price'], alpha=0.6, color='orange')
axes[1].set_title('Simulated Data: Carat vs Price')
axes[1].set_xlabel('Carat')
axes[1].set_ylabel('Price')
axes[1].grid(True)

plt.tight_layout()
plt.show()
```

OK, let's point out a few things this model is doing wrong (and by the way, this is much easier to see when generating data):
-The model produces a decent number of negative values at small carats. I don't think this is real because noone has paid me to take a diamond before.
-The variation around the mean value is the same regardless of whether it is a 0.3 carat diamond or a 1.5 carat diamond.

The second one listed about is *Heteroscedasticity*, one of the funnest words in the english language.


Observations about what's wrong, heteroscadicity, negative values...


Now you're thinking, well you just didn't use a complicated enough model... Maybe throw a neural network at it. And while you aren't all wrong - you ware also on the path to the dark side.



## More Models

We are gradually making more 'bespoke' models... There tend to be common types of models because they can be applied so generally, these include the generalized linear models and other 'machine learning' types of models like decision trees. However, there's nothing in statistics that limits us to just these... If we have a specialized case, like modeling the motion of a spring, we can generate a specialized model.