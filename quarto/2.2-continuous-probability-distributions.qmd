---
title: Continuous Probability Distributions
format: html
filters:
  - shinylive
---

## Preview

We continue with $P(D|M)$, the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line $\mathbb{R}$ is effectively zero.

Like we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.

## Models of Continuous Data Generating Processes

Like we said before - when we change the data generating process, the distribution of outcomes changes. Here we'll examine data generating processes that create continuous data.

### The Random ~~Walk~~ Rocket

Let's assume we are shooting a rocket into the sky and letting it land. We have designed a simple guidance system that will correct the rocket to fly vertically after deviating from vertical flight. However, before a correction, the rocket will have wandered slightly from its original launch point. For simplicity in modeling, we'll assume it only wanders left and right. We also assume that the deviations in flight are totally random - i.e. there is not a tendency to always move in one of the two directions. We want to answer the question, how far away is our rocket likely to land?

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Random Rocket Simulator"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("stepSize", "Deviation Size", 
                           min=0.01, max=10, value=1, step=0.01),
            ui.input_slider("numSteps", "Number of Deviations", 
                           min=10, max=1000, value=100, step=10),
            ui.input_slider("numTrials", "Number of Trials", 
                           min=10, max=10000, value=100, step=10),
            ui.input_slider("numBins", "Number of Histogram Bins", 
                           min=5, max=100, value=30, step=5),
        ),
        ui.navset_tab(
            ui.nav_panel("Histogram",
                ui.output_plot("distPlot", height="400px"),
            ),
            ui.nav_panel("Percentile Plot",
                ui.output_plot("percentilePlot", height="400px"),
            ),
        ),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def calculate_distances():
        distances = []
        for _ in range(input.numTrials()):
            steps = np.random.uniform(0, 1, input.numSteps()) * input.stepSize()
            final_position = np.sum(steps)
            distances.append(final_position)
        return distances

    @output
    @render.plot
    def distPlot():
        distances = calculate_distances()
        fig, ax = plt.subplots()
        
        ax.hist(distances, bins=input.numBins(), color="steelblue", edgecolor="black")
        ax.set_title("Distribution of Distances from Launch")
        ax.set_xlabel("Distance from Start")
        ax.set_ylabel("Frequency")

        return fig

    @output
    @render.plot
    def percentilePlot():
        distances = calculate_distances()
        fig, ax = plt.subplots()
        
        # Calculate percentiles
        sorted_distances = np.sort(distances)
        percentiles = np.linspace(0, 100, len(distances))
        
        ax.plot(sorted_distances, percentiles, color="steelblue")
        ax.set_title("Percentile Plot of Distances")
        ax.set_xlabel("Distance from Start")
        ax.set_ylabel("Percentile")
        ax.grid(True)

        return fig

app = App(app_ui, server)
```

Even though this is similar to some other outputs we've seen, we need to stress a few key points:

-   The output values are no longer discrete/integers as we saw previously.
-   The app now lets choose the size of the bin since there is no 'right' answer to the interval used.
-   There is now a *percentile plot* option for the type of graph generated.
-   The percentile plot shows, in a cumulative fashion, what percent of distances are closer to the start.

::: callout-note
The percentile plot is a foreshadowing of another way to compute continuous probability distributions, called the cumulative distribution function. This approach can remove some confusing properties of continuous probability distributions, however, it tends to make the shape of the distribution harder to interpret as many distributions tend to look similar.
:::

### Continuous Distributions as Models of Data Generating Processes

The real world is full of data generating processes, each of which has a probability distribution, and very few, if any, would match the named probability distributions used constantly in statistics. That doesn't mean the named distributions aren't useful - they are extremely useful approximations due to their ability to model common processes and calculate $P(D|M)$ easily.

#### The Normal Distribution

You may have spotted a trend in the Random Rocket example, as well as a lot of our earlier examples. Whenever we increased the number of samples, the histograms started to look an awful lot like the well-known normal/gaussian distribution. There's actually a theorem for that, called the the *Central Limit Theorem*. You can easily research the details, but we'll summarize it by saying that any data generating process that is additive in nature tends to produce normal/gaussian distributions. And *almost* everything we've seen so far has utilized additive processes.

To restate slightly from our perspective of models of data generating processes, the Normal distribution takes additive processes to the limit, in which they generate the perfect 'Bell Curve'. Normal distributions are a good approximation for modeling the variability/variation in many things. An example is modeling height - but beware it is the logical extreme - real heights cannot be perfectly Normal, because they cannot have negative values.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.stats import norm

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Simulation with Binned Histogram"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "mean", 
                "Mean (μ)", 
                min=-100.0, 
                max=100.0, 
                value=0.0, 
                step=0.1
            ),
            ui.input_slider(
                "stddev", 
                "Standard Deviation (σ)", 
                min=0.1, 
                max=50.0, 
                value=1.0, 
                step=0.1
            ),
            ui.input_slider(
                "num_trials",
                "Number of Trials",
                min=100,
                max=10000,
                value=1000,
                step=100
            ),
            ui.input_slider(
                "num_bins",
                "Number of Bins",
                min=10,
                max=100,
                value=30,
                step=5
            ),
        ),
        ui.output_plot("normPlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def normal_samples():
        mu = input.mean()
        sigma = input.stddev()
        size = input.num_trials()
        return np.random.normal(mu, sigma, size)

    @output
    @render.plot
    def normPlot():
        samples = normal_samples()
        
        # Use the number of bins from the slider
        num_bins = input.num_bins()
        
        # Compute histogram (both count and density)
        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)
        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)
        bin_width = bin_edges[1] - bin_edges[0]
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Create the second y-axis sharing the same x-axis
        ax2 = ax1.twinx()
        
        # Plot histogram with counts on left y-axis
        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color="steelblue", 
                      alpha=0.6, edgecolor="black", align='center', 
                      label=f'Histogram (n={input.num_trials():,})')
        
        # Calculate theoretical normal distribution
        x = np.linspace(min(samples), max(samples), 100)
        pdf = norm.pdf(x, input.mean(), input.stddev())
        
        # Plot theoretical curve on right y-axis
        line = ax2.plot(x, pdf, 'r-', lw=2, label='Normal PDF')[0]
        
        # Set labels and title
        ax1.set_xlabel("Value", fontsize=14)
        ax1.set_ylabel("Count", fontsize=12, color='steelblue')
        ax2.set_ylabel("Density", fontsize=12, color='red')
        plt.title("Normal Distribution: Histogram and PDF", fontsize=16)
        
        # Color the tick labels to match the respective plots
        ax1.tick_params(axis='y', labelcolor='steelblue')
        ax2.tick_params(axis='y', labelcolor='red')
        
        # Ensure both axes start at 0
        ax1.set_ylim(bottom=0)
        ax2.set_ylim(bottom=0)
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers], rotation=90)
        
        # Add legends for both axes
        lines = [bars, line]
        labels = [b.get_label() for b in lines]
        ax1.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```

We've also included the exact values of the *probability density function* (PDF) to suggest that the underlying distribution is continuous.

#### The Lognormal Distribution

Data generating processes do not need to be additive though, some processes tend to multiply. These kinds of processes will create a notably different distribution, called the log-normal distribution. It has two important differences from the normal distribution:

-   It contains only positive values.
-   It has a very long 'tail' on the right hand side. Another way to describe this is skewness.

It's worth noting that there are many real world problems where values can only be positive. This simple fact also implies that many real distributions skew towards larger values.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.stats import lognorm

app_ui = ui.page_fluid(
    ui.h2("Log-Normal Distribution Simulation with Binned Histogram"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "mu", 
                "Log-mean (μ)", 
                min=-2.0, 
                max=2.0, 
                value=0.0, 
                step=0.1
            ),
            ui.input_slider(
                "sigma", 
                "Log-standard deviation (σ)", 
                min=0.1, 
                max=2.0, 
                value=0.5, 
                step=0.1
            ),
            ui.input_slider(
                "num_trials",
                "Number of Trials",
                min=100,
                max=10000,
                value=1000,
                step=100
            ),
            ui.input_slider(
                "num_bins",
                "Number of Bins",
                min=10,
                max=100,
                value=30,
                step=5
            ),
        ),
        ui.output_plot("lognormPlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def lognormal_samples():
        mu = input.mu()
        sigma = input.sigma()
        size = input.num_trials()
        return np.random.lognormal(mu, sigma, size)

    @output
    @render.plot
    def lognormPlot():
        samples = lognormal_samples()
        
        # Use the number of bins from the slider
        num_bins = input.num_bins()
        
        # Compute histogram (both count and density)
        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)
        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)
        bin_width = bin_edges[1] - bin_edges[0]
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Create the second y-axis sharing the same x-axis
        ax2 = ax1.twinx()
        
        # Plot histogram with counts on left y-axis
        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color="steelblue", 
                      alpha=0.6, edgecolor="black", align='center', 
                      label=f'Histogram (n={input.num_trials():,})')
        
        # Calculate theoretical log-normal distribution
        x = np.linspace(min(samples), max(samples), 1000)
        pdf = lognorm.pdf(x, input.sigma(), scale=np.exp(input.mu()))
        
        # Plot theoretical curve on right y-axis
        line = ax2.plot(x, pdf, 'r-', lw=2, label='Log-Normal PDF')[0]
        
        # Set labels and title
        ax1.set_xlabel("Value", fontsize=14)
        ax1.set_ylabel("Count", fontsize=12, color='steelblue')
        ax2.set_ylabel("Density", fontsize=12, color='red')
        plt.title("Log-Normal Distribution: Histogram and PDF", fontsize=16)
        
        # Color the tick labels to match the respective plots
        ax1.tick_params(axis='y', labelcolor='steelblue')
        ax2.tick_params(axis='y', labelcolor='red')
        
        # Ensure both axes start at 0
        ax1.set_ylim(bottom=0)
        ax2.set_ylim(bottom=0)
        
        # Set x-axis limits to focus on the main part of the distribution
        upper_limit = np.percentile(samples, 99)  # Show up to 99th percentile
        ax1.set_xlim(0, upper_limit)
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers], rotation=90)
        
        # Add legends for both axes
        lines = [bars, line]
        labels = [b.get_label() for b in lines]
        ax1.legend(lines, labels, loc='upper right')
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```

You may have guessed that if you have a set of values from the lognormal distribution, if you take the the log of their values and replot them, you'll end up plotting values in a normal distribution.

#### Summary

Again we keep this section brief as there are plenty of easily accessible references for continuous probability distributions. Hopefully the point was made though - that each continuous probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcome of the process.

::: callout-note
We've touched on this briefly, but we should explicitly acknowledge that the named probability distributions can be used in different ways:

-   Generate data from a known distribution.
-   Calculate the probability of data that has come from a known distribution.
-   Communiciate uncertainty in the value of a parameter.

We haven't seen the third bullet yet, it will eventually come up in the second half of the primer. But it is fundamentally different than the other two in that it represents *knowledge*, and with additional *knowledge* the uncertainty can become zero. This is not the same as the inherent variation in statistical processes, for example, the variation in peoples height will not become zero with more knowledge.
:::

## Probability of Data

We return to the main theme of the first half of the primer, the probability of data given a model, $P(D|M)$.

### Probability *Density* Function

We've been hinting that the probability density function would require some explanation, and we've finally come to the right place to tackle it. We've created histograms of our continuous data, in which we take multiple exact values and lump them together in a bin of the histogram. It's possible to use our relative frequency technique to estimate the probability of the bin (just divide the bin count by the total count of all bins). However, how would estimate the probability of a single point within the bin? The problem is that, as we've noted earlier, the probability of any exact value on the real number line $\mathbb{R}$ is effectively zero.

To solve this, the probability density function does not give a true probability, it gives a value such that the following properties are true:

* The area under the curve sums to 1, i.e. all possible values have a total probability of 1.
* The values give the relative probability of that point vs other points.

### Single Data Point Example

As already mentioned, referring to the probability of an exact value on the real number line is meaningless. Instead, we refer to either the relative probability of a value, or the probability of getting a value as extreme or more extreme than the value that we observed. With a single data point we use an example of the 'as extreme or more extreme' approach.

In the app below, we find where the data point lies on the chart and then find the area under all larger values. The area is equal to the probability of sampling points larger than the observation. This is utilizing bullet one from the properties of the probability density function given above. Note that we can get this value more directly if we work from the cumulative distribution, as shown in the other plot tab.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

import math
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Probability Calculator"),
    
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
            ui.input_numeric(
                "xInput", "Value (x):",
                value=105, min=0, max=200
            ),
            ui.br(),
            ui.h4("Probability (P(X ≥ x)):"),
            ui.output_text("probOutput"),
            width=300
        ),
        
        # Main panel with tabset
        ui.navset_tab(
            ui.nav_panel("PDF Plot",
                ui.output_plot("normalPlot", height="400px"),
            ),
            ui.nav_panel("CDF Plot",
                ui.output_plot("cdfPlot", height="400px"),
            ),
        ),
    )
)

def server(input, output, session):
    # Calculate probability
    @reactive.Calc
    def calculate_probability():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)
        return 1 - stats.norm.cdf(x, mu, sigma)

    # Show the probability
    @output
    @render.text
    def probOutput():
        prob = calculate_probability()
        return f"{prob:.4f}"

    # Plot the normal PDF with shaded area
    @output
    @render.plot
    def normalPlot():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)

        x_min = mu - 4 * sigma
        x_max = mu + 4 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)

        fig, ax = plt.subplots(figsize=(10, 6))
        
        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')
        
        x_shade = x_vals[x_vals >= x]
        y_shade = stats.norm.pdf(x_shade, mu, sigma)
        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, 
                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')

        ax.axvline(x, color='red', linestyle='--', alpha=0.5)

        ax.set_title(f"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})")
        ax.set_xlabel("X")
        ax.set_ylabel("Density")
        ax.legend()
        ax.grid(True, alpha=0.3)

        return fig

    # Plot the CDF
    @output
    @render.plot
    def cdfPlot():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)

        # Generate random data
        data = np.random.normal(mu, sigma, 1000)
        
        # Calculate empirical CDF
        sorted_data = np.sort(data)
        empirical_cdf = np.arange(1, len(data) + 1) / len(data)
        
        # Calculate theoretical CDF
        x_vals = np.linspace(mu - 4*sigma, mu + 4*sigma, 200)
        theoretical_cdf = stats.norm.cdf(x_vals, mu, sigma)

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot empirical and theoretical CDFs
        ax.plot(sorted_data, empirical_cdf, 'b-', label='Empirical CDF', alpha=0.7)
        ax.plot(x_vals, theoretical_cdf, 'r--', label='Theoretical CDF')
        
        # Add vertical line at x
        ax.axvline(x, color='red', linestyle='--', alpha=0.5)
        
        ax.set_title(f"Cumulative Distribution Function (μ={mu:.1f}, σ²={var:.1f})")
        ax.set_xlabel("X")
        ax.set_ylabel("Cumulative Probability")
        ax.legend()
        ax.grid(True, alpha=0.3)

        return fig

app = App(app_ui, server)
```

When we refer to probabilities of "as extreme or more extreme" values, we are effectively computing tail probabilities using integrals. For example, for a right-tailed probability beyond a point $c$, we evaluate:

$$
P(X \geq c) = \int_c^\infty f(x) \, dx
$$

Those who prefer to avoid calculus (and what engineer doesn't?), can simply utilize the cumulative (CDF) plot.

### Multiple Data Points Example

#### Relative Probability

For multiple data points, we are interested in the *relative* probability of one series of events vs some other series of events. This is utilizing bullet two from the properties of the probability density function given above. In the example below we will multiply the probability densities similar to how we multiplied actual probabilities in the previous chapter on discrete probability distributions - this should given you reason to pause - the reason we can do this is that we will only use the results for relative comparison, and *not* as an absolute probability.

Again, we assume that in the series of events each event is independent. And again, this typically cannot be proven to be strictly true - however avoid the cases where it is obviously *not true*, such as time series. Also, we use only log probabilities and addition (in contrast with the more obvious multiplication of non-log probabilities) as they are more convenient.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Relative Probability of Normally Distributed Data"),

    # Row 1: Sliders and New Sample button
    ui.row(
        ui.column(
            5,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            5,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            2,
            ui.br(),
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
    ),

    ui.br(),

    # Row 2: All data and probabilities
    ui.row(
        ui.column(
            4,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            5,
            ui.h4("Log Relative Probabilities:"),
            ui.output_text_verbatim("pointLogProbs"),
        ),
        ui.column(
            3,
            ui.h4("Log Sum:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 5 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=5)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=5)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # Calculate log probability for each point
    @reactive.Calc
    def point_log_probs():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        if var <= 0:
            return [float("nan")] * len(y)
        
        log_probs = []
        for yi in y:
            term1 = -0.5 * math.log(2 * math.pi * var)
            term2 = -0.5 * ((yi - mu)**2) / var
            log_probs.append(term1 + term2)
        return log_probs

    # Display individual log probabilities
    @output
    @render.text
    def pointLogProbs():
        probs = point_log_probs()
        return ", ".join(f"{p:.2f}" for p in probs)

    # Reactive expression for total log-likelihood
    @reactive.Calc
    def log_likelihood():
        return sum(point_log_probs())

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```

As you adjust the parameters of the probability distribution, the relative probability of seeing the data changes. When the probability distribution is far away from the points, we see the sum of the probabilities becomes smaller (less likely). When the probability distribution is near the points, the sum gets larger, showing it is comparitively more likely.

::: callout-warning
I've reserved the use of *likelihood* until the second half of this primer during which we'll find the best model based on the data from the data generating process. However, in other texts you will also find descriptions of the relative probability of continuous distributions called the relative likelihood.
:::

#### Approximate P-Value

In the following app, there will be 100 samples of a series of 10 events. Each of those will have been generated from a Normal distribution with a mean of 100 and a variance of 10. You can then create your own series of 10 events, sampled from a Normal distribution with a mean and variance of your choosing. The relative probability of your series of 10 events will be calculated as if they had come from a distribution with a mean of 100 and a variance of 10. The percentile shown will indicate how unusual your series of events appears to be. 

Like the similar app for discrete distributions, this is an approximation of your data sets p-value. If your dataset is in a reasonable percentile, you have little evidence to assume it was not created by the *null* model with mean of 100 and variance of 10. If it is at an extreme percentile, you may reasonably suspect it was generated by a different data generating process.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 800

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Approximate P-Value Calculator"),
    ui.p("Generate your own series of 10 events from a Normal distribution with your chosen mean and variance. The probability will be calculated assuming the events came from a Normal(μ=100, σ²=10) distribution."),

    # Row 1: Sliders and New Sample button
    ui.row(
        ui.column(
            5,
            ui.input_slider(
                "muInput", "Your Mean (μ):",
                min=80, max=120, value=95, step=0.1
            ),
        ),
        ui.column(
            5,
            ui.input_slider(
                "varInput", "Your Variance (σ²):",
                min=1, max=100, value=20, step=1
            ),
        ),
        ui.column(
            2,
            ui.br(),
            ui.input_action_button("newSampleBtn", "NEW SAMPLES"),
        ),
    ),

    ui.br(),

    # Row 2: Data and percentile
    ui.row(
        ui.column(
            9,
            ui.h4("Your Data:"),
            ui.output_text_verbatim("dataText1"),
        ),
        ui.column(
            3,
            ui.h4("Percentile:"),
            ui.output_text("percentileOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("cumulativePlot", height="400px"),
)

def server(input, output, session):
    # Initialize user's data and reference datasets
    data_vals1 = reactive.Value(None)
    reference_data = reactive.Value(None)

    # Generate new samples when parameters change or button is pressed
    @reactive.Effect
    @reactive.event(input.muInput, input.varInput, input.newSampleBtn)
    def _():
        data_vals1.set(
            np.random.normal(loc=input.muInput(), scale=np.sqrt(input.varInput()), size=10)
        )
        reference_data.set(
            [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]
        )

    # Initial data generation
    @reactive.Effect
    def _():
        if data_vals1() is None:
            data_vals1.set(
                np.random.normal(loc=95, scale=np.sqrt(20), size=10)
            )
        if reference_data() is None:
            reference_data.set(
                [np.random.normal(loc=100, scale=np.sqrt(10), size=10) for _ in range(100)]
            )

    # Display the current data
    @output
    @render.text
    def dataText1():
        y = data_vals1()
        return ", ".join(str(round(val, 1)) for val in y)

    # Calculate cumulative log probabilities
    def calc_cum_log_probs(data, mu=100, var=10):  # Default parameters set to true distribution
        cum_probs = []
        running_sum = 0
        
        for yi in data:
            term1 = -0.5 * math.log(2 * math.pi * var)
            term2 = -0.5 * ((yi - mu)**2) / var
            running_sum += (term1 + term2)
            cum_probs.append(running_sum)
            
        return cum_probs

    # Calculate and show the percentile
    @output
    @render.text
    def percentileOutput():
        user_final_prob = calc_cum_log_probs(data_vals1())[-1]
        ref_final_probs = [calc_cum_log_probs(ref_data)[-1] 
                          for ref_data in reference_data()]
        percentile = sum(1 for x in ref_final_probs if x < user_final_prob) / len(ref_final_probs) * 100
        return f"{percentile:.1f}%"

    # Plot the cumulative log probabilities
    @output
    @render.plot
    def cumulativePlot():
        # Calculate probabilities for user's data
        user_probs = calc_cum_log_probs(data_vals1())
        
        # Calculate probabilities for reference data
        ref_probs_list = [calc_cum_log_probs(data) 
                         for data in reference_data()]

        events = range(1, 11)  # 10 events

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot reference lines in gray
        for ref_probs in ref_probs_list:
            ax.plot(events, ref_probs, 'gray', alpha=0.1, linewidth=1)

        # Plot user's line in red
        ax.plot(events, user_probs, 'r-', label='Your samples', 
                linewidth=2, alpha=1.0)

        # Add a dummy plot for the reference distribution legend
        ax.plot([], [], 'gray', alpha=0.5, label='100 samples from N(100,10)')

        ax.set_title("Cumulative Log Probability vs. Number of Events")
        ax.set_xlabel("Number of Events")
        ax.set_ylabel("Cumulative Log Probability")
        ax.grid(True, alpha=0.3)
        ax.legend()

        return fig

app = App(app_ui, server)
```

