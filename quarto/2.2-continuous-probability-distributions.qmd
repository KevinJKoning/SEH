---
title: Continuous Probability Distributions
format: html
filters:
  - shinylive
---

## Preview

We continue with $P(D|M)$, the probability of the data given a model of a data generating process. Here we shift to models that produce continuous data (as opposed to discrete). Many of the same concepts apply, however there is a major wrinkle, in that the probability of any exact value on the real number line $\mathbb{R}$ is effectively zero.

Like we did for discrete probability distributions, we will touch on data generating models, the probability of a single event from the model, and the probability of multiple events from the model. However, since we are in a hurry, we will be briefer if the concept is similar to last section.

## Models of Continuous Data Generating Processes

Like we said before - when we change the data generating process, the distribution of outcomes changes. Here we'll examine data generating processes that create continuous data.

### The Random ~~Walk~~ Rocket

Let's assume we are shooting a rocket into the sky and letting it land. We have designed a simple guidance system that will correct the rocket to fly vertically after deviating from vertical flight. However, before a correction, the rocket will have wandered slightly from its original launch point. For simplicity in modeling, we'll assume it only wanders left and right. We also assume that the deviations in flight are totally random - i.e. there is not a tendency to always move in one of the two directions. We want to answer the question, how far away is our rocket likely to land?


```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt

app_ui = ui.page_fluid(
    ui.h2("Random Rocket Simulator"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider("stepSize", "Deviation Size", 
                           min=0.01, max=10, value=1, step=0.01),
            ui.input_slider("numSteps", "Number of Deviations", 
                           min=10, max=1000, value=100, step=10),
            ui.input_slider("numTrials", "Number of Trials", 
                           min=10, max=10000, value=100, step=10),
            ui.input_slider("numBins", "Number of Histogram Bins", 
                           min=5, max=100, value=30, step=5),
        ),
        ui.output_plot("distPlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def calculate_distances():
        distances = []
        for _ in range(input.numTrials()):
            # Generate random steps between 0 and 1, then scale by step size
            steps = np.random.uniform(0, 1, input.numSteps()) * input.stepSize()
            # Calculate final position (sum of all steps)
            final_position = np.sum(steps)
            # Store the absolute distance from start
            distances.append(final_position)
        return distances

    @output
    @render.plot
    def distPlot():
        distances = calculate_distances()
        fig, ax = plt.subplots()
        
        ax.hist(distances, bins=input.numBins(), color="steelblue", edgecolor="black")
        ax.set_title("Distribution of Distances from Launch")
        ax.set_xlabel("Distance from Start")
        ax.set_ylabel("Frequency")

        return fig

app = App(app_ui, server)
```

Even though this is similar to some other outputs we've seen, we need to stress a couple key points:

* The output values are no longer discrete/integers as we saw previously.
* It is still useful to visualize the results with a histogram - however, the app now lets choose the size of the bin since there is no 'right' answer to the interval used.

Previously the histograms you've seen had fairly obvious bins, they were just based on the discrete/integer results of our previous data generating processes. Now there is no obvious bin size to choose because the results are continuous.

::: callout-note
There is, at least in some respects, a more elegant way to visualize continuous data, which is to use the cumulative distribution of values based on their percentile rank. It starts at zero (nothing is smaller) and increases to one (where everything is smaller). There are a number of ways cumulative distributions can be helpful - however, they have a major downside in that many distributions tend to look the same when plotted this way.
:::

## Continuous Distributions as Models of Data Generating Processes

### The Normal Distribution

You may have spotted a trend in the Random Rocket example, as well as a lot of our earlier examples. Whenever we increased the number of samples, the histograms started to look an awful lot like the well-known normal/gaussian distribution. There's actually a theorem for that, call the the *Central Limit Theorem*. You can easily research the details, but we'll summarize it by saying that any data generating process that is additive in nature tends to produce normal/gaussian distributions. And *almost* everything we've seen so far has utilized additive processes.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.stats import norm

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Simulation with Binned Histogram"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "mean", 
                "Mean (μ)", 
                min=-100.0, 
                max=100.0, 
                value=0.0, 
                step=0.1
            ),
            ui.input_slider(
                "stddev", 
                "Standard Deviation (σ)", 
                min=0.1, 
                max=50.0, 
                value=1.0, 
                step=0.1
            ),
            ui.input_slider(
                "num_trials",
                "Number of Trials",
                min=100,
                max=10000,
                value=1000,
                step=100
            ),
            ui.input_slider(
                "num_bins",
                "Number of Bins",
                min=10,
                max=100,
                value=30,
                step=5
            ),
        ),
        ui.output_plot("normPlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def normal_samples():
        mu = input.mean()
        sigma = input.stddev()
        size = input.num_trials()
        return np.random.normal(mu, sigma, size)

    @output
    @render.plot
    def normPlot():
        samples = normal_samples()
        
        # Use the number of bins from the slider
        num_bins = input.num_bins()
        
        # Compute histogram (both count and density)
        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)
        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)
        bin_width = bin_edges[1] - bin_edges[0]
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Create the second y-axis sharing the same x-axis
        ax2 = ax1.twinx()
        
        # Plot histogram with counts on left y-axis
        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color="steelblue", 
                      alpha=0.6, edgecolor="black", align='center', 
                      label=f'Histogram (n={input.num_trials():,})')
        
        # Calculate theoretical normal distribution
        x = np.linspace(min(samples), max(samples), 100)
        pdf = norm.pdf(x, input.mean(), input.stddev())
        
        # Plot theoretical curve on right y-axis
        line = ax2.plot(x, pdf, 'r-', lw=2, label='Normal PDF')[0]
        
        # Set labels and title
        ax1.set_xlabel("Value", fontsize=14)
        ax1.set_ylabel("Count", fontsize=12, color='steelblue')
        ax2.set_ylabel("Density", fontsize=12, color='red')
        plt.title("Normal Distribution: Histogram and PDF", fontsize=16)
        
        # Color the tick labels to match the respective plots
        ax1.tick_params(axis='y', labelcolor='steelblue')
        ax2.tick_params(axis='y', labelcolor='red')
        
        # Ensure both axes start at 0
        ax1.set_ylim(bottom=0)
        ax2.set_ylim(bottom=0)
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers], rotation=90)
        
        # Add legends for both axes
        lines = [bars, line]
        labels = [b.get_label() for b in lines]
        ax1.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```

We've also included the exact values of the *probability density function* (PDF) to suggest that the underlying distribution is continuous. Very soon we will dive deeper into the probability density function and see why it is a little trickier than the *probability mass function* that we saw with discrete distributions.

### The Lognormal Distribution

Data generating processes do not need to be additive though, some processes tend to multiply. These kinds of processes will create a notably different distribution, called the log-normal distribution. It has two important differences from the normal distribution:

* It contains only positive values. 
* It has a very long 'tail' on the right hand side. Another way to describe this is skewness.

It's worth noting that there are many real world problems where values can only be positive.

```{shinylive-python}
#| standalone: true
#| viewerHeight: 550

from shiny import App, ui, render, reactive
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.stats import lognorm

app_ui = ui.page_fluid(
    ui.h2("Log-Normal Distribution Simulation with Binned Histogram"),
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "mu", 
                "Log-mean (μ)", 
                min=-2.0, 
                max=2.0, 
                value=0.0, 
                step=0.1
            ),
            ui.input_slider(
                "sigma", 
                "Log-standard deviation (σ)", 
                min=0.1, 
                max=2.0, 
                value=0.5, 
                step=0.1
            ),
            ui.input_slider(
                "num_trials",
                "Number of Trials",
                min=100,
                max=10000,
                value=1000,
                step=100
            ),
            ui.input_slider(
                "num_bins",
                "Number of Bins",
                min=10,
                max=100,
                value=30,
                step=5
            ),
        ),
        ui.output_plot("lognormPlot", height="400px"),
    ),
)

def server(input, output, session):
    @reactive.Calc
    def lognormal_samples():
        mu = input.mu()
        sigma = input.sigma()
        size = input.num_trials()
        return np.random.lognormal(mu, sigma, size)

    @output
    @render.plot
    def lognormPlot():
        samples = lognormal_samples()
        
        # Use the number of bins from the slider
        num_bins = input.num_bins()
        
        # Compute histogram (both count and density)
        counts_raw, bin_edges = np.histogram(samples, bins=num_bins)
        counts_density, _ = np.histogram(samples, bins=num_bins, density=True)
        bin_width = bin_edges[1] - bin_edges[0]
        bin_centers = bin_edges[:-1] + bin_width / 2
        
        fig, ax1 = plt.subplots(figsize=(10, 6))
        
        # Create the second y-axis sharing the same x-axis
        ax2 = ax1.twinx()
        
        # Plot histogram with counts on left y-axis
        bars = ax1.bar(bin_centers, counts_raw, width=bin_width*0.9, color="steelblue", 
                      alpha=0.6, edgecolor="black", align='center', 
                      label=f'Histogram (n={input.num_trials():,})')
        
        # Calculate theoretical log-normal distribution
        x = np.linspace(min(samples), max(samples), 1000)
        pdf = lognorm.pdf(x, input.sigma(), scale=np.exp(input.mu()))
        
        # Plot theoretical curve on right y-axis
        line = ax2.plot(x, pdf, 'r-', lw=2, label='Log-Normal PDF')[0]
        
        # Set labels and title
        ax1.set_xlabel("Value", fontsize=14)
        ax1.set_ylabel("Count", fontsize=12, color='steelblue')
        ax2.set_ylabel("Density", fontsize=12, color='red')
        plt.title("Log-Normal Distribution: Histogram and PDF", fontsize=16)
        
        # Color the tick labels to match the respective plots
        ax1.tick_params(axis='y', labelcolor='steelblue')
        ax2.tick_params(axis='y', labelcolor='red')
        
        # Ensure both axes start at 0
        ax1.set_ylim(bottom=0)
        ax2.set_ylim(bottom=0)
        
        # Set x-axis limits to focus on the main part of the distribution
        upper_limit = np.percentile(samples, 99)  # Show up to 99th percentile
        ax1.set_xlim(0, upper_limit)
        
        # Set x-axis ticks
        if len(bin_centers) > 20:
            step = math.ceil(len(bin_centers) / 20)
            ax1.set_xticks(bin_centers[::step])
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers[::step]], rotation=90)
        else:
            ax1.set_xticks(bin_centers)
            ax1.set_xticklabels([f"{x:.2f}" for x in bin_centers], rotation=90)
        
        # Add legends for both axes
        lines = [bars, line]
        labels = [b.get_label() for b in lines]
        ax1.legend(lines, labels, loc='upper right')
        
        plt.tight_layout()
        
        return fig

app = App(app_ui, server)
```

You may have guessed that if you have a set of values from the lognormal distribution, if you take the the log of their values and replot them, you'll end up plotting values in a normal distribution. 


### Summary

Again we keep this section brief as there are plenty of easily accessible references for continuous probability distributions. Hopefully the point was made though - that each continuous probability distribution is built on an idealized data generating process, and we can sample from the distribution as a way to model the outcome of the process.

## Probability of Data

### Probability *Density* Function

We've been hinting that the probability density function would require some explanation, and we've finally come to the right place to tackle it. We've continued to create histograms of our continuous data, in which we take multiple exact values and lump them together in a bin of the histogram. It's possible to use our relative frequency techinique to estimate the probability of the bin (just divide the bin count by the total count of all bins). However, how would estimate the probability of a single point within the bin? The problem is that, as we've noted earlier, the probability of any exact value on the real number line $\mathbb{R}$ is effectively zero.

To solve this, the probability density function does not give a true probability, it gives a value such that the following properties are true:

* The area under the curve sums to 1, i.e. all possible values have a total probability of 1.
* The values give the relative probability of that point vs other points.

### Single Data Point Example

When discussing continuous probability distributions, the probability of a specific value is effectively zero, so talking about the probability of that value is meaningless. Instead, we generally talk about the probability of getting a value as extreme or more extreme than the value that we observed.

In the app below, we find where the data point lies on the chart and then find the area, i.e. probability, of sampling points larger than the observation. This is utilizing bullet one from the properties of the probability density function given above. 

```{shinylive-python}
#| standalone: true
#| viewerHeight: 600

import math
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Probability Calculator"),
    
    ui.layout_sidebar(
        ui.sidebar(
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
            ui.input_numeric(
                "xInput", "Value (x):",
                value=105, min=0, max=200
            ),
            ui.br(),
            ui.h4("Probability (P(X ≥ x)):"),
            ui.output_text("probOutput"),
            width=300
        ),
        
        # Main panel
        ui.output_plot("normalPlot", height="400px"),
    )
)

def server(input, output, session):
    # Calculate probability
    @reactive.Calc
    def calculate_probability():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)
        return 1 - stats.norm.cdf(x, mu, sigma)

    # Show the probability
    @output
    @render.text
    def probOutput():
        prob = calculate_probability()
        return f"{prob:.4f}"

    # Plot the normal PDF with shaded area
    @output
    @render.plot
    def normalPlot():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)

        # Create x values for plotting
        x_min = mu - 4 * sigma
        x_max = mu + 4 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot the PDF
        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')
        
        # Shade the area for P(X ≥ x)
        x_shade = x_vals[x_vals >= x]
        y_shade = stats.norm.pdf(x_shade, mu, sigma)
        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, 
                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')

        # Add vertical line at x
        ax.axvline(x, color='red', linestyle='--', alpha=0.5)

        ax.set_title(f"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})")
        ax.set_xlabel("X")
        ax.set_ylabel("Density")
        ax.legend()
        ax.grid(True, alpha=0.3)

        return fig

app = App(app_ui, server)
```

When we refer to probabilities of "as extreme or more extreme" values, we are effectively computing tail probabilities using integrals. For example, for a right-tailed probability beyond a point $c$, we evaluate:

$$
P(X \geq c) = \int_c^\infty f(x) \, dx
$$



### Multiple Data Points Example

For multiple data points, we are generally interested in the *relative* probability of one series of events vs some other series of events. This is utilizing bullet two from the properties of the probability density function given above. 

**TODO Left off here, need to fix up relative probability in app below, and possibly replicate the dice multiple event probability app**

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Relative Probability of Normally Distributed Data"),

    # Row 1: Sliders
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "nInput", "Number of samples:",
                min=1, max=100, value=10, step=1
            ),
        ),
    ),

    ui.br(),

    # Row 2: Button, current data, and log-likelihood
    ui.row(
        ui.column(
            2,
            ui.input_action_button("newSampleBtn", "NEW SAMPLE"),
        ),
        ui.column(
            7,
            ui.h4("Current Data (Y):"),
            ui.output_text_verbatim("dataText"),
        ),
        ui.column(
            3,
            ui.h4("Relative Probability:"),
            ui.output_text("llOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Initialize data with 10 random points
    data_vals = reactive.Value(
        np.random.normal(loc=100, scale=np.sqrt(10), size=10)
    )

    # Generate a new sample when 'NEW SAMPLE' is pressed
    @reactive.Effect
    @reactive.event(input.newSampleBtn)
    def _():
        n = input.nInput()
        data_vals.set(
            np.random.normal(loc=100, scale=np.sqrt(10), size=n)
        )

    # Display the current data
    @output
    @render.text
    def dataText():
        y = data_vals()
        return ", ".join(str(round(val, 1)) for val in y)

    # Reactive expression for log-likelihood
    @reactive.Calc
    def log_likelihood():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        n = len(y)
        if var <= 0:
            return float("nan")
        term1 = -0.5 * n * math.log(2 * math.pi * var)
        term2 = -0.5 * np.sum((y - mu)**2) / var
        return term1 + term2

    # Show the log-likelihood
    @output
    @render.text
    def llOutput():
        ll = log_likelihood()
        return str(round(ll, 2))

    # Plot the normal PDF and data points
    @output
    @render.plot
    def normalPlot():
        y = data_vals()
        mu = input.muInput()
        var = input.varInput()
        sigma = math.sqrt(var)

        x_min = min(y) - 3 * sigma
        x_max = max(y) + 3 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = (1.0 / (sigma * np.sqrt(2 * math.pi))) * np.exp(
            -0.5 * ((x_vals - mu) / sigma)**2
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(
            x_vals, pdf_vals,
            color="blue",
            label=f"Normal PDF (μ={round(mu,1)}, σ²={round(var,1)})"
        )

        # Scatter the data at y=0 with some jitter
        jittered = y + np.random.uniform(-0.1, 0.1, size=len(y))
        ax.scatter(jittered, np.zeros_like(y), color="darkgreen", alpha=0.7, label="Data points")

        ax.axvline(mu, color="gray", linestyle="--")
        ax.set_title("Normal PDF vs. Observed Data")
        ax.set_xlabel("Y")
        ax.set_ylabel("Density")
        ax.legend()
        ax.set_ylim(bottom=0)

        return fig

app = App(app_ui, server)
```




**I think this is repeated but am not sure so not deleting quite yet**

Some data generating processes are quite common, so statisticians have named them and \[mostly\] standardized their inputs/parameters. We can think of **probability distributions** as models of common data generating processes. You can generate a random event from a probability distribution like we generated a single dice roll. If you generate an infinite number of events, you'll get a very smooth curve called the probability density, which is similar to what we saw when we really cranked up the number of dice rolls earlier.

A 'coin-flip' process produces the binomial distribution, although the binomial distribution also has the nice property of not requiring that the coin is fair. The Poisson distribution models a process that results in a count, like a coin flip, but it doesn't have an upper limit because there are not a fixed number of trials. An example of this is a failure count, we can specify a rate at which failures occur, and we will typically observe generated failure rates near this value, but there's no fixed maximum.

Additive processes will eventually create normal/gaussian distributions. It is why that distribution is so common in nature, and if interested in the details, research the Central Limit Theorem. If you hadn't noticed in the dice example, if you keep adding enough dice in enough rolls, the outcome looks awfully normal/gaussian. Of course not all processes interact in an additive way, if it tends to multiply instead, you'll get the log-normal distribution.

::: callout-warning
SHORTEN THIS.

There is a little slight of hand that statisticians use without making it explicit - they use probability distributions in different contexts with different meanings. No doubt this is part of the confusing welcome. The other use of probability distributions, to describe uncertainty, we will touch on at the end of this chapter.

If teaching someone a new language and using a single word repeatedly with two different meanings - it would be confusing. We can learn faster when things like this are explicitly disambiguated. I'll do that with the uses of probability distributions. When we use it in the context of data generation, I will refer to it as a *probability distribution model*. When we use it in the context of uncertainty, I will call it an *uncertainty distribution*. Just like the a single word with one spelling and two meanings, the math of the two cases will be exactly the same, but by calling out it's meaning explicitly I hope to aid your understanding substantially.

If you want to a more detailed description of the two use cases, just briefly scroll to the end of this chapter.
:::

The point here is that each probability distribution model has the basic form of:

1.  The model is mimicing a \[idealized\] process with inherent characteristics

2.  These characteristics determine the shape/frequency of the outcomes



## Normal/Gaussian Likelihood

Hopefully it was fairly intuitive how to find the probability of a certain dice-roll total. One advantage of that data generating process and model is that the outcomes were discrete - a total of nine, for example is one discrete outcome. Many data generating processes produce continuous outcomes. One common example is height.

A simple way to understand the probability of outcomes of a continuous process is to bin the values to certain ranges. Say heights of less than 4 feet, 4-5 feet, 5-6 feet, and 6-7 feet, and more than 7 feet. You can then use the same basic counting techniques we used earlier - how many outcomes are in the 5-6 feet bin, and divide that by the total number of outcomes. For example, the 5-6 feet bin may occur 740/1,000 times, for an approximate probability of 0.74.

**Histogram of continuous data??**

It's tempting to leave the discussion here and to essentially just use histograms of the continuous data and our relative frequency technique to find the approximate probability of a value. However, there are unfortunately problems. The first one that comes to mind is the arbitrary size of the bins you choose, although for practical problems we can probably get around it. The final straw though is that having a robust understanding of probability distributions will pay big dividends later (so we can use standard likelihood techniques), so we slip a little further into the statistical void.

## "Probability" of Continuous Data

When we plot a continuous probability distribution, we refer to any point as the probability density, and the area under the curve as the cumulative probability, where all the area is equal to one. An important point is the probability of an *exact* value is effectively zero. 

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Define the range for the x-axis
x = np.linspace(-4, 4, 1000)

# Calculate the probability density function (PDF) of the standard normal distribution
pdf = norm.pdf(x, loc=0, scale=1)

# Create the plot
plt.figure(figsize=(8, 5))
plt.plot(x, pdf, label="Standard Normal Distribution", linewidth=2)
plt.title("Standard Normal (Gaussian) Distribution", fontsize=14)
plt.xlabel("x", fontsize=12)
plt.ylabel("Probability Density", fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.axvline(0, color='red', linestyle='--', label="Mean (μ = 0)")
plt.legend(fontsize=12)
plt.show()
```

Although we shouldn't intepret a point on the graph as a probability, we *can* interpret it as the *relative probability* of that location compared to another. For example if a point has a probability density of 0.4 and another at 0.2, we can conclude the point at 0.4 has twice the relative probability.

::: callout-warning
I've reserved the use of *likelihood* until the second half of this primer during which we'll find the best model based on the data from the data generating process. However, in other texts you will find also find descriptions of the relative probability of continuous distributions called the relative likelihood.
:::

## Probability of a Single Event

When discussing continuous probability distributions, the probability of a specific value is effectively zero, so talking about the probability of that value is meaningless. Instead, we generally talk about the probability of getting a value as extreme or more extreme than the value that we observed. 

```{shinylive-python}
#| standalone: true
#| viewerHeight: 700

import math
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from shiny import App, ui, reactive, render

app_ui = ui.page_fluid(
    ui.h2("Normal Distribution Probability Calculator"),

    # Row 1: Parameters and input value
    ui.row(
        ui.column(
            4,
            ui.input_slider(
                "muInput", "Mean (μ):",
                min=50, max=150, value=100, step=0.1
            ),
        ),
        ui.column(
            4,
            ui.input_slider(
                "varInput", "Variance (σ²):",
                min=1, max=200, value=10, step=1
            ),
        ),
        ui.column(
            4,
            ui.input_numeric(
                "xInput", "Value (x):",
                value=110, min=0, max=200
            ),
        ),
    ),

    ui.br(),

    # Row 2: Probability output
    ui.row(
        ui.column(
            12,
            ui.h4("Probability (P(X ≥ x)):"),
            ui.output_text("probOutput"),
        ),
    ),

    ui.br(),

    # Plot
    ui.output_plot("normalPlot", height="400px"),
)

def server(input, output, session):
    # Calculate probability
    @reactive.Calc
    def calculate_probability():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)
        return 1 - stats.norm.cdf(x, mu, sigma)

    # Show the probability
    @output
    @render.text
    def probOutput():
        prob = calculate_probability()
        return f"{prob:.4f}"

    # Plot the normal PDF with shaded area
    @output
    @render.plot
    def normalPlot():
        mu = input.muInput()
        var = input.varInput()
        x = input.xInput()
        sigma = math.sqrt(var)

        # Create x values for plotting
        x_min = mu - 4 * sigma
        x_max = mu + 4 * sigma
        x_vals = np.linspace(x_min, x_max, 200)
        pdf_vals = stats.norm.pdf(x_vals, mu, sigma)

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot the PDF
        ax.plot(x_vals, pdf_vals, 'b-', label='Normal PDF')
        
        # Shade the area for P(X ≥ x)
        x_shade = x_vals[x_vals >= x]
        y_shade = stats.norm.pdf(x_shade, mu, sigma)
        ax.fill_between(x_shade, y_shade, color='red', alpha=0.3, 
                       label=f'P(X ≥ {x:.1f}) = {calculate_probability():.4f}')

        # Add vertical line at x
        ax.axvline(x, color='red', linestyle='--', alpha=0.5)

        ax.set_title(f"Normal Distribution (μ={mu:.1f}, σ²={var:.1f})")
        ax.set_xlabel("X")
        ax.set_ylabel("Density")
        ax.legend()
        ax.grid(True, alpha=0.3)

        return fig

app = App(app_ui, server)
```

HERE WE WANT TO VISUALIZE THE AREA UNDER THE CURVE.

Mathematically, what we just did is this:

For a continuous random variable $X$ with a PDF $f(x)$, the probability of observing a value within a range $[a, b]$ is calculated as:

$$
P(a \leq X \leq b) = \int_a^b f(x) \, dx
$$

This integral represents the area under the curve of the PDF between $a$ and $b$. When we refer to probabilities of "as extreme or more extreme" values, we typically compute tail probabilities using similar integrals. For example, for a right-tailed probability beyond a point $c$, we evaluate:

$$
P(X \geq c) = \int_c^\infty f(x) \, dx
$$



## Probability of Multiple Events

Maybe not get too sucked into the probability of multiple events when we really want to get to likelihood???

So far we only considered the probability/likelihood of a single outcome, however, we are often interested in the probability of multiple outcomes. Let's say we have $n$ independent events $E$. The probability of all these events occurring together is the product of their individual probabilities:

$$\prod_{i=1}^{n} P(E_i)$$

For example, if we flip a fair coin twice, the probability of getting heads both times is (1/2) \* (1/2) = 1/4. When dealing with many events or very small probabilities, multiplying probabilities can lead to numerical instability. A clever *and* useful trick is to use the logarithm of the probability product: $$\sum_{i=1}^{n} log(P(E_i))$$

You will find adding logarithms to be the standard for likelihood calculations.

## Assumptions
::: callout-warning
When determining the likelihood of multiple outcomes, there is an incredibly important assumption - that each event is \[mostly\] independent of each other. Perfect independence is rarely achieved, but as long as the correlations are not particularly strong, they are commonly ignored. In practice though, a common situation of not having independent events is the data in a time-series, do not assume independence in time-series data.

This is also a lesson in statistics in general, although the approach outlined here tries to limit the need for assumptions, many traditional statistical approaches have assumptions hidden underneath, and a very common one is that some aspect of the data is normally distributed. BE CAREFUL OF ASSUMPTIONS.
:::

## Probability Distribution or Uncertainty?

While I think a first order understanding of probability distributions should consider them as data generating processes, it turns out that they are conveniently used in another application, which is to simply express uncertainty about a value, or similarly, a prior belief about a value. When conceptualizing them as data generating processes, the variability in the outcome is an inherent part of the data generating process, there is no reason to think that the variability would shrink if we improved our understanding of the process. However, if we conceptualize them as an expression of uncertainty, or a prior belief, about a particular value or parameter, then the variability can shrink, and possibily shrink to a single value, when we gain more knowledge.

The second half of the book we will figure out how to best find the form and parameters (a model) of a data generating process. Often this requires probability distributions used in both contexts, and this is inherently confusing. It is best to think of it this way: 1) There may be a data generating process that is best described by a probability distribution. A perfect understanding of this process will not reduce the variability of its outputs. 2) This data generating probability distribution has parameters, and these parameters, with infinite knowledge, may have exact values. Unfortunately we don't have that knowledge and so we need to conceptualize them as uncertain. However, unlike the data generation of the probability distribution itself which will always be variable even with infinite knowledge, the uncertainty in the parameter values would shrink to a single value with infinite knowledge.

In the following chart we describe a data generating process based on the normal distribution (a data generating distribution) that generates height observations. We may have some uncertainty, however, in the correct values of the mean and variance parameters used in the normal distribution. We can express our uncertainty in the mean and variance parameters by describing them with a Gamma distribution (an uncertainty distribution).

```{mermaid}
flowchart LR
    subgraph DGD[Data Generating Distribution]
        subgraph UD[Uncertainty Distributions]
            GammaMean[Gamma Distribution]
            GammaVar[Gamma Distribution]
        end
        Mean[Mean]
        Variance[Variance]
        GammaMean --> Mean
        GammaVar --> Variance
        Mean --> Normal
        Variance --> Normal
        Normal[Normal Distribution]
    end
    Normal --> Height[Height Observations]

```



## Note to self

Models of wave heights could be a cool one.

We definitely want to mention the ways in which a probability distribution can be used - to generate data, to find the probability of data - and to also model the uncertainty of data, which is different than trying to model a process. (Uncertainty could be reduced to zero, while the variability in processes cannot).